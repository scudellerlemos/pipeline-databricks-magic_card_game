{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "334141d6-7348-4862-9af8-f8a09fc83687",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Camada Silver - Types - Magic: The Gathering\n",
    "# Objetivo: Transformação e limpeza de dados da Bronze para Silver\n",
    "# Características: Extract da Bronze, Transform com limpeza, Load na Silver com merge incremental\n",
    "\n",
    "# =============================================================================\n",
    "# BIBLIOTECAS UTILIZADAS\n",
    "# =============================================================================\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURAÇÃO DE SEGREDOS\n",
    "# =============================================================================\n",
    "def get_secret(secret_name, default_value=None):\n",
    "    try:\n",
    "        return dbutils.secrets.get(scope=\"mtg-pipeline\", key=secret_name)\n",
    "    except:\n",
    "        if default_value is not None:\n",
    "            print(f\"Secret '{secret_name}' não encontrado, usando valor padrão\")\n",
    "            return default_value\n",
    "        else:\n",
    "            print(f\"Secret obrigatório '{secret_name}' não encontrado\")\n",
    "            raise Exception(f\"Secret '{secret_name}' não configurado\")\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09c844e7-547a-45c3-a7db-7f4fa4bd2a91",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Configurações"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FUNÇÕES UTILITÁRIAS\n",
    "# =============================================================================\n",
    "def setup_unity_catalog(catalog_name, schema_name):\n",
    "    # Configura o Unity Catalog e schema\n",
    "    try:\n",
    "        spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog_name}\")\n",
    "        spark.sql(f\"USE CATALOG {catalog_name}\")\n",
    "        spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema_name}\")\n",
    "        spark.sql(f\"USE SCHEMA {schema_name}\")\n",
    "        print(f\"Schema {catalog_name}.{schema_name} criado ou já existente.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao configurar Unity Catalog: {e}\")\n",
    "        return False\n",
    "\n",
    "def extract_from_bronze(catalog_name, table_name_bronze):\n",
    "    # EXTRACT: Lê dados da camada Bronze\n",
    "    try:\n",
    "        bronze_table = f\"{catalog_name}.bronze.{table_name_bronze}\"\n",
    "        df = spark.table(bronze_table)\n",
    "        print(f\"Extraídos {df.count()} registros da Bronze\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Erro no EXTRACT da Bronze: {e}\")\n",
    "        return None\n",
    "\n",
    "def transform_types_silver(df):\n",
    "    # TRANSFORM: Aplica transformações e limpeza\n",
    "    if not df:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        print(\"Iniciando transformações...\")\n",
    "        \n",
    "        # Converter para maiúsculas e aplicar title case\n",
    "        df = df.withColumn(\"NME_TYPE\", initcap(trim(col(\"NME_TYPE\"))))\n",
    "        df = df.withColumn(\"NME_SOURCE\", initcap(trim(col(\"NME_SOURCE\"))))\n",
    "        \n",
    "        # Limpeza e tratamento de nulos\n",
    "        # Para descrições: substituir nulos/vazios por \"NA\"\n",
    "        df = df.withColumn(\"NME_TYPE\", when(col(\"NME_TYPE\").isNull() | (col(\"NME_TYPE\") == \"\"), lit(\"NA\")).otherwise(trim(col(\"NME_TYPE\"))))\n",
    "        df = df.withColumn(\"NME_SOURCE\", when(col(\"NME_SOURCE\").isNull() | (col(\"NME_SOURCE\") == \"\"), lit(\"NA\")).otherwise(trim(col(\"NME_SOURCE\"))))\n",
    "        \n",
    "        # Para números: substituir nulos por 0\n",
    "        df = df.withColumn(\"INGESTION_YEAR\", coalesce(col(\"INGESTION_YEAR\"), lit(0)))\n",
    "        df = df.withColumn(\"INGESTION_MONTH\", coalesce(col(\"INGESTION_MONTH\"), lit(0)))\n",
    "        \n",
    "        # Conversão de datas\n",
    "        df = df.withColumn(\"DT_INGESTION\", to_timestamp(col(\"DT_INGESTION\")))\n",
    "        \n",
    "        # Remover duplicatas baseadas em NME_TYPE\n",
    "        total_before = df.count()\n",
    "        df = df.dropDuplicates([\"NME_TYPE\"])\n",
    "        total_after = df.count()\n",
    "        print(f\"Removidas {total_before - total_after} duplicatas baseadas em NME_TYPE\")\n",
    "        \n",
    "        # Seleção final de colunas\n",
    "        colunas_finais = [\n",
    "            \"NME_TYPE\", \"NME_SOURCE\", \n",
    "            \"DT_INGESTION\", \"INGESTION_YEAR\", \"INGESTION_MONTH\"\n",
    "        ]\n",
    "        \n",
    "        # Filtrar colunas que existem no DataFrame\n",
    "        colunas_disponiveis = [c for c in colunas_finais if c in df.columns]\n",
    "        \n",
    "        df_final = df.select(*colunas_disponiveis)\n",
    "        return df_final\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro na transformação: {e}\")\n",
    "        return None\n",
    "\n",
    "def check_unity_table_exists(full_table_name):\n",
    "    # Verifica se a tabela Unity Catalog existe\n",
    "    try:\n",
    "        test_query = f\"SELECT 1 FROM {full_table_name} LIMIT 1\"\n",
    "        spark.sql(test_query)\n",
    "        print(f\"Tabela Unity Catalog '{full_table_name}' existe\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Tabela Unity Catalog '{full_table_name}' não existe\")\n",
    "        return False\n",
    "\n",
    "def load_to_silver_unity_incremental(df, catalog_name, schema_name, table_name, s3_silver_path):\n",
    "    # LOAD: Carrega dados na camada Silver\n",
    "    if not df:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        delta_path = f\"s3://{s3_silver_path}/{table_name}\"\n",
    "        full_table_name = f\"{catalog_name}.{schema_name}.{table_name}\"\n",
    "        \n",
    "        print(f\"Salvando dados em: {delta_path}\")\n",
    "        print(f\"Qtd linhas df_final: {df.count()}\")\n",
    "        print(f\"Colunas df_final: {df.columns}\")\n",
    "        print(f\"delta_path: {delta_path}\")\n",
    "        \n",
    "        # Verificar se tabela Delta existe\n",
    "        try:\n",
    "            existing_df = spark.read.format(\"delta\").load(delta_path)\n",
    "            existing_count = existing_df.count()\n",
    "            print(f\"Tabela Delta existe com {existing_count} registros\")\n",
    "            \n",
    "            # Verificar se schema mudou\n",
    "            existing_schema = set(existing_df.columns)\n",
    "            new_schema = set(df.columns)\n",
    "            \n",
    "            if existing_schema != new_schema:\n",
    "                print(\"Schema mudou. Salvando com overwrite e overwriteSchema=True.\")\n",
    "                df.write.format(\"delta\") \\\n",
    "                       .mode(\"overwrite\") \\\n",
    "                       .option(\"overwriteSchema\", \"true\") \\\n",
    "                       .partitionBy(\"INGESTION_YEAR\", \"INGESTION_MONTH\") \\\n",
    "                       .save(delta_path)\n",
    "            else:\n",
    "                # Merge incremental\n",
    "                from delta.tables import DeltaTable\n",
    "                delta_table = DeltaTable.forPath(spark, delta_path)\n",
    "                \n",
    "                # Merge baseado em NME_TYPE\n",
    "                delta_table.alias(\"silver\").merge(\n",
    "                    df.alias(\"novo\"),\n",
    "                    \"silver.NME_TYPE = novo.NME_TYPE\"\n",
    "                ).whenMatchedUpdateAll() \\\n",
    "                 .whenNotMatchedInsertAll() \\\n",
    "                 .execute()\n",
    "                print(\"Merge incremental executado com sucesso\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Tabela Delta não existe ou schema mudou. Salvando com overwrite e overwriteSchema=True.\")\n",
    "            df.write.format(\"delta\") \\\n",
    "                   .mode(\"overwrite\") \\\n",
    "                   .option(\"overwriteSchema\", \"true\") \\\n",
    "                   .partitionBy(\"INGESTION_YEAR\", \"INGESTION_MONTH\") \\\n",
    "                   .save(delta_path)\n",
    "        \n",
    "        print(\"Write Delta concluído com sucesso!\")\n",
    "        \n",
    "        # Criar ou atualizar tabela Unity Catalog\n",
    "        table_exists = check_unity_table_exists(full_table_name)\n",
    "        \n",
    "        if not table_exists:\n",
    "            print(f\"Tabela {full_table_name} não existe. Será criada.\")\n",
    "            # Criar tabela com schema explícito para permitir particionamento\n",
    "            spark.sql(f'''\\\n",
    "                CREATE TABLE {full_table_name} (\n",
    "                    NME_TYPE STRING,\n",
    "                    NME_SOURCE STRING,\n",
    "                    DT_INGESTION TIMESTAMP,\n",
    "                    INGESTION_YEAR INT,\n",
    "                    INGESTION_MONTH INT\n",
    "                )\n",
    "                USING DELTA\n",
    "                PARTITIONED BY (INGESTION_YEAR, INGESTION_MONTH)\n",
    "                LOCATION '{delta_path}'\n",
    "            ''')\n",
    "        else:\n",
    "            print(f\"Tabela {full_table_name} já existe.\")\n",
    "        \n",
    "        print(\"Dados salvos com sucesso na camada Silver!\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro no LOAD para Silver: {e}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0d17c0a-d1b8-4254-981f-1b23a39fe72d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Pipe"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PIPELINE PRINCIPAL\n",
    "# =============================================================================\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "CATALOG_NAME = get_secret(\"catalog_name\")\n",
    "SCHEMA_NAME = \"silver\"\n",
    "TABLE_NAME_BRONZE = \"TB_BRONZE_TYPES\"\n",
    "TABLE_NAME = \"TB_REF_SILVER_TYPES\"\n",
    "S3_BUCKET = get_secret(\"s3_bucket\")\n",
    "S3_SILVER_PREFIX = get_secret(\"s3_silver_prefix\", \"magic_the_gathering/silver\")\n",
    "S3_SILVER_PATH = f\"{S3_BUCKET}/{S3_SILVER_PREFIX}\"\n",
    "\n",
    "setup_unity_catalog(CATALOG_NAME, SCHEMA_NAME)\n",
    "df_bronze = extract_from_bronze(CATALOG_NAME, TABLE_NAME_BRONZE)\n",
    "df_final = transform_types_silver(df_bronze)\n",
    "load_to_silver_unity_incremental(df_final, CATALOG_NAME, SCHEMA_NAME, TABLE_NAME, S3_SILVER_PATH)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "TB_REF_SILVER_TYPES",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

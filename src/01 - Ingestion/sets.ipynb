{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "912fbeb9-48c8-4eba-86aa-d0b85c91bc4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ingestão de Sets - Magic: The Gathering\n",
    "# Objetivo: Ingerir dados de sets da API do Magic: The Gathering para staging em Parquet no S3\n",
    "# Características: Dados brutos, formato Parquet, filtro temporal, particionamento, incremental, tratamento de campos complexos\n",
    "\n",
    "# =============================================================================\n",
    "# BIBLIOTECAS UTILIZADAS\n",
    "# =============================================================================\n",
    "import requests\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.functions import current_timestamp, lit, col, year, month, when\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURAÇÃO DE SEGREDOS\n",
    "# =============================================================================\n",
    "\n",
    "def get_secret(secret_name, default_value=None):\n",
    "    try:\n",
    "        return dbutils.secrets.get(scope=\"mtg-pipeline\", key=secret_name)\n",
    "    except:\n",
    "        if default_value is not None:\n",
    "            print(f\"Segredo '{secret_name}' não encontrado, usando valor padrão\")\n",
    "            return default_value\n",
    "        else:\n",
    "            print(f\"Segredo obrigatório '{secret_name}' não encontrado\")\n",
    "            raise Exception(f\"Segredo '{secret_name}' não configurado\")\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURAÇÕES GLOBAIS\n",
    "# =============================================================================\n",
    "\n",
    "# Configuração de logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configurações da API\n",
    "API_BASE_URL = get_secret(\"api_base_url\")\n",
    "BATCH_SIZE = int(get_secret(\"batch_size\", \"100\"))\n",
    "MAX_RETRIES = int(get_secret(\"max_retries\", \"3\"))\n",
    "\n",
    "# Configurações do S3\n",
    "S3_BUCKET = get_secret(\"s3_bucket\")\n",
    "S3_PREFIX = get_secret(\"s3_prefix\")\n",
    "S3_BASE_PATH = f\"s3://{S3_BUCKET}/{S3_PREFIX}\"\n",
    "\n",
    "# Configurações de período\n",
    "YEARS_BACK = int(get_secret(\"years_back\", \"5\"))\n",
    "current_year = datetime.now().year\n",
    "cutoff_year = current_year - YEARS_BACK\n",
    "CUTOFF_DATE = datetime(cutoff_year, 1, 1)\n",
    "CUTOFF_DATE_STR = CUTOFF_DATE.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Log das configurações\n",
    "print(\"=\" * 60)\n",
    "print(\"CONFIGURAÇÕES PARA INGESTÃO DE SETS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"API_BASE_URL: [CONFIGURADO]\")\n",
    "print(\"S3_BASE_PATH: [CONFIGURADO]\")\n",
    "print(f\"YEARS_BACK: {YEARS_BACK}\")\n",
    "print(f\"CUTOFF_DATE_STR: {CUTOFF_DATE_STR}\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ffcf63e-1049-4036-bfb5-08069defd441",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Configurações"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FUNÇÕES UTILITÁRIAS\n",
    "# =============================================================================\n",
    "\n",
    "def setup_s3_storage():\n",
    "    try:\n",
    "        # Verificar se o diretório existe (criar se necessário)\n",
    "        try:\n",
    "            dbutils.fs.ls(S3_BASE_PATH)\n",
    "            print(\"Diretório do S3 já existe\")\n",
    "        except:\n",
    "            dbutils.fs.mkdirs(S3_BASE_PATH)\n",
    "            print(\"Diretório do S3 criado com sucesso\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao configurar S3 storage: {e}\")\n",
    "        return False\n",
    "\n",
    "def make_api_request(endpoint, params=None, retries=MAX_RETRIES):\n",
    "    url = f\"{API_BASE_URL}/{endpoint}\"\n",
    "    \n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, params=params, timeout=30)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                return response.json()\n",
    "            elif response.status_code == 429:  # Rate limit\n",
    "                wait_time = min((attempt + 1) * 5, 60)\n",
    "                print(f\"Rate limit atingido. Aguardando {wait_time}s...\")\n",
    "                time.sleep(wait_time)\n",
    "            elif response.status_code == 503:  # Service unavailable\n",
    "                wait_time = min((attempt + 1) * 10, 120)\n",
    "                print(f\"Serviço indisponível. Aguardando {wait_time}s...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"Erro {response.status_code} na API\")\n",
    "                if attempt < retries - 1:\n",
    "                    time.sleep(5)\n",
    "                \n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"Timeout na tentativa {attempt + 1}\")\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(10)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            if attempt == retries - 1:\n",
    "                print(f\"Erro na requisição para endpoint após {retries} tentativas: {e}\")\n",
    "                return None\n",
    "            print(f\"Tentativa {attempt + 1} falhou, tentando novamente...\")\n",
    "            time.sleep(1)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def clean_sets_data(data):\n",
    "    cleaned_data = []\n",
    "    for item in data:\n",
    "        if isinstance(item, dict):\n",
    "            cleaned_item = {}\n",
    "            \n",
    "            # Mapear campos conhecidos com tipos seguros\n",
    "            field_mappings = {\n",
    "                'code': str,\n",
    "                'name': str,\n",
    "                'type': str,\n",
    "                'border': str,\n",
    "                'mkm_id': int,\n",
    "                'mkm_name': str,\n",
    "                'releaseDate': str,\n",
    "                'gathererCode': str,\n",
    "                'magicCardsInfoCode': str,\n",
    "                'oldCode': str,\n",
    "                'onlineOnly': bool,\n",
    "                'source': str\n",
    "            }\n",
    "            \n",
    "            # Processar campos conhecidos\n",
    "            for field, field_type in field_mappings.items():\n",
    "                if field in item:\n",
    "                    try:\n",
    "                        if item[field] is not None:\n",
    "                            cleaned_item[field] = field_type(item[field])\n",
    "                        else:\n",
    "                            cleaned_item[field] = None\n",
    "                    except (ValueError, TypeError):\n",
    "                        # Se não conseguir converter, usar string\n",
    "                        cleaned_item[field] = str(item[field]) if item[field] is not None else None\n",
    "                else:\n",
    "                    cleaned_item[field] = None\n",
    "            \n",
    "            # Tratar campo booster complexo - EXPLODIR EM MÚLTIPLAS COLUNAS\n",
    "            if 'booster' in item and item['booster'] is not None:\n",
    "                booster_data = item['booster']\n",
    "                \n",
    "                if isinstance(booster_data, list):\n",
    "                    # Para cada elemento da lista booster, criar uma coluna separada\n",
    "                    for i, booster_item in enumerate(booster_data):\n",
    "                        if isinstance(booster_item, list):\n",
    "                            # Se o item é uma lista, converter para string JSON\n",
    "                            cleaned_item[f'booster_{i}'] = json.dumps(booster_item)\n",
    "                        else:\n",
    "                            # Se é um valor simples, converter para string\n",
    "                            cleaned_item[f'booster_{i}'] = str(booster_item)\n",
    "                    \n",
    "                    # Adicionar campo booster original como string JSON para referência\n",
    "                    cleaned_item['booster'] = json.dumps(booster_data)\n",
    "                else:\n",
    "                    # Se não é lista, manter como string\n",
    "                    cleaned_item['booster'] = str(booster_data)\n",
    "            else:\n",
    "                cleaned_item['booster'] = None\n",
    "            \n",
    "            cleaned_data.append(cleaned_item)\n",
    "    \n",
    "    return cleaned_data\n",
    "\n",
    "def save_to_parquet(data, table_name):\n",
    "    if not data:\n",
    "        print(f\"Nenhum dado para salvar na tabela {table_name}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Criar DataFrame com schema explícito para sets\n",
    "        if table_name == 'sets':\n",
    "            # Schema explícito para sets com colunas booster explodidas\n",
    "            schema_fields = [\n",
    "                StructField(\"code\", StringType(), True),\n",
    "                StructField(\"name\", StringType(), True),\n",
    "                StructField(\"type\", StringType(), True),\n",
    "                StructField(\"border\", StringType(), True),\n",
    "                StructField(\"mkm_id\", IntegerType(), True),\n",
    "                StructField(\"mkm_name\", StringType(), True),\n",
    "                StructField(\"releaseDate\", StringType(), True),\n",
    "                StructField(\"gathererCode\", StringType(), True),\n",
    "                StructField(\"magicCardsInfoCode\", StringType(), True),\n",
    "                StructField(\"booster\", StringType(), True),  # Campo original como JSON\n",
    "                StructField(\"oldCode\", StringType(), True),\n",
    "                StructField(\"onlineOnly\", BooleanType(), True),\n",
    "                StructField(\"source\", StringType(), True)\n",
    "            ]\n",
    "            \n",
    "            # Adicionar colunas booster explodidas (até 20 posições para cobrir a maioria dos casos)\n",
    "            for i in range(20):\n",
    "                schema_fields.append(StructField(f\"booster_{i}\", StringType(), True))\n",
    "            \n",
    "            schema = StructType(schema_fields)\n",
    "            df = spark.createDataFrame(data, schema)\n",
    "        else:\n",
    "            # Para outras tabelas, usar inferência automática\n",
    "            df = spark.createDataFrame(data)\n",
    "        \n",
    "        # Adicionar metadados de ingestão\n",
    "        df = df.withColumn(\"ingestion_timestamp\", current_timestamp()) \\\n",
    "               .withColumn(\"source\", lit(\"mtg_api\")) \\\n",
    "               .withColumn(\"endpoint\", lit(table_name))\n",
    "        \n",
    "        # Particionamento por Ano/Mês para sets\n",
    "        if table_name == 'sets' and 'releaseDate' in df.columns:\n",
    "            df = df.withColumn(\"partition_year\", \n",
    "                              when(col(\"releaseDate\").isNotNull(), \n",
    "                                   year(col(\"releaseDate\")))\n",
    "                              .otherwise(lit(datetime.now().year))) \\\n",
    "                   .withColumn(\"partition_month\", \n",
    "                              when(col(\"releaseDate\").isNotNull(), \n",
    "                                   month(col(\"releaseDate\")))\n",
    "                              .otherwise(lit(datetime.now().month)))\n",
    "            \n",
    "            # Filtrar apenas sets dos últimos 5 anos\n",
    "            total_sets = df.count()\n",
    "            df = df.filter(col(\"releaseDate\") >= lit(CUTOFF_DATE_STR))\n",
    "            filtered_sets = df.count()\n",
    "            print(f\"Sets filtrados: {filtered_sets}/{total_sets}\")\n",
    "        else:\n",
    "            # Para outras tabelas, usar data atual\n",
    "            df = df.withColumn(\"partition_year\", lit(datetime.now().year)) \\\n",
    "                   .withColumn(\"partition_month\", lit(datetime.now().month))\n",
    "        \n",
    "        # Salvar como Parquet no S3 com particionamento\n",
    "        partition_combinations = df.select(\"partition_year\", \"partition_month\").distinct().collect()\n",
    "        \n",
    "        for partition_row in partition_combinations:\n",
    "            partition_year = partition_row[\"partition_year\"]\n",
    "            partition_month = partition_row[\"partition_month\"]\n",
    "            \n",
    "            # Filtrar dados da partição\n",
    "            partition_df = df.filter((col(\"partition_year\") == partition_year) & \n",
    "                                   (col(\"partition_month\") == partition_month))\n",
    "            \n",
    "            # Nome do arquivo\n",
    "            file_name = f\"{partition_year}_{partition_month:02d}_{table_name}.parquet\"\n",
    "            file_path = f\"{S3_BASE_PATH}/{file_name}\"\n",
    "                \n",
    "            # Verificar se arquivo já existe\n",
    "            try:\n",
    "                existing_files = dbutils.fs.ls(file_path)\n",
    "                if len(existing_files) > 0:\n",
    "                    print(f\"Arquivo {file_name} já existe - pulando\")\n",
    "                    continue\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Salvar arquivo\n",
    "            partition_df.drop(\"partition_year\", \"partition_month\").write.mode(\"overwrite\").format(\"parquet\").save(file_path)\n",
    "            print(f\"Arquivo {file_name} criado com sucesso\")\n",
    "        \n",
    "        print(f\"Registros salvos como Parquet para {table_name}\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao salvar dados em {table_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "def ingest_simple_data(endpoint, table_name, data_key=None):\n",
    "    print(f\"Iniciando ingestão simples: {table_name}\")\n",
    "    \n",
    "    if data_key is None:\n",
    "        data_key = table_name\n",
    "    \n",
    "    data = make_api_request(endpoint)\n",
    "    \n",
    "    if data and data_key in data:\n",
    "        table_data = data[data_key]\n",
    "        \n",
    "        # Limpar dados se for sets\n",
    "        if table_name == 'sets':\n",
    "            print(\"Limpando dados de sets...\")\n",
    "            table_data = clean_sets_data(table_data)\n",
    "            \n",
    "            # Mostrar exemplo de explosão do booster\n",
    "            if table_data and len(table_data) > 0:\n",
    "                example_item = table_data[0]\n",
    "                booster_fields = [k for k in example_item.keys() if k.startswith('booster_')]\n",
    "                print(f\"Campo booster explodido em {len(booster_fields)} colunas: {booster_fields[:5]}...\")\n",
    "        \n",
    "        df = save_to_parquet(table_data, table_name)\n",
    "        \n",
    "        if df:\n",
    "            count = df.count()\n",
    "            print(f\"{table_name}: {count} registros processados\")\n",
    "            display(df.limit(5))\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"Falha ao obter dados de {table_name}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "# Configurar S3 Storage\n",
    "setup_success = setup_s3_storage()\n",
    "if not setup_success:\n",
    "    raise Exception(\"Falha ao configurar S3 storage\")\n",
    "\n",
    "print(\"Setup concluído com sucesso\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "711d0f50-c68a-4d0e-9a1c-8eeaa93053b5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Ingestão"
    }
   },
   "outputs": [],
   "source": [
    "# Iniciar ingestão de sets\n",
    "print(\"Iniciando ingestão de sets...\")\n",
    "\n",
    "sets_df = ingest_simple_data(\n",
    "    endpoint=\"sets\",\n",
    "    table_name=\"sets\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Gerar relatório\n",
    "print(\"=\" * 50)\n",
    "print(\"RELATÓRIO DE INGESTÃO DE SETS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if sets_df:\n",
    "    print(\"Arquivos salvos\")\n",
    "\n",
    "else:\n",
    "    print(\"Falha na ingestão de sets\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "sets",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

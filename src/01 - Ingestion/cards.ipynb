{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a61367a-af5d-4d39-86e6-d159901b7074",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Imports e Segredos"
    }
   },
   "outputs": [],
   "source": [
    "# Ingestão de Cards - Magic: The Gathering (Versão Corrigida)\n",
    "# Objetivo: Ingerir dados de cards da API do Magic: The Gathering para staging em Parquet no S3\n",
    "# Características: Dados brutos, formato Parquet, filtro temporal, particionamento, incremental, paginado\n",
    "\n",
    "# =============================================================================\n",
    "# BIBLIOTECAS UTILIZADAS\n",
    "# =============================================================================\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, current_timestamp, year, month\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, BooleanType\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURAÇÃO DE SEGREDOS\n",
    "# =============================================================================\n",
    "def get_secret(secret_name, default_value=None):\n",
    "    \"\"\"\n",
    "    Função para obter segredos do Databricks de forma segura\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return dbutils.secrets.get(scope=\"mtg-pipeline\", key=secret_name)\n",
    "    except:\n",
    "        if default_value is not None:\n",
    "            print(f\"Segredo '{secret_name}' não encontrado, usando valor padrão\")\n",
    "            return default_value\n",
    "        else:\n",
    "            print(f\"Segredo obrigatório '{secret_name}' não encontrado\")\n",
    "            raise Exception(f\"Segredo '{secret_name}' não configurado\")\n",
    "\n",
    "# =============================================================================\n",
    "# VARIÁVEIS DE CONFIGURAÇÃO\n",
    "# =============================================================================\n",
    "# Configurações da API\n",
    "API_BASE_URL = get_secret(\"api_base_url\",)\n",
    "BATCH_SIZE = int(get_secret(\"batch_size\", \"100\"))\n",
    "MAX_RETRIES = int(get_secret(\"max_retries\", \"3\"))\n",
    "\n",
    "# Configurações do S3\n",
    "S3_BUCKET = get_secret(\"s3_bucket\")\n",
    "S3_STAGE_PREFIX = get_secret(\"s3_stage_prefix\", \"magic_the_gathering/stage\")\n",
    "S3_BASE_PATH = f\"s3://{S3_BUCKET}/{S3_STAGE_PREFIX}\"\n",
    "\n",
    "# Configurações de janela temporal (5 anos)\n",
    "YEARS_BACK = int(get_secret(\"years_back\", \"5\"))\n",
    "current_year = datetime.now().year\n",
    "cutoff_year = current_year - YEARS_BACK\n",
    "CUTOFF_DATE = datetime(cutoff_year, 1, 1)\n",
    "CUTOFF_DATE_STR = CUTOFF_DATE.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1584e8be-ac55-40d7-a16c-4a568daacb7c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Configurações"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FUNÇÕES UTILITÁRIAS\n",
    "# =============================================================================\n",
    "def setup_s3_storage():\n",
    "    # Configura o storage S3\n",
    "    try:\n",
    "        # Verificar se o diretório existe (criar se necessário)\n",
    "        try:\n",
    "            dbutils.fs.ls(S3_BASE_PATH)\n",
    "            print(\"Diretório do S3 já existe\")\n",
    "        except:\n",
    "            dbutils.fs.mkdirs(S3_BASE_PATH)\n",
    "            print(\"Diretório do S3 criado com sucesso\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao configurar S3 storage: {e}\")\n",
    "        return False\n",
    "\n",
    "def make_api_request(endpoint, params=None, retries=MAX_RETRIES):\n",
    "    url = f\"{API_BASE_URL}/{endpoint}\"\n",
    "    print(f\"Fazendo requisição para: {url}\")\n",
    "    \n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, params=params, timeout=30)\n",
    "            print(f\"Status Code: {response.status_code}\")\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                print(f\"Dados recebidos: {type(data)}\")\n",
    "                if isinstance(data, dict):\n",
    "                    print(f\"Chaves disponíveis: {list(data.keys())}\")\n",
    "                return data\n",
    "            elif response.status_code == 429:  # Rate limit\n",
    "                wait_time = min((attempt + 1) * 5, 60)\n",
    "                print(f\"Rate limit atingido. Aguardando {wait_time}s...\")\n",
    "                time.sleep(wait_time)\n",
    "            elif response.status_code == 503:  # Service unavailable\n",
    "                wait_time = min((attempt + 1) * 10, 120)\n",
    "                print(f\"Serviço indisponível. Aguardando {wait_time}s...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"Erro {response.status_code} na API: {response.text[:200]}\")\n",
    "                if attempt < retries - 1:\n",
    "                    time.sleep(5)\n",
    "                \n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"Timeout na tentativa {attempt + 1}\")\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(10)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            if attempt == retries - 1:\n",
    "                print(f\"Erro na requisição para endpoint após {retries} tentativas: {e}\")\n",
    "                return None\n",
    "            print(f\"Tentativa {attempt + 1} falhou, tentando novamente...\")\n",
    "            time.sleep(1)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Erro ao decodificar JSON na tentativa {attempt + 1}: {e}\")\n",
    "            if attempt == retries - 1:\n",
    "                return None\n",
    "            time.sleep(1)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def clean_cards_data(data):\n",
    "    # Limpa e estrutura dados de cards\n",
    "    cleaned_data = []\n",
    "    \n",
    "    for item in data:\n",
    "        cleaned_item = {}\n",
    "        \n",
    "        # Campos simples - converter para string se necessário\n",
    "        simple_fields = ['name', 'manaCost', 'type', 'rarity', 'set', 'setName', 'text', \n",
    "                        'artist', 'number', 'power', 'toughness', 'layout', 'imageUrl', \n",
    "                        'originalText', 'originalType', 'id']\n",
    "        \n",
    "        for field in simple_fields:\n",
    "            if field in item and item[field] is not None:\n",
    "                try:\n",
    "                    # Tentar converter para o tipo apropriado\n",
    "                    if field in ['cmc', 'multiverseid']:\n",
    "                        cleaned_item[field] = float(item[field]) if field == 'cmc' else int(item[field])\n",
    "                    else:\n",
    "                        cleaned_item[field] = str(item[field])\n",
    "                except (ValueError, TypeError):\n",
    "                    # Se não conseguir converter, usar string\n",
    "                    cleaned_item[field] = str(item[field]) if item[field] is not None else None\n",
    "            else:\n",
    "                cleaned_item[field] = None\n",
    "        \n",
    "        # Tratar campos de lista - converter para string JSON\n",
    "        list_fields = ['colors', 'colorIdentity', 'types', 'subtypes', 'variations', 'foreignNames', 'printings', 'legalities']\n",
    "        for field in list_fields:\n",
    "            if field in item and item[field] is not None:\n",
    "                if isinstance(item[field], list):\n",
    "                    cleaned_item[field] = json.dumps(item[field])\n",
    "                else:\n",
    "                    cleaned_item[field] = str(item[field])\n",
    "            else:\n",
    "                cleaned_item[field] = None\n",
    "        \n",
    "        cleaned_data.append(cleaned_item)\n",
    "    \n",
    "    return cleaned_data\n",
    "\n",
    "def save_to_parquet(data, table_name):\n",
    "    if not data:\n",
    "        print(f\"Nenhum dado para salvar na tabela {table_name}\")\n",
    "        return None\n",
    "    try:\n",
    "        if table_name == 'cards':\n",
    "            # Schema explícito para cards\n",
    "            schema_fields = [\n",
    "                StructField(\"name\", StringType(), True),\n",
    "                StructField(\"manaCost\", StringType(), True),\n",
    "                StructField(\"cmc\", FloatType(), True),\n",
    "                StructField(\"colors\", StringType(), True),\n",
    "                StructField(\"colorIdentity\", StringType(), True),\n",
    "                StructField(\"type\", StringType(), True),\n",
    "                StructField(\"types\", StringType(), True),\n",
    "                StructField(\"subtypes\", StringType(), True),\n",
    "                StructField(\"rarity\", StringType(), True),\n",
    "                StructField(\"set\", StringType(), True),\n",
    "                StructField(\"setName\", StringType(), True),\n",
    "                StructField(\"text\", StringType(), True),\n",
    "                StructField(\"artist\", StringType(), True),\n",
    "                StructField(\"number\", StringType(), True),\n",
    "                StructField(\"power\", StringType(), True),\n",
    "                StructField(\"toughness\", StringType(), True),\n",
    "                StructField(\"layout\", StringType(), True),\n",
    "                StructField(\"multiverseid\", IntegerType(), True),\n",
    "                StructField(\"imageUrl\", StringType(), True),\n",
    "                StructField(\"variations\", StringType(), True),\n",
    "                StructField(\"foreignNames\", StringType(), True),\n",
    "                StructField(\"printings\", StringType(), True),\n",
    "                StructField(\"originalText\", StringType(), True),\n",
    "                StructField(\"originalType\", StringType(), True),\n",
    "                StructField(\"legalities\", StringType(), True),\n",
    "                StructField(\"id\", StringType(), True)\n",
    "            ]\n",
    "            \n",
    "            schema = StructType(schema_fields)\n",
    "            df = spark.createDataFrame(data, schema)\n",
    "        else:\n",
    "            df = spark.createDataFrame(data)\n",
    "        \n",
    "        # Adicionar metadados de ingestão\n",
    "        df = df.withColumn(\"ingestion_timestamp\", current_timestamp()) \\\n",
    "               .withColumn(\"source\", lit(\"mtg_api\")) \\\n",
    "               .withColumn(\"endpoint\", lit(table_name))\n",
    "        \n",
    "        # Particionamento CORRIGIDO: por data de ingestão\n",
    "        df = df.withColumn(\"partition_year\", year(col(\"ingestion_timestamp\"))) \\\n",
    "               .withColumn(\"partition_month\", month(col(\"ingestion_timestamp\")))\n",
    "        \n",
    "        # Salvar como Parquet no S3 com particionamento\n",
    "        partition_combinations = df.select(\"partition_year\", \"partition_month\").distinct().collect()\n",
    "        \n",
    "        for partition_row in partition_combinations:\n",
    "            partition_year = partition_row[\"partition_year\"]\n",
    "            partition_month = partition_row[\"partition_month\"]\n",
    "            \n",
    "            # Filtrar dados da partição\n",
    "            partition_df = df.filter((col(\"partition_year\") == partition_year) & \n",
    "                                   (col(\"partition_month\") == partition_month))\n",
    "            \n",
    "            # Nome do arquivo\n",
    "            file_name = f\"{partition_year}_{partition_month:02d}_{table_name}.parquet\"\n",
    "            file_path = f\"{S3_BASE_PATH}/{file_name}\"\n",
    "            \n",
    "            # Verificar se arquivo já existe\n",
    "            try:\n",
    "                existing_files = dbutils.fs.ls(file_path)\n",
    "                if len(existing_files) > 0:\n",
    "                    print(f\"Arquivo {file_name} já existe - pulando\")\n",
    "                    continue\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Salvar arquivo\n",
    "            partition_df.drop(\"partition_year\", \"partition_month\").write.mode(\"overwrite\").format(\"parquet\").save(file_path)\n",
    "            print(f\"Arquivo {file_name} criado com sucesso\")\n",
    "        \n",
    "        print(f\"Registros salvos como Parquet para {table_name}\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao salvar dados em {table_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "def ingest_paginated_data(endpoint, table_name, data_key=None, max_pages=100):\n",
    "    print(f\"Iniciando ingestão paginada: {table_name}\")\n",
    "    \n",
    "    if data_key is None:\n",
    "        data_key = table_name\n",
    "    \n",
    "    all_data = []\n",
    "    page = 1\n",
    "    \n",
    "    while page <= max_pages:\n",
    "        print(\"Processando página...\")\n",
    "        \n",
    "        params = {\"page\": page, \"pageSize\": BATCH_SIZE}\n",
    "        data = make_api_request(endpoint, params)\n",
    "        \n",
    "        if data and data_key in data:\n",
    "            page_data = data[data_key]\n",
    "            print(f\"Processando página {page}...\")\n",
    "            \n",
    "            if not page_data:\n",
    "                print(\"Página vazia - fim da paginação\")\n",
    "                break\n",
    "            \n",
    "            # Limpar dados se for cards\n",
    "            if table_name == 'cards':\n",
    "                print(\"Limpando dados de cards...\")\n",
    "                page_data = clean_cards_data(page_data)\n",
    "                print(f\"Dados limpos: {len(page_data)} registros\")\n",
    "            \n",
    "            all_data.extend(page_data)\n",
    "            page += 1\n",
    "            \n",
    "            # Aguardar um pouco entre as requisições\n",
    "            time.sleep(0.5)\n",
    "        else:\n",
    "            print(\"Falha ao obter dados da página\")\n",
    "            break\n",
    "    \n",
    "    # Verificar se há dados antes de tentar salvar\n",
    "    if not all_data:\n",
    "        print(f\"Nenhum dado válido para {table_name}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        df = save_to_parquet(all_data, table_name)\n",
    "        \n",
    "        if df:\n",
    "            count = df.count()\n",
    "            print(f\"{table_name}: {count} registros processados\")\n",
    "            return df\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao processar dados de {table_name}: {e}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "052ee362-a35c-4541-b1e8-be190dfda407",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Ingestão"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXECUÇÃO PRINCIPAL\n",
    "# =============================================================================\n",
    "\n",
    "# Verificar Spark\n",
    "try:\n",
    "    spark\n",
    "    print(\"Spark disponível\")\n",
    "except NameError:\n",
    "    print(\"Spark não está disponível - tentando obter do contexto\")\n",
    "    try:\n",
    "        from pyspark.sql import SparkSession\n",
    "        spark = SparkSession.builder.getOrCreate()\n",
    "        print(\"Spark criado com sucesso\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao criar Spark: {e}\")\n",
    "        raise Exception(\"Spark não está disponível\")\n",
    "\n",
    "# Configurar S3 Storage\n",
    "setup_success = setup_s3_storage()\n",
    "if not setup_success:\n",
    "    raise Exception(\"Falha ao configurar S3 storage\")\n",
    "\n",
    "print(\"Setup concluído com sucesso\")\n",
    "\n",
    "# Iniciar ingestão de cards\n",
    "print(\"Iniciando ingestão de cards...\")\n",
    "\n",
    "cards_df = ingest_paginated_data(\n",
    "    endpoint=\"cards\",\n",
    "    table_name=\"cards\",\n",
    "    max_pages=100  # Limite de 100 páginas para demonstração\n",
    ")\n",
    "\n",
    "# Gerar relatório\n",
    "print(\"=\" * 50)\n",
    "print(\"RELATÓRIO DE INGESTÃO DE CARDS (CORRIGIDO)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if cards_df:\n",
    "    print(\"✅ Arquivos salvos com sucesso\")\n",
    "    print(f\"📊 Total de registros: {cards_df.count()}\")\n",
    "    print(\"🎯 Particionamento: por ingestion_timestamp (ano/mês)\")\n",
    "else:\n",
    "    print(\"❌ Falha na ingestão de cards\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "cards",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a61367a-af5d-4d39-86e6-d159901b7074",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ingestão de Cards - Magic: The Gathering (Versão Corrigida)\n",
    "# Objetivo: Ingerir dados de cards da API do Magic: The Gathering para staging em Parquet no S3\n",
    "# Características: Dados brutos, formato Parquet, filtro temporal, particionamento, incremental, paginado\n",
    "\n",
    "# =============================================================================\n",
    "# BIBLIOTECAS UTILIZADAS\n",
    "# =============================================================================\n",
    "import requests\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.functions import current_timestamp, lit, col, year, month, when\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURAÇÃO DE SEGREDOS\n",
    "# =============================================================================\n",
    "\n",
    "def get_secret(secret_name, default_value=None):\n",
    "    try:\n",
    "        return dbutils.secrets.get(scope=\"mtg-pipeline\", key=secret_name)\n",
    "    except:\n",
    "        if default_value is not None:\n",
    "            print(f\"Segredo '{secret_name}' não encontrado, usando valor padrão\")\n",
    "            return default_value\n",
    "        else:\n",
    "            print(f\"Segredo obrigatório '{secret_name}' não encontrado\")\n",
    "            raise Exception(f\"Segredo '{secret_name}' não configurado\")\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURAÇÕES GLOBAIS\n",
    "# =============================================================================\n",
    "\n",
    "# Configuração de logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configurações da API\n",
    "API_BASE_URL = get_secret(\"api_base_url\", \"https://api.magicthegathering.io/v1\")\n",
    "BATCH_SIZE = int(get_secret(\"batch_size\", \"100\"))\n",
    "MAX_RETRIES = int(get_secret(\"max_retries\", \"3\"))\n",
    "\n",
    "# Configurações do S3\n",
    "S3_BUCKET = get_secret(\"s3_bucket\", \"lakenokiseki\")\n",
    "S3_PREFIX = get_secret(\"s3_prefix\", \"magic_the_gathering/stage\")\n",
    "S3_BASE_PATH = f\"s3://{S3_BUCKET}/{S3_PREFIX}\"\n",
    "\n",
    "# Configurações de período\n",
    "YEARS_BACK = int(get_secret(\"years_back\", \"5\"))\n",
    "current_year = datetime.now().year\n",
    "cutoff_year = current_year - YEARS_BACK\n",
    "CUTOFF_DATE = datetime(cutoff_year, 1, 1)\n",
    "CUTOFF_DATE_STR = CUTOFF_DATE.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Log das configurações\n",
    "print(\"=\" * 60)\n",
    "print(\"CONFIGURAÇÕES PARA INGESTÃO DE CARDS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"API_BASE_URL: [CONFIGURADO]\")\n",
    "print(\"S3_BASE_PATH: [CONFIGURADO]\")\n",
    "print(f\"YEARS_BACK: {YEARS_BACK}\")\n",
    "print(f\"CUTOFF_DATE_STR: {CUTOFF_DATE_STR}\")\n",
    "print(\"=\" * 60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1584e8be-ac55-40d7-a16c-4a568daacb7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FUNÇÕES UTILITÁRIAS\n",
    "# =============================================================================\n",
    "\n",
    "def setup_s3_storage():\n",
    "    try:\n",
    "        # Verificar se o diretório existe (criar se necessário)\n",
    "        try:\n",
    "            dbutils.fs.ls(S3_BASE_PATH)\n",
    "            print(\"Diretório do S3 já existe\")\n",
    "        except:\n",
    "            dbutils.fs.mkdirs(S3_BASE_PATH)\n",
    "            print(\"Diretório do S3 criado com sucesso\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao configurar S3 storage: {e}\")\n",
    "        return False\n",
    "\n",
    "def make_api_request(endpoint, params=None, retries=MAX_RETRIES):\n",
    "    url = f\"{API_BASE_URL}/{endpoint}\"\n",
    "    print(\"Fazendo requisição para API...\")\n",
    "    \n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, params=params, timeout=30)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                return data\n",
    "            elif response.status_code == 429:  # Rate limit\n",
    "                wait_time = min((attempt + 1) * 5, 60)\n",
    "                print(f\"Rate limit atingido. Aguardando {wait_time}s...\")\n",
    "                time.sleep(wait_time)\n",
    "            elif response.status_code == 503:  # Service unavailable\n",
    "                wait_time = min((attempt + 1) * 10, 120)\n",
    "                print(f\"Serviço indisponível. Aguardando {wait_time}s...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"Erro {response.status_code} na API: {response.text[:200]}\")\n",
    "                if attempt < retries - 1:\n",
    "                    time.sleep(5)\n",
    "                \n",
    "        except requests.exceptions.Timeout:\n",
    "            print(\"Timeout na requisição\")\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(10)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            if attempt == retries - 1:\n",
    "                print(f\"Erro na requisição para endpoint: {e}\")\n",
    "                return None\n",
    "            print(\"Tentativa falhou, tentando novamente...\")\n",
    "            time.sleep(1)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Erro ao decodificar JSON: {e}\")\n",
    "            if attempt == retries - 1:\n",
    "                return None\n",
    "            time.sleep(1)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def clean_cards_data(data):\n",
    "    cleaned_data = []\n",
    "    for item in data:\n",
    "        if isinstance(item, dict):\n",
    "            cleaned_item = {}\n",
    "            \n",
    "            # Mapear campos conhecidos com tipos seguros\n",
    "            field_mappings = {\n",
    "                'name': str,\n",
    "                'manaCost': str,\n",
    "                'cmc': float,\n",
    "                'type': str,\n",
    "                'rarity': str,\n",
    "                'set': str,\n",
    "                'setName': str,\n",
    "                'text': str,\n",
    "                'artist': str,\n",
    "                'number': str,\n",
    "                'power': str,\n",
    "                'toughness': str,\n",
    "                'layout': str,\n",
    "                'multiverseid': int,\n",
    "                'imageUrl': str,\n",
    "                'originalText': str,\n",
    "                'originalType': str,\n",
    "                'id': str\n",
    "            }\n",
    "            \n",
    "            # Processar campos conhecidos\n",
    "            for field, field_type in field_mappings.items():\n",
    "                if field in item:\n",
    "                    try:\n",
    "                        if item[field] is not None:\n",
    "                            cleaned_item[field] = field_type(item[field])\n",
    "                        else:\n",
    "                            cleaned_item[field] = None\n",
    "                    except (ValueError, TypeError):\n",
    "                        # Se não conseguir converter, usar string\n",
    "                        cleaned_item[field] = str(item[field]) if item[field] is not None else None\n",
    "                else:\n",
    "                    cleaned_item[field] = None\n",
    "            \n",
    "            # Tratar campos de lista - converter para string JSON\n",
    "            list_fields = ['colors', 'colorIdentity', 'types', 'subtypes', 'variations', 'foreignNames', 'printings', 'legalities']\n",
    "            for field in list_fields:\n",
    "                if field in item and item[field] is not None:\n",
    "                    if isinstance(item[field], list):\n",
    "                        cleaned_item[field] = json.dumps(item[field])\n",
    "                    else:\n",
    "                        cleaned_item[field] = str(item[field])\n",
    "                else:\n",
    "                    cleaned_item[field] = None\n",
    "            \n",
    "            cleaned_data.append(cleaned_item)\n",
    "    \n",
    "    return cleaned_data\n",
    "\n",
    "def save_to_parquet(data, table_name):\n",
    "    if not data:\n",
    "        print(f\"Nenhum dado para salvar na tabela {table_name}\")\n",
    "        return None\n",
    "    try:\n",
    "        if table_name == 'cards':\n",
    "            # Schema explícito para cards\n",
    "            schema_fields = [\n",
    "                StructField(\"name\", StringType(), True),\n",
    "                StructField(\"manaCost\", StringType(), True),\n",
    "                StructField(\"cmc\", FloatType(), True),\n",
    "                StructField(\"colors\", StringType(), True),\n",
    "                StructField(\"colorIdentity\", StringType(), True),\n",
    "                StructField(\"type\", StringType(), True),\n",
    "                StructField(\"types\", StringType(), True),\n",
    "                StructField(\"subtypes\", StringType(), True),\n",
    "                StructField(\"rarity\", StringType(), True),\n",
    "                StructField(\"set\", StringType(), True),\n",
    "                StructField(\"setName\", StringType(), True),\n",
    "                StructField(\"text\", StringType(), True),\n",
    "                StructField(\"artist\", StringType(), True),\n",
    "                StructField(\"number\", StringType(), True),\n",
    "                StructField(\"power\", StringType(), True),\n",
    "                StructField(\"toughness\", StringType(), True),\n",
    "                StructField(\"layout\", StringType(), True),\n",
    "                StructField(\"multiverseid\", IntegerType(), True),\n",
    "                StructField(\"imageUrl\", StringType(), True),\n",
    "                StructField(\"variations\", StringType(), True),\n",
    "                StructField(\"foreignNames\", StringType(), True),\n",
    "                StructField(\"printings\", StringType(), True),\n",
    "                StructField(\"originalText\", StringType(), True),\n",
    "                StructField(\"originalType\", StringType(), True),\n",
    "                StructField(\"legalities\", StringType(), True),\n",
    "                StructField(\"id\", StringType(), True)\n",
    "            ]\n",
    "            \n",
    "            schema = StructType(schema_fields)\n",
    "            df = spark.createDataFrame(data, schema)\n",
    "        else:\n",
    "            df = spark.createDataFrame(data)\n",
    "        \n",
    "        df = df.withColumn(\"ingestion_timestamp\", current_timestamp()) \\\n",
    "               .withColumn(\"source\", lit(\"mtg_api\")) \\\n",
    "               .withColumn(\"endpoint\", lit(table_name))\n",
    "        \n",
    "        # Particionamento por data atual (cards não têm data de lançamento individual)\n",
    "        df = df.withColumn(\"partition_year\", lit(datetime.now().year)) \\\n",
    "               .withColumn(\"partition_month\", lit(datetime.now().month))\n",
    "        \n",
    "        partition_combinations = df.select(\"partition_year\", \"partition_month\").distinct().collect()\n",
    "        for partition_row in partition_combinations:\n",
    "            partition_year = partition_row[\"partition_year\"]\n",
    "            partition_month = partition_row[\"partition_month\"]\n",
    "            partition_df = df.filter((col(\"partition_year\") == partition_year) & \n",
    "                                   (col(\"partition_month\") == partition_month))\n",
    "            file_name = f\"{partition_year}_{partition_month:02d}_{table_name}.parquet\"\n",
    "            file_path = f\"{S3_BASE_PATH}/{file_name}\"\n",
    "            try:\n",
    "                existing_files = dbutils.fs.ls(file_path)\n",
    "                if len(existing_files) > 0:\n",
    "                    print(f\"Arquivo {file_name} já existe - pulando\")\n",
    "                    continue\n",
    "            except:\n",
    "                pass\n",
    "            partition_df.drop(\"partition_year\", \"partition_month\").write.mode(\"overwrite\").format(\"parquet\").save(file_path)\n",
    "            print(f\"Arquivo {file_name} criado com sucesso\")\n",
    "        print(f\"Registros salvos como Parquet para {table_name}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao salvar dados em {table_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "def ingest_paginated_data(endpoint, table_name, data_key=None, max_pages=100):\n",
    "    print(f\"Iniciando ingestão paginada: {table_name}\")\n",
    "    \n",
    "    if data_key is None:\n",
    "        data_key = table_name\n",
    "    \n",
    "    all_data = []\n",
    "    page = 1\n",
    "    \n",
    "    while page <= max_pages:\n",
    "        print(\"Processando página...\")\n",
    "        \n",
    "        params = {\"page\": page, \"pageSize\": BATCH_SIZE}\n",
    "        data = make_api_request(endpoint, params)\n",
    "        \n",
    "        if data and data_key in data:\n",
    "            page_data = data[data_key]\n",
    "            print(f\"Processando página {page}...\")\n",
    "            \n",
    "            if not page_data:\n",
    "                print(\"Página vazia - fim da paginação\")\n",
    "                break\n",
    "            \n",
    "            # Limpar dados se for cards\n",
    "            if table_name == 'cards':\n",
    "                print(\"Limpando dados de cards...\")\n",
    "                page_data = clean_cards_data(page_data)\n",
    "                print(f\"Limpando dados de cards...\")\n",
    "            \n",
    "            all_data.extend(page_data)\n",
    "            page += 1\n",
    "            \n",
    "            # Aguardar um pouco entre as requisições\n",
    "            time.sleep(0.5)\n",
    "        else:\n",
    "            print(\"Falha ao obter dados da página\")\n",
    "            break\n",
    "    \n",
    "\n",
    "    \n",
    "    # Verificar se há dados antes de tentar salvar\n",
    "    if not all_data:\n",
    "        print(f\"Nenhum dado válido para {table_name}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        df = save_to_parquet(all_data, table_name)\n",
    "        \n",
    "        if df:\n",
    "            count = df.count()\n",
    "            print(f\"{table_name}: {count} registros processados\")\n",
    "            display(df.limit(5))\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao processar dados de {table_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Verificar Spark\n",
    "try:\n",
    "    spark\n",
    "    print(\"Spark disponível\")\n",
    "except NameError:\n",
    "    print(\"Spark não está disponível - tentando obter do contexto\")\n",
    "    try:\n",
    "        from pyspark.sql import SparkSession\n",
    "        spark = SparkSession.builder.getOrCreate()\n",
    "        print(\"Spark criado com sucesso\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao criar Spark: {e}\")\n",
    "        raise Exception(\"Spark não está disponível\")\n",
    "\n",
    "# Configurar S3 Storage\n",
    "setup_success = setup_s3_storage()\n",
    "if not setup_success:\n",
    "    raise Exception(\"Falha ao configurar S3 storage\")\n",
    "\n",
    "print(\"Setup concluído com sucesso\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "052ee362-a35c-4541-b1e8-be190dfda407",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Ingestão"
    }
   },
   "outputs": [],
   "source": [
    "# Iniciar ingestão de cards\n",
    "print(\"Iniciando ingestão de cards...\")\n",
    "\n",
    "cards_df = ingest_paginated_data(\n",
    "    endpoint=\"cards\",\n",
    "    table_name=\"cards\",\n",
    "    max_pages=100  # Limite de 100 páginas para demonstração\n",
    ")\n",
    "\n",
    "# Gerar relatório\n",
    "print(\"=\" * 50)\n",
    "print(\"RELATÓRIO DE INGESTÃO DE CARDS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if cards_df:\n",
    "    print(\"Arquivos salvos com sucesso\")\n",
    "else:\n",
    "    print(\"Falha na ingestão de cards\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Ingestão de Cards Concluída!\n",
    "# Resumo: Tabela cards, registros processados, arquivos salvos em S3\n",
    "# Como acessar: df = spark.read.parquet(\"[S3_PATH]/cards_*.parquet\") "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "cards",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c750bba-2ef1-4f49-9eee-65bbc18c1b4a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import e Segredos"
    }
   },
   "outputs": [],
   "source": [
    "# Ingestão de Preços de Cards - Magic: The Gathering\n",
    "# Objetivo: Ingerir preços das cartas da Scryfall API para staging em Parquet no S3\n",
    "# Características: Preços atualizados, formato Parquet, filtro temporal, particionamento por ano/mês, incremental, idempotente\n",
    "\n",
    "# =============================================================================\n",
    "# BIBLIOTECAS UTILIZADAS\n",
    "# =============================================================================\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import year, month, to_timestamp\n",
    "from datetime import datetime\n",
    "import time\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURAÇÃO DE SEGREDOS\n",
    "# =============================================================================\n",
    "def get_secret(secret_name, default_value=None):\n",
    "    # Função para obter segredos do Databricks\n",
    "    try:\n",
    "        return dbutils.secrets.get(scope=\"mtg-pipeline\", key=secret_name)\n",
    "    except:\n",
    "        if default_value is not None:\n",
    "            print(f\"Segredo '{secret_name}' não encontrado, usando valor padrão\")\n",
    "            return default_value\n",
    "        else:\n",
    "            print(f\"Segredo obrigatório '{secret_name}' não encontrado\")\n",
    "            raise Exception(f\"Segredo '{secret_name}' não configurado\")\n",
    "\n",
    "# =============================================================================\n",
    "# VARIÁVEIS DE CONFIGURAÇÃO\n",
    "# =============================================================================\n",
    "BATCH_SIZE = int(get_secret(\"batch_size\", \"100\"))  # Tamanho do lote de processamento de cartas\n",
    "S3_BUCKET = get_secret(\"s3_bucket\")  # Nome do bucket S3\n",
    "S3_STAGE_PREFIX = get_secret(\"s3_stage_prefix\", \"magic_the_gathering/stage\")  # Prefixo da pasta staging\n",
    "S3_BASE_PATH = f\"s3://{S3_BUCKET}/{S3_STAGE_PREFIX}\"\n",
    "SLEEP_BETWEEN = 0.1  # Segundos entre requests (não utilizado no batch)\n",
    "SCRYFALL_API_URL = get_secret(\"scryfall_api_url\")  # URL da Scryfall API\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99d30dc1-919f-4954-9600-c0250e2c5d49",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Configurações"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FUNÇÕES UTILITÁRIAS\n",
    "# =============================================================================\n",
    "def get_card_price(card_name):\n",
    "    # Busca preço da carta na Scryfall API\n",
    "    url = f\"{SCRYFALL_API_URL}/cards/named?exact={card_name}\"\n",
    "    try:\n",
    "        resp = requests.get(url, timeout=10)\n",
    "        if resp.status_code == 200:\n",
    "            data = resp.json()\n",
    "            return {\n",
    "                \"name\": data.get(\"name\"),\n",
    "                \"set\": data.get(\"set\"),\n",
    "                \"rarity\": data.get(\"rarity\"),\n",
    "                \"usd\": data.get(\"prices\", {}).get(\"usd\"),\n",
    "                \"eur\": data.get(\"prices\", {}).get(\"eur\"),\n",
    "                \"tix\": data.get(\"prices\", {}).get(\"tix\"),\n",
    "                \"scryfall_uri\": data.get(\"scryfall_uri\"),\n",
    "                \"image_url\": data.get(\"image_uris\", {}).get(\"normal\") if data.get(\"image_uris\") else None,\n",
    "                \"ingestion_timestamp\": datetime.utcnow().isoformat(),\n",
    "                \"source\": \"scryfall\"\n",
    "            }\n",
    "        else:\n",
    "            return {\"name\": card_name, \"error\": f\"Status {resp.status_code}\"}\n",
    "    except Exception as e:\n",
    "        return {\"name\": card_name, \"error\": str(e)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cecede86-34c4-42b0-94f6-6bdcb98bdf4e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Ingestão"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXECUÇÃO PRINCIPAL\n",
    "# =============================================================================\n",
    "# Inicializa sessão Spark\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "# Lista todos os arquivos na pasta staging\n",
    "files = dbutils.fs.ls(S3_BASE_PATH)\n",
    "# Seleciona apenas arquivos de cards no padrão ano_mes_cards.parquet\n",
    "card_files = [f.path for f in files if re.match(r'.*/(\\d{4})_(\\d{2})_cards.parquet/?$', f.path)]\n",
    "\n",
    "current_year = datetime.now().year\n",
    "cutoff_year = current_year - 5  # Mantém apenas 5 anos completos\n",
    "\n",
    "# Limpeza automática de arquivos antigos (mantém apenas 5 anos completos)\n",
    "for f in files:\n",
    "    match_price = re.search(r'(\\d{4})_(\\d{2})_card_prices.parquet', f.path)\n",
    "    if match_price:\n",
    "        year_val = int(match_price.group(1))\n",
    "        if year_val < cutoff_year:\n",
    "            print(f\"Deletando arquivo antigo de preços: {f.path}\")\n",
    "            dbutils.fs.rm(f.path, True)\n",
    "    match_cards = re.search(r'(\\d{4})_(\\d{2})_cards.parquet', f.path)\n",
    "    if match_cards:\n",
    "        year_val = int(match_cards.group(1))\n",
    "        if year_val < cutoff_year:\n",
    "            print(f\"Deletando arquivo antigo de cards: {f.path}\")\n",
    "            dbutils.fs.rm(f.path, True)\n",
    "\n",
    "# Processa cada arquivo de cards (um batch por arquivo ano/mês)\n",
    "total_files = len(card_files)\n",
    "for idx, card_file in enumerate(card_files, 1):\n",
    "    match = re.search(r'(\\d{4})_(\\d{2})_cards.parquet', card_file)\n",
    "    if match:\n",
    "        year_val, month_val = int(match.group(1)), int(match.group(2))\n",
    "        if year_val < cutoff_year:\n",
    "            print(f\"Pulando arquivo antigo: {card_file}\")\n",
    "            continue\n",
    "        price_file = f\"{S3_BASE_PATH}/{year_val:04d}_{month_val:02d}_card_prices.parquet\"\n",
    "        # Verifica se já existe arquivo de preços para esse ano/mês\n",
    "        if any(f.path.rstrip('/') == price_file for f in files):\n",
    "            print(f\"Arquivo de preços já existe para {year_val}-{month_val}, pulando.\")\n",
    "            continue\n",
    "        file_name = card_file.split('/')[-1]  # Nome do arquivo de cards\n",
    "        price_file_name = price_file.split('/')[-1]  # Nome do arquivo de preços\n",
    "        percent_files = (idx / total_files) * 100\n",
    "        print(f\"Processando arquivo {idx}/{total_files} ({percent_files:.1f}%) - {file_name}\")\n",
    "        # Lê os nomes das cartas desse arquivo de cards\n",
    "        df_cards = spark.read.parquet(card_file)\n",
    "        card_names = [row['name'] for row in df_cards.select('name').distinct().collect()]\n",
    "        prices = []\n",
    "        total_cards = len(card_names)\n",
    "        num_batches = (total_cards + BATCH_SIZE - 1) // BATCH_SIZE  # Número de batches\n",
    "        # Processa as cartas em lotes (batches) de tamanho BATCH_SIZE\n",
    "        for batch_idx in range(num_batches):\n",
    "            start = batch_idx * BATCH_SIZE\n",
    "            end = min(start + BATCH_SIZE, total_cards)\n",
    "            batch_names = card_names[start:end]\n",
    "            # Busca preços em paralelo usando até 7 workers (limite seguro para Scryfall)\n",
    "            with ThreadPoolExecutor(max_workers=7) as executor:\n",
    "                future_to_name = {executor.submit(get_card_price, name): name for name in batch_names}\n",
    "                batch_prices = []\n",
    "                for future in as_completed(future_to_name):\n",
    "                    batch_prices.append(future.result())\n",
    "            # Mantém a ordem dos preços igual à ordem dos nomes\n",
    "            batch_prices_sorted = [next(bp for bp in batch_prices if bp['name'] == name) for name in batch_names]\n",
    "            prices.extend(batch_prices_sorted)\n",
    "            percent_total = (end / total_cards) * 100\n",
    "            print(f\"Batch {batch_idx+1}/{num_batches} - Progresso total da ingestão: {percent_total:.2f} %\")\n",
    "        # Cria DataFrame Spark diretamente da lista de dicts\n",
    "        spark_df_prices = spark.createDataFrame(prices)\n",
    "        # Adiciona colunas de partição por ano/mês\n",
    "        spark_df_prices = spark_df_prices.withColumn(\n",
    "            \"partition_year\", year(to_timestamp(\"ingestion_timestamp\"))\n",
    "        ).withColumn(\n",
    "            \"partition_month\", month(to_timestamp(\"ingestion_timestamp\"))\n",
    "        )\n",
    "        # Salva o arquivo de preços para o ano/mês, particionando fisicamente por ano/mês\n",
    "        spark_df_prices.coalesce(1).write.mode(\"overwrite\") \\\n",
    "            .partitionBy(\"partition_year\", \"partition_month\") \\\n",
    "            .parquet(price_file)\n",
    "        print(f\"Preços salvos: {price_file_name}\")\n",
    "        print(f\"Total de cartas processadas: {len(card_names)}\")\n",
    "        print(f\"Total de registros de preço: {spark_df_prices.count()}\")\n",
    "        print(\"Checklist de execução:\")\n",
    "        print(\"- [x] Parâmetros configurados\")\n",
    "        print(\"- [x] Nomes de cartas carregados da staging\")\n",
    "        print(\"- [x] Preços buscados na Scryfall API\")\n",
    "        print(\"- [x] Dados limpos e estruturados\")\n",
    "        print(f\"- [x] Parquet salvo: {price_file_name}\")\n",
    "        print(\"- [x] Logs gerados com sucesso\")\n",
    "    else:\n",
    "        print(f\"Nome de arquivo inesperado: {card_file}\") "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "card_prices",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

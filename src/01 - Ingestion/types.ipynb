{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88609f39-5e36-495f-b32d-bd0a19828e17",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import e Segredos"
    }
   },
   "outputs": [],
   "source": [
    "# Ingestão de Types - Magic: The Gathering (Versão Corrigida)\n",
    "# Objetivo: Ingerir dados de types da API do Magic: The Gathering para staging em Parquet no S3\n",
    "# Características: Dados brutos, formato Parquet, tabela de referência, particionamento, incremental\n",
    "\n",
    "# =============================================================================\n",
    "# BIBLIOTECAS UTILIZADAS\n",
    "# =============================================================================\n",
    "import requests\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.functions import current_timestamp, lit, col, year, month, when\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURAÇÃO DE SEGREDOS\n",
    "# =============================================================================\n",
    "\n",
    "def get_secret(secret_name, default_value=None):\n",
    "    try:\n",
    "        return dbutils.secrets.get(scope=\"mtg-pipeline\", key=secret_name)\n",
    "    except:\n",
    "        if default_value is not None:\n",
    "            print(f\"Segredo '{secret_name}' não encontrado, usando valor padrão\")\n",
    "            return default_value\n",
    "        else:\n",
    "            print(f\"Segredo obrigatório '{secret_name}' não encontrado\")\n",
    "            raise Exception(f\"Segredo '{secret_name}' não configurado\")\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURAÇÕES GLOBAIS\n",
    "# =============================================================================\n",
    "\n",
    "# Configuração de logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configurações da API\n",
    "API_BASE_URL = get_secret(\"api_base_url\")\n",
    "BATCH_SIZE = int(get_secret(\"batch_size\", \"100\"))\n",
    "MAX_RETRIES = int(get_secret(\"max_retries\", \"3\"))\n",
    "\n",
    "# Configurações do S3\n",
    "S3_BUCKET = get_secret(\"s3_bucket\")\n",
    "S3_PREFIX = get_secret(\"s3_prefix\")\n",
    "S3_BASE_PATH = f\"s3://{S3_BUCKET}/{S3_PREFIX}\"\n",
    "\n",
    "# Log das configurações\n",
    "print(\"=\" * 60)\n",
    "print(\"CONFIGURAÇÕES PARA INGESTÃO DE TYPES\")\n",
    "print(\"=\" * 60)\n",
    "print(\"API_BASE_URL: [CONFIGURADO]\")\n",
    "print(\"S3_BASE_PATH: [CONFIGURADO]\")\n",
    "print(\"Tabela de referência - sem filtro temporal\")\n",
    "print(\"=\" * 60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15a1275f-b361-4b19-a1f8-af24170a02ab",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Configurações"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FUNÇÕES UTILITÁRIAS\n",
    "# =============================================================================\n",
    "\n",
    "def setup_s3_storage():\n",
    "    try:\n",
    "        # Verificar se o diretório existe (criar se necessário)\n",
    "        try:\n",
    "            dbutils.fs.ls(S3_BASE_PATH)\n",
    "            print(\"Diretório do S3 já existe\")\n",
    "        except:\n",
    "            dbutils.fs.mkdirs(S3_BASE_PATH)\n",
    "            print(\"Diretório do S3 criado com sucesso\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao configurar S3 storage: {e}\")\n",
    "        return False\n",
    "\n",
    "def make_api_request(endpoint, params=None, retries=MAX_RETRIES):\n",
    "    url = f\"{API_BASE_URL}/{endpoint}\"\n",
    "    print(f\"Fazendo requisição para: {url}\")\n",
    "    \n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, params=params, timeout=30)\n",
    "            print(f\"Status Code: {response.status_code}\")\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                print(f\"Dados recebidos: {type(data)}\")\n",
    "                if isinstance(data, dict):\n",
    "                    print(f\"Chaves disponíveis: {list(data.keys())}\")\n",
    "                return data\n",
    "            elif response.status_code == 429:  # Rate limit\n",
    "                wait_time = min((attempt + 1) * 5, 60)\n",
    "                print(f\"Rate limit atingido. Aguardando {wait_time}s...\")\n",
    "                time.sleep(wait_time)\n",
    "            elif response.status_code == 503:  # Service unavailable\n",
    "                wait_time = min((attempt + 1) * 10, 120)\n",
    "                print(f\"Serviço indisponível. Aguardando {wait_time}s...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"Erro {response.status_code} na API: {response.text[:200]}\")\n",
    "                if attempt < retries - 1:\n",
    "                    time.sleep(5)\n",
    "                \n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"Timeout na tentativa {attempt + 1}\")\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(10)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            if attempt == retries - 1:\n",
    "                print(f\"Erro na requisição para endpoint após {retries} tentativas: {e}\")\n",
    "                return None\n",
    "            print(f\"Tentativa {attempt + 1} falhou, tentando novamente...\")\n",
    "            time.sleep(1)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Erro ao decodificar JSON na tentativa {attempt + 1}: {e}\")\n",
    "            if attempt == retries - 1:\n",
    "                return None\n",
    "            time.sleep(1)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def clean_types_data(data):\n",
    "    # data é uma lista de strings\n",
    "    cleaned_data = []\n",
    "    for item in data:\n",
    "        if isinstance(item, str):\n",
    "            cleaned_data.append({\"type_name\": item})\n",
    "    return cleaned_data\n",
    "\n",
    "def save_to_parquet(data, table_name):\n",
    "    if not data:\n",
    "        print(f\"Nenhum dado para salvar na tabela {table_name}\")\n",
    "        return None\n",
    "    try:\n",
    "        if table_name == 'types':\n",
    "            schema_fields = [StructField(\"type_name\", StringType(), True)]\n",
    "            schema = StructType(schema_fields)\n",
    "            df = spark.createDataFrame(data, schema)\n",
    "        else:\n",
    "            df = spark.createDataFrame(data)\n",
    "        df = df.withColumn(\"ingestion_timestamp\", current_timestamp()) \\\n",
    "               .withColumn(\"source\", lit(\"mtg_api\")) \\\n",
    "               .withColumn(\"endpoint\", lit(table_name))\n",
    "        df = df.withColumn(\"partition_year\", lit(datetime.now().year)) \\\n",
    "               .withColumn(\"partition_month\", lit(datetime.now().month))\n",
    "        partition_combinations = df.select(\"partition_year\", \"partition_month\").distinct().collect()\n",
    "        for partition_row in partition_combinations:\n",
    "            partition_year = partition_row[\"partition_year\"]\n",
    "            partition_month = partition_row[\"partition_month\"]\n",
    "            partition_df = df.filter((col(\"partition_year\") == partition_year) & \n",
    "                                   (col(\"partition_month\") == partition_month))\n",
    "            file_name = f\"{partition_year}_{partition_month:02d}_{table_name}.parquet\"\n",
    "            file_path = f\"{S3_BASE_PATH}/{file_name}\"\n",
    "            try:\n",
    "                existing_files = dbutils.fs.ls(file_path)\n",
    "                if len(existing_files) > 0:\n",
    "                    print(f\"Arquivo {file_name} já existe - pulando\")\n",
    "                    continue\n",
    "            except:\n",
    "                pass\n",
    "            partition_df.drop(\"partition_year\", \"partition_month\").write.mode(\"overwrite\").format(\"parquet\").save(file_path)\n",
    "            print(f\"Arquivo {file_name} criado com sucesso\")\n",
    "        print(f\"Registros salvos como Parquet para {table_name}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao salvar dados em {table_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "def ingest_simple_data(endpoint, table_name, data_key=None):\n",
    "    print(f\"Iniciando ingestão simples: {table_name}\")\n",
    "    \n",
    "    if data_key is None:\n",
    "        data_key = table_name\n",
    "    \n",
    "    data = make_api_request(endpoint)\n",
    "    \n",
    "    if data and data_key in data:\n",
    "        table_data = data[data_key]\n",
    "        print(f\"Dados obtidos para {table_name}: {len(table_data)} registros\")\n",
    "        \n",
    "        # Limpar dados se for types\n",
    "        if table_name == 'types':\n",
    "            print(\"Limpando dados de types...\")\n",
    "            table_data = clean_types_data(table_data)\n",
    "            print(f\"Dados limpos: {len(table_data)} registros\")\n",
    "        \n",
    "        # Verificar se há dados antes de tentar salvar\n",
    "        if not table_data:\n",
    "            print(f\"Nenhum dado válido para {table_name}\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            df = save_to_parquet(table_data, table_name)\n",
    "            \n",
    "            if df:\n",
    "                count = df.count()\n",
    "                print(f\"{table_name}: {count} registros processados\")\n",
    "                display(df.limit(5))\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao processar dados de {table_name}: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Falha ao obter dados de {table_name}\")\n",
    "        if data:\n",
    "            print(f\"Chaves disponíveis nos dados: {list(data.keys())}\")\n",
    "        return None\n",
    "\n",
    "# Verificar Spark\n",
    "try:\n",
    "    spark\n",
    "    print(\"Spark disponível\")\n",
    "except NameError:\n",
    "    print(\"Spark não está disponível - tentando obter do contexto\")\n",
    "    try:\n",
    "        from pyspark.sql import SparkSession\n",
    "        spark = SparkSession.builder.getOrCreate()\n",
    "        print(\"Spark criado com sucesso\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao criar Spark: {e}\")\n",
    "        raise Exception(\"Spark não está disponível\")\n",
    "\n",
    "# Configurar S3 Storage\n",
    "setup_success = setup_s3_storage()\n",
    "if not setup_success:\n",
    "    raise Exception(\"Falha ao configurar S3 storage\")\n",
    "\n",
    "print(\"Setup concluído com sucesso\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "244c560c-f30e-492e-9f09-4f8035874ebc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Ingestão"
    }
   },
   "outputs": [],
   "source": [
    "# Iniciar ingestão de types\n",
    "print(\"Iniciando ingestão de types...\")\n",
    "\n",
    "types_df = ingest_simple_data(\n",
    "    endpoint=\"types\",\n",
    "    table_name=\"types\"\n",
    ")\n",
    "\n",
    "# Gerar relatório\n",
    "print(\"=\" * 50)\n",
    "print(\"RELATÓRIO DE INGESTÃO DE TYPES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if types_df:\n",
    "    print(\"Arquivos salvos.\")\n",
    "else:\n",
    "    print(\"Falha na ingestão de types\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "types",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

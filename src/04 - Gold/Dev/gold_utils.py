# ============================================================================
# GOLD UTILS - M√≥dulo de Fun√ß√µes Utilit√°rias para Camada Gold
# ============================================================================
"""
M√≥dulo centralizado com fun√ß√µes reutiliz√°veis para scripts da camada Gold.
Inclui fun√ß√µes para Unity Catalog, Secrets, carregamento Delta e logging.

ADAPTADO PARA DATABRICKS NOTEBOOKS:
- dbutils e spark s√£o dispon√≠veis globalmente nos notebooks
- SparkSession obtido automaticamente do contexto global
- Use %run ./gold_utils para importar no notebook

EXEMPLO DE USO NO NOTEBOOK:

OP√á√ÉO 1 - Com Secrets configurados:
%run ./gold_utils
processor = GoldTableProcessor("MINHA_TABELA")

OP√á√ÉO 2 - Configura√ß√£o manual (sem secrets):
%run ./gold_utils
config = create_manual_config("meu_catalog", "s3://meu-bucket")
processor = GoldTableProcessor("MINHA_TABELA", config)

# Usar normalmente:
dfs = processor.load_silver_data(['cards', 'prices'])
processor.save_gold_table(df_final, partition_cols=["DATA_REF"])
"""

import logging
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.sql.window import Window
from delta.tables import DeltaTable
from pyspark.sql.utils import AnalysisException

# ============================================================================
# INICIALIZA√á√ÉO PARA DATABRICKS
# ============================================================================
def get_spark_session():
    """Obt√©m SparkSession do contexto global do Databricks"""
    try:
        return spark  # Dispon√≠vel globalmente no Databricks
    except:
        from pyspark.sql import SparkSession
        return SparkSession.builder.getOrCreate()

# ============================================================================
# CONFIGURA√á√ÉO GLOBAL
# ============================================================================
class GoldConfig:
    """Configura√ß√µes globais para a camada Gold"""
    
    def __init__(self):
        self.spark = get_spark_session()
        self.setup_logging()
        
    def setup_logging(self):
        """Configura logging padr√£o"""
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)

# ============================================================================
# FUN√á√ïES DE SECRETS E CONFIGURA√á√ÉO
# ============================================================================
def get_secret(secret_name, default_value=None):
    """
    Obt√©m segredos do Databricks Secret Scope
    
    Args:
        secret_name (str): Nome do secret
        default_value (str, optional): Valor padr√£o se secret n√£o for encontrado
        
    Returns:
        str: Valor do secret
        
    Raises:
        Exception: Se secret obrigat√≥rio n√£o for encontrado
    """
    try:
        return dbutils.secrets.get(scope="mtg-pipeline", key=secret_name)
    except:
        if default_value is not None:
            print(f"Secret '{secret_name}' n√£o encontrado, usando valor padr√£o: {default_value}")
            return default_value
        else:
            # Valores padr√£o seguros para secrets comuns
            safe_defaults = {
                'catalog_name': 'magic_the_gathering',
                's3_bucket': 's3://meu-bucket-default',
                's3_gold_prefix': 'magic_the_gathering/gold'
            }
            
            if secret_name in safe_defaults:
                print(f"Secret '{secret_name}' n√£o encontrado, usando valor padr√£o: {safe_defaults[secret_name]}")
                return safe_defaults[secret_name]
            else:
                print(f"‚ö†Ô∏è Secret '{secret_name}' n√£o encontrado e sem valor padr√£o")
                print(f"üí° Configure o secret ou use create_manual_config()")
                raise Exception(f"Secret '{secret_name}' not configured and no default available")

def setup_unity_catalog(catalog, schema):
    """
    Configura Unity Catalog criando catalog e schema se necess√°rio
    
    Args:
        catalog (str): Nome do catalog
        schema (str): Nome do schema
        
    Returns:
        bool: True se configura√ß√£o foi bem-sucedida
    """
    spark_session = get_spark_session()
    try:
        spark_session.sql(f"CREATE CATALOG IF NOT EXISTS {catalog}")
        spark_session.sql(f"USE CATALOG {catalog}")
        spark_session.sql(f"CREATE SCHEMA IF NOT EXISTS {schema}")
        spark_session.sql(f"USE SCHEMA {schema}")
        print(f"Schema {catalog}.{schema} configurado com sucesso")
        return True
    except Exception as e:
        print(f"Erro ao configurar Unity Catalog: {e}")
        return False

def get_standard_config():
    """
    Retorna configura√ß√£o padr√£o para scripts Gold com valores padr√£o seguros
    
    Returns:
        dict: Dicion√°rio com configura√ß√µes padr√£o
    """
    # Valores padr√£o seguros para desenvolvimento/teste
    defaults = {
        'catalog_name': 'magic_the_gathering',
        's3_bucket': 's3://meu-bucket-default',
        's3_gold_prefix': 'magic_the_gathering/gold'
    }
    
    config = {}
    for key, default_value in defaults.items():
        try:
            config[key] = dbutils.secrets.get(scope="mtg-pipeline", key=key)
            print(f"Secret '{key}' configurado: {config[key]}")
        except:
            config[key] = default_value
            print(f"Secret '{key}' n√£o encontrado, usando padr√£o: {default_value}")
    
    # Configura√ß√µes fixas
    config['schema_silver'] = "silver"
    config['schema_gold'] = "gold"
    
    return config

def create_manual_config(catalog_name, s3_bucket, s3_gold_prefix=None):
    """
    Cria configura√ß√£o manual sem usar secrets (para testes/desenvolvimento)
    
    Args:
        catalog_name (str): Nome do catalog Unity
        s3_bucket (str): Bucket S3 (com s3://)
        s3_gold_prefix (str, optional): Prefixo para Gold layer
        
    Returns:
        dict: Configura√ß√£o manual
        
    Example:
        config = create_manual_config("meu_catalog", "s3://meu-bucket")
        processor = GoldTableProcessor("MINHA_TABELA", config)
    """
    return {
        'catalog_name': catalog_name,
        'schema_silver': "silver",
        'schema_gold': "gold",
        's3_bucket': s3_bucket,
        's3_gold_prefix': s3_gold_prefix or "magic_the_gathering/gold"
    }

# ============================================================================
# FUN√á√ïES DE CARREGAMENTO DELTA/UNITY CATALOG
# ============================================================================
def load_to_gold_unity_incremental(df_final, catalog, schema, table_name, s3_gold_path, 
                                  partition_cols=None, mode="overwrite"):
    """
    Carrega dados na camada Gold com suporte a Unity Catalog e Delta Lake
    
    Args:
        df_final (DataFrame): DataFrame final para salvar
        catalog (str): Nome do catalog Unity
        schema (str): Nome do schema Unity
        table_name (str): Nome da tabela
        s3_gold_path (str): Caminho S3 base para Gold
        partition_cols (list, optional): Colunas para particionamento
        mode (str): Modo de escrita (overwrite, append)
    """
    delta_path = f"s3://{s3_gold_path}/{table_name}"
    full_table_name = f"{catalog}.{schema}.{table_name}"
    
    print(f"Salvando dados em: {delta_path}")
    print(f"Qtd linhas df_final: {df_final.count()}")
    
    try:
        # Configurar writer
        writer = df_final.write.format("delta") \
                        .mode(mode) \
                        .option("overwriteSchema", "true")
        
        # Adicionar particionamento se especificado
        if partition_cols:
            writer = writer.partitionBy(*partition_cols)
            
        # Salvar dados
        writer.save(delta_path)
        
        # Criar/atualizar tabela Unity Catalog
        spark_session = get_spark_session()
        try:
            spark_session.sql(f"SELECT 1 FROM {full_table_name} LIMIT 1")
            print(f"Tabela Unity Catalog '{full_table_name}' j√° existe")
        except:
            spark_session.sql(f"CREATE TABLE {full_table_name} USING DELTA LOCATION '{delta_path}'")
            print(f"Tabela Unity Catalog criada: {full_table_name}")
        
        print("Dados salvos com sucesso!")
        
    except Exception as e:
        print(f"Erro ao salvar dados: {e}")
        raise

# ============================================================================
# FUN√á√ïES DE CARREGAMENTO DE DADOS SILVER
# ============================================================================
def load_silver_tables(config, table_list=None):
    """
    Carrega tabelas Silver padr√£o com aliases
    
    Args:
        config (dict): Configura√ß√£o com catalog e schemas
        table_list (list, optional): Lista espec√≠fica de tabelas para carregar
        
    Returns:
        dict: Dicion√°rio com DataFrames carregados
    """
    spark_session = get_spark_session()
    catalog = config['catalog_name']
    schema_silver = config['schema_silver']
    
    # Tabelas padr√£o
    default_tables = {
        'cards': f"{catalog}.{schema_silver}.TB_FATO_SILVER_CARDS",
        'prices': f"{catalog}.{schema_silver}.TB_FATO_SILVER_CARDPRICES", 
        'sets': f"{catalog}.{schema_silver}.TB_REF_SILVER_SETS",
        'types': f"{catalog}.{schema_silver}.TB_REF_SILVER_TYPES",
        'subtypes': f"{catalog}.{schema_silver}.TB_REF_SILVER_SUBTYPES",
        'supertypes': f"{catalog}.{schema_silver}.TB_REF_SILVER_SUPERTYPES"
    }
    
    # Usar lista espec√≠fica se fornecida
    if table_list:
        tables_to_load = {k: v for k, v in default_tables.items() if k in table_list}
    else:
        tables_to_load = default_tables
    
    # Carregar tabelas com aliases
    dataframes = {}
    for alias, table_name in tables_to_load.items():
        try:
            dataframes[alias] = spark_session.table(table_name).alias(alias)
            print(f"Tabela carregada: {alias} -> {table_name}")
        except Exception as e:
            print(f"Erro ao carregar {table_name}: {e}")
            
    return dataframes

# ============================================================================
# FUN√á√ïES DE TRANSFORMA√á√ÉO COMUM
# ============================================================================
def resolve_column_ambiguity(df, ambiguous_cols_map):
    """
    Resolve ambiguidades de colunas selecionando explicitamente
    
    Args:
        df (DataFrame): DataFrame com poss√≠veis ambiguidades
        ambiguous_cols_map (dict): Mapeamento de colunas amb√≠guas
        
    Returns:
        DataFrame: DataFrame com colunas resolvidas
        
    Example:
        ambiguous_cols_map = {
            'NME_RARITY': 'cards.NME_RARITY',
            'DT_INGESTION': 'prices.DT_INGESTION'
        }
    """
    select_cols = []
    for col_name in df.columns:
        if col_name in ambiguous_cols_map:
            # Usar mapeamento espec√≠fico
            select_cols.append(col(ambiguous_cols_map[col_name]).alias(col_name))
        else:
            # Usar coluna direta
            select_cols.append(col_name)
    
    return df.select(*select_cols)

def add_data_reference(df, ref_type="current", year_col=None, month_col=None):
    """
    Adiciona coluna DATA_REF baseada no tipo especificado
    
    Args:
        df (DataFrame): DataFrame base
        ref_type (str): Tipo de refer√™ncia ('current', 'release', 'ingestion')
        year_col (str): Nome da coluna de ano (para ref_type='release')
        month_col (str): Nome da coluna de m√™s (para ref_type='release')
        
    Returns:
        DataFrame: DataFrame com DATA_REF adicionado
    """
    if ref_type == "current":
        return df.withColumn("DATA_REF", current_date())
    elif ref_type == "release" and year_col and month_col:
        return df.withColumn("DATA_REF", make_date(col(year_col), col(month_col), lit(1)))
    elif ref_type == "ingestion":
        return df.withColumn("DATA_REF", to_date(col("DT_INGESTION")))
    else:
        raise ValueError(f"Tipo de refer√™ncia inv√°lido: {ref_type}")

# ============================================================================
# CLASSES AUXILIARES
# ============================================================================
class GoldTableProcessor:
    """
    Classe para processar tabelas Gold com padr√µes comuns
    """
    
    def __init__(self, table_name, config=None):
        self.table_name = table_name
        self.config = config or get_standard_config()
        self.spark = get_spark_session()
        self.s3_gold_path = f"{self.config['s3_bucket']}/{self.config['s3_gold_prefix']}"
        
        # Setup Unity Catalog
        setup_unity_catalog(self.config['catalog_name'], self.config['schema_gold'])
    
    def load_silver_data(self, tables):
        """Carrega dados Silver necess√°rios"""
        return load_silver_tables(self.config, tables)
    
    def save_gold_table(self, df, partition_cols=None):
        """Salva tabela na Gold com configura√ß√µes padr√£o"""
        load_to_gold_unity_incremental(
            df_final=df,
            catalog=self.config['catalog_name'],
            schema=self.config['schema_gold'],
            table_name=self.table_name,
            s3_gold_path=self.s3_gold_path,
            partition_cols=partition_cols
        )
        
        print(f"‚úÖ {self.table_name} criada com sucesso!")
        print(f"Tabela criada: {self.config['catalog_name']}.{self.config['schema_gold']}.{self.table_name}")
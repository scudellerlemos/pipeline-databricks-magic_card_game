{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1205f433-1b8f-436b-a175-da829f6a3597",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Camada Silver - Cards - Magic: The Gathering\n",
    "# Pipeline 100% PySpark DataFrame API\n",
    "# Modularizado seguindo padrão Bronze\n",
    "\n",
    "# =============================================================================\n",
    "# BIBLIOTECAS UTILIZADAS\n",
    "# =============================================================================\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.sql.functions import udf, year, month, initcap, upper, coalesce, when, lit, split, size, regexp_replace, trim, translate, md5, lower\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7760cd96-d446-430f-abab-1a0b0c22b968",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FUNÇÕES UTILITÁRIAS\n",
    "# =============================================================================\n",
    "def get_secret(secret_name, default_value=None):\n",
    "    try:\n",
    "        return dbutils.secrets.get(scope=\"mtg-pipeline\", key=secret_name)\n",
    "    except:\n",
    "        if default_value is not None:\n",
    "            print(f\"Secret '{secret_name}' não encontrado, usando valor padrão\")\n",
    "            return default_value\n",
    "        else:\n",
    "            print(f\"Secret obrigatório '{secret_name}' não encontrado\")\n",
    "            raise Exception(f\"Required secret '{secret_name}' not configured\")\n",
    "\n",
    "def setup_unity_catalog(catalog, schema):\n",
    "    try:\n",
    "        spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog}\")\n",
    "        spark.sql(f\"USE CATALOG {catalog}\")\n",
    "        spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema}\")\n",
    "        spark.sql(f\"USE SCHEMA {schema}\")\n",
    "        print(f\"Schema {catalog}.{schema} criado ou já existente.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao configurar Unity Catalog: {e}\")\n",
    "        return False\n",
    "\n",
    "# =============================================================================\n",
    "# EXTRAÇÃO\n",
    "# =============================================================================\n",
    "def extract_from_bronze(catalog, table_name):\n",
    "    bronze_table = f\"{catalog}.bronze.TB_BRONZE_CARDS\"\n",
    "    df = spark.table(bronze_table)\n",
    "    return df\n",
    "\n",
    "# =============================================================================\n",
    "# TRANSFORMAÇÃO\n",
    "# =============================================================================\n",
    "def transform_cards_silver(df):\n",
    "    # Filtro temporal (últimos 5 anos)\n",
    "    df = df.filter(col(\"DT_INGESTION\") >= add_months(current_date(), -12*5))\n",
    "\n",
    "    # Padronização de nomes (Title Case)\n",
    "    df = df.withColumn(\"NME_CARD\", initcap(col(\"NME_CARD\")))\n",
    "    df = df.withColumn(\"NME_ARTIST\", initcap(col(\"NME_ARTIST\")))\n",
    "    df = df.withColumn(\"NME_RARITY\", initcap(col(\"NME_RARITY\")))\n",
    "    df = df.withColumn(\"NME_SET\", initcap(col(\"NME_SET\")))\n",
    "    df = df.withColumn(\"COD_SET\", upper(col(\"COD_SET\")))\n",
    "\n",
    "    # Limpeza e tratamento de nulos\n",
    "    df = df.withColumn(\"DESC_MANA_COST\", when((col(\"DESC_MANA_COST\").isNull()) | (col(\"DESC_MANA_COST\") == \"\") | (col(\"DESC_MANA_COST\") == \"null\"), lit(\"0\")).otherwise(trim(col(\"DESC_MANA_COST\"))))\n",
    "    df = df.withColumn(\"MANA_COST\", coalesce(col(\"MANA_COST\"), lit(0.0)))\n",
    "    df = df.withColumn(\"NME_POWER\", coalesce(col(\"NME_POWER\").cast(\"int\"), lit(0)))\n",
    "    df = df.withColumn(\"NME_TOUGHNESS\", coalesce(col(\"NME_TOUGHNESS\").cast(\"int\"), lit(0)))\n",
    "\n",
    "    # Derivação de NME_CARD_TYPE e DESC_CARD_TYPE\n",
    "    def split_card_type(card_type):\n",
    "        if card_type is None:\n",
    "            return (None, None)\n",
    "        if \"planeswalker\" in card_type.lower():\n",
    "            return (\"Planeswalker\", card_type)\n",
    "        if \"—\" in card_type:\n",
    "            parts = card_type.split(\"—\", 1)\n",
    "            return (parts[0].strip(), parts[1].strip())\n",
    "        return (card_type.strip(), \"NA\")\n",
    "    from pyspark.sql.functions import udf\n",
    "    from pyspark.sql.types import StructType, StructField, StringType\n",
    "    split_card_type_udf = udf(split_card_type, StructType([\n",
    "        StructField(\"NME_CARD_TYPE\", StringType()),\n",
    "        StructField(\"DESC_CARD_TYPE\", StringType())\n",
    "    ]))\n",
    "    df = df.withColumn(\"_CARD_TYPE_STRUCT\", split_card_type_udf(col(\"NME_CARD_TYPE\")))\n",
    "    df = df.withColumn(\"NME_CARD_TYPE\", col(\"_CARD_TYPE_STRUCT.NME_CARD_TYPE\"))\n",
    "    df = df.withColumn(\"DESC_CARD_TYPE\", col(\"_CARD_TYPE_STRUCT.DESC_CARD_TYPE\"))\n",
    "    df = df.drop(\"_CARD_TYPE_STRUCT\")\n",
    "\n",
    "    # Limpeza de colunas tipo array/JSON para string simples\n",
    "    from pyspark.sql.functions import regexp_replace\n",
    "    df = df.withColumn(\"NME_PRINTINGS\", regexp_replace(col(\"NME_PRINTINGS\"), r'\\[|\\]|\"', \"\"))\n",
    "    df = df.withColumn(\"COD_COLORS\", regexp_replace(col(\"COD_COLORS\"), r'\\[|\\]|\"', \"\"))\n",
    "    df = df.withColumn(\"COD_COLOR_IDENTITY\", regexp_replace(col(\"COD_COLOR_IDENTITY\"), r'\\[|\\]|\"', \"\"))\n",
    "    df = df.withColumn(\"DESC_SUBTYPES\", regexp_replace(col(\"DESC_SUBTYPES\"), r'\\[|\\]|\"', \"\"))\n",
    "    df = df.withColumn(\"NME_ORIGINAL_TYPE\", when(col(\"NME_ORIGINAL_TYPE\").isNull(), lit(\"NA\")).otherwise(col(\"NME_ORIGINAL_TYPE\")))\n",
    "\n",
    "    # COD_COLORS como string separada por vírgula\n",
    "    df = df.withColumn(\n",
    "        \"COD_COLORS\",\n",
    "        when(col(\"COD_COLORS\").isNull() | (col(\"COD_COLORS\") == \"\") | (col(\"COD_COLORS\") == '[\"\"]'), lit(\"Colorless\")).otherwise(col(\"COD_COLORS\"))\n",
    "    )\n",
    "\n",
    "    # NME_COLOR_CATEGORY\n",
    "    df = df.withColumn(\n",
    "        \"NME_COLOR_CATEGORY\",\n",
    "        when(col(\"COD_COLORS\") == \"Colorless\", \"Colorless\")\n",
    "        .when(size(split(col(\"COD_COLORS\"), \",\")) == 1, \"Mono\")\n",
    "        .when(size(split(col(\"COD_COLORS\"), \",\")) == 2, \"Dual Color\")\n",
    "        .when(size(split(col(\"COD_COLORS\"), \",\")) >= 3, \"Multicolor\")\n",
    "        .otherwise(\"Mono\")\n",
    "    )\n",
    "\n",
    "    # QTY_COLORS: número de cores diferentes no custo de mana\n",
    "    df = df.withColumn(\n",
    "        \"QTY_COLORS\",\n",
    "        when((col(\"DESC_MANA_COST\").isNull()) | (col(\"DESC_MANA_COST\") == \"0\") | (col(\"DESC_MANA_COST\") == \"1\"), lit(0))\n",
    "        .when(regexp_replace(col(\"DESC_MANA_COST\"), \"^[0-9X]+$\", \"\") == \"\", lit(0))\n",
    "        .otherwise(length(regexp_replace(upper(col(\"DESC_MANA_COST\")), \"[^WUBRG]\", \"\")))\n",
    "    )\n",
    "\n",
    "    # DESC_TYPES e DESC_SUBTYPES como string separada por vírgula\n",
    "    df = df.withColumn(\"DESC_TYPES\", when(col(\"DESC_TYPES\").isNull() | (col(\"DESC_TYPES\") == \"\"), lit(\"NA\")).otherwise(col(\"DESC_TYPES\")))\n",
    "    df = df.withColumn(\n",
    "        \"DESC_SUBTYPES\",\n",
    "        when(col(\"DESC_SUBTYPES\").isNull() | (col(\"DESC_SUBTYPES\") == \"\") | (col(\"DESC_SUBTYPES\") == '[\"\"]'), lit(\"NA\")).otherwise(col(\"DESC_SUBTYPES\"))\n",
    "    )\n",
    "\n",
    "    # Substituições e normalização em DESC_CARD\n",
    "    desc_card_clean = regexp_replace(col(\"DESC_CARD\"), r\"\\{W\\}\", \"[White]\")\n",
    "    desc_card_clean = regexp_replace(desc_card_clean, r\"\\{U\\}\", \"[Blue]\")\n",
    "    desc_card_clean = regexp_replace(desc_card_clean, r\"\\{B\\}\", \"[Black]\")\n",
    "    desc_card_clean = regexp_replace(desc_card_clean, r\"\\{R\\}\", \"[Red]\")\n",
    "    desc_card_clean = regexp_replace(desc_card_clean, r\"\\{G\\}\", \"[Green]\")\n",
    "    desc_card_clean = regexp_replace(desc_card_clean, r\"\\{C\\}\", \"[Colorless]\")\n",
    "    desc_card_clean = regexp_replace(desc_card_clean, r\"\\{X\\}\", \"[X]\")\n",
    "    desc_card_clean = regexp_replace(desc_card_clean, r\"\\{T\\}\", \"[Tap]\")\n",
    "    desc_card_clean = regexp_replace(desc_card_clean, r\"\\{Q\\}\", \"[Untap]\")\n",
    "    desc_card_clean = regexp_replace(desc_card_clean, r\"\\{S\\}\", \"[Snow]\")\n",
    "    desc_card_clean = regexp_replace(desc_card_clean, r\"\\{E\\}\", \"[Energy]\")\n",
    "    desc_card_clean = regexp_replace(desc_card_clean, r\"\\{0\\}\", \"[0]\")\n",
    "    desc_card_clean = regexp_replace(desc_card_clean, r\"\\{1\\}\", \"[1]\")\n",
    "    desc_card_clean = translate(\n",
    "        desc_card_clean,\n",
    "        \"áàãâäéèêëíìîïóòõôöúùûüçÁÀÃÂÄÉÈÊËÍÌÎÏÓÒÕÔÖÚÙÛÜÇ\",\n",
    "        \"aaaaaeeeeiiiiooooouuuucAAAAAEEEEIIIIOOOOOUUUUC\"\n",
    "    )\n",
    "    desc_card_clean = trim(desc_card_clean)\n",
    "    df = df.withColumn(\"DESC_CARD\", desc_card_clean)\n",
    "    df = df.withColumn(\n",
    "        \"DESC_CARD\",\n",
    "        when(col(\"DESC_CARD\").isNull() | (col(\"DESC_CARD\") == \"null\"), lit(\"NA\")).otherwise(col(\"DESC_CARD\"))\n",
    "    )\n",
    "\n",
    "    # Adiciona colunas de partição\n",
    "    df = df.withColumn(\"ANO_PART\", year(col(\"DT_INGESTION\")))\n",
    "    df = df.withColumn(\"MES_PART\", month(col(\"DT_INGESTION\")))\n",
    "\n",
    "    # Conversão explícita dos tipos\n",
    "    df = df.withColumn(\"DT_INGESTION\", col(\"DT_INGESTION\").cast(\"date\"))\n",
    "    df = df.withColumn(\"MANA_COST\", col(\"MANA_COST\").cast(\"int\"))\n",
    "\n",
    "    # Seleção final de colunas\n",
    "    colunas_finais = [\n",
    "        \"NME_PRINTINGS\", \"NME_ORIGINAL_TYPE\", \"ID_CARD\", \"DT_INGESTION\", \"NME_SOURCE\", \"NME_ENDPOINT\",\n",
    "        \"RELEASE_YEAR\", \"RELEASE_MONTH\", \"NME_CARD\", \"DESC_MANA_COST\", \"MANA_COST\", \"COD_COLORS\",\n",
    "        \"COD_COLOR_IDENTITY\", \"NME_CARD_TYPE\", \"DESC_SUBTYPES\", \"NME_RARITY\", \"COD_SET\", \"NME_SET\",\n",
    "        \"DESC_CARD\", \"NME_ARTIST\", \"COD_NUMBER\", \"NME_POWER\", \"NME_TOUGHNESS\", \"NME_LAYOUT\",\n",
    "        \"URL_IMAGE\", \"NME_COLOR_CATEGORY\", \"QTY_COLORS\", \"ANO_PART\", \"MES_PART\"\n",
    "    ]\n",
    "    df_final = df.select(*colunas_finais)\n",
    "    return df_final\n",
    "\n",
    "# =============================================================================\n",
    "# CARGA (WRITE/MERGE)\n",
    "# =============================================================================\n",
    "def delta_table_exists_and_schema_ok(spark, delta_path, df_final):\n",
    "    try:\n",
    "        delta_table = DeltaTable.forPath(spark, delta_path)\n",
    "        current_schema = set([f.name for f in delta_table.toDF().schema.fields])\n",
    "        new_schema = set([f.name for f in df_final.schema.fields])\n",
    "        if current_schema != new_schema:\n",
    "            return False, None\n",
    "        return True, delta_table\n",
    "    except Exception:\n",
    "        return False, None\n",
    "\n",
    "def load_to_silver_unity_incremental(df_final, catalog, schema, table_name, s3_silver_path):\n",
    "    delta_path = f\"s3://{s3_silver_path}/{table_name}\"\n",
    "    full_table_name = f\"{catalog}.{schema}.{table_name}\"\n",
    "    print(f\"Salvando dados em: {delta_path}\")\n",
    "    print(\"Qtd linhas df_final:\", df_final.count())\n",
    "    print(\"Colunas df_final:\", df_final.columns)\n",
    "    print(\"delta_path:\", delta_path)\n",
    "\n",
    "    exists, delta_table = delta_table_exists_and_schema_ok(spark, delta_path, df_final)\n",
    "    if not exists:\n",
    "        print(\"Tabela Delta não existe ou schema mudou. Salvando com overwrite e overwriteSchema=True.\")\n",
    "        try:\n",
    "            df_final.write.format(\"delta\") \\\n",
    "                .mode(\"overwrite\") \\\n",
    "                .option(\"overwriteSchema\", \"true\") \\\n",
    "                .partitionBy(\"ANO_PART\", \"MES_PART\") \\\n",
    "                .save(delta_path)\n",
    "            print(\"Write Delta concluído com sucesso!\")\n",
    "        except Exception as e:\n",
    "            print(\"Erro no write Delta:\", e)\n",
    "            raise\n",
    "    else:\n",
    "        print(\"Tabela Delta já existe e schema é igual. Executando merge incremental por ID_CARD.\")\n",
    "        count_antes = delta_table.toDF().count()\n",
    "        df_final = df_final.dropDuplicates([\"ID_CARD\"])\n",
    "        update_cols = [c for c in df_final.columns if c != \"ID_CARD\"]\n",
    "        set_expr = {col: f\"novo.{col}\" for col in update_cols}\n",
    "        merge_result = delta_table.alias(\"silver\").merge(\n",
    "            df_final.alias(\"novo\"),\n",
    "            \"silver.ID_CARD = novo.ID_CARD\"\n",
    "        ).whenMatchedUpdate(set=set_expr) \\\n",
    "         .whenNotMatchedInsertAll() \\\n",
    "         .execute()\n",
    "        count_depois = delta_table.toDF().count()\n",
    "        print(f\"Linhas antes do merge: {count_antes}\")\n",
    "        print(f\"Linhas depois do merge: {count_depois}\")\n",
    "        print(f\"Linhas adicionadas (diferença): {count_depois - count_antes}\")\n",
    "        # Exibe estatísticas do merge incremental\n",
    "        print(\"Estatísticas do merge incremental:\")\n",
    "        # Tenta acessar os principais atributos, se existirem\n",
    "        for attr in [\n",
    "            \"insertedRowsCount\", \"updatedRowsCount\", \"deletedRowsCount\",\n",
    "            \"copiedRowsCount\", \"sourceRowsCount\", \"outputRowsCount\"\n",
    "        ]:\n",
    "            if hasattr(merge_result, attr):\n",
    "                print(f\"{attr}: {getattr(merge_result, attr)}\")\n",
    "    print(\"Dados salvos com sucesso na camada Silver!\")\n",
    "\n",
    "    # Criação/atualização da tabela no Unity Catalog\n",
    "    try:\n",
    "        spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{schema}\")\n",
    "        print(f\"Schema {catalog}.{schema} criado ou já existente.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao criar schema: {e}\")\n",
    "    try:\n",
    "        from pyspark.sql.utils import AnalysisException\n",
    "        # Verifica se a tabela já existe\n",
    "        if spark.catalog.tableExists(full_table_name):\n",
    "            # Pega o schema atual da tabela\n",
    "            existing_schema = spark.table(full_table_name).schema\n",
    "            # Função para comparar apenas nome e tipo das colunas (ignorando ordem e nullable)\n",
    "            def schema_to_set(schema):\n",
    "                return set((f.name.lower(), str(f.dataType).lower()) for f in schema.fields)\n",
    "            if schema_to_set(existing_schema) == schema_to_set(df_final.schema):\n",
    "                print(f\"Tabela {full_table_name} já existe e schema é igual. Não será recriada.\")\n",
    "            else:\n",
    "                print(f\"Tabela {full_table_name} existe mas schema é diferente. Será recriada.\")\n",
    "                spark.sql(f\"DROP TABLE IF EXISTS {full_table_name}\")\n",
    "                spark.sql(f'''\\\n",
    "                    CREATE TABLE {full_table_name} (\n",
    "                        NME_PRINTINGS STRING,\n",
    "                        NME_ORIGINAL_TYPE STRING,\n",
    "                        ID_CARD STRING,\n",
    "                        DT_INGESTION DATE,\n",
    "                        NME_SOURCE STRING,\n",
    "                        NME_ENDPOINT STRING,\n",
    "                        RELEASE_YEAR INT,\n",
    "                        RELEASE_MONTH INT,\n",
    "                        NME_CARD STRING,\n",
    "                        DESC_MANA_COST STRING,\n",
    "                        MANA_COST INT,\n",
    "                        COD_COLORS STRING,\n",
    "                        COD_COLOR_IDENTITY STRING,\n",
    "                        NME_CARD_TYPE STRING,\n",
    "                        DESC_SUBTYPES STRING,\n",
    "                        NME_RARITY STRING,\n",
    "                        COD_SET STRING,\n",
    "                        NME_SET STRING,\n",
    "                        DESC_CARD STRING,\n",
    "                        NME_ARTIST STRING,\n",
    "                        COD_NUMBER STRING,\n",
    "                        NME_POWER INT,\n",
    "                        NME_TOUGHNESS INT,\n",
    "                        NME_LAYOUT STRING,\n",
    "                        URL_IMAGE STRING,\n",
    "                        NME_COLOR_CATEGORY STRING,\n",
    "                        QTY_COLORS INT,\n",
    "                        ANO_PART INT,\n",
    "                        MES_PART INT\n",
    "                    )\n",
    "                    USING DELTA\n",
    "                    PARTITIONED BY (ANO_PART, MES_PART)\n",
    "                    LOCATION '{delta_path}'\n",
    "                ''')\n",
    "                print(f\"Tabela Unity Catalog criada com particionamento explícito: {full_table_name}\")\n",
    "        else:\n",
    "            print(f\"Tabela {full_table_name} não existe. Será criada.\")\n",
    "            spark.sql(f'''\\\n",
    "                CREATE TABLE {full_table_name} (\n",
    "                    NME_PRINTINGS STRING,\n",
    "                    NME_ORIGINAL_TYPE STRING,\n",
    "                    ID_CARD STRING,\n",
    "                    DT_INGESTION DATE,\n",
    "                    NME_SOURCE STRING,\n",
    "                    NME_ENDPOINT STRING,\n",
    "                    RELEASE_YEAR INT,\n",
    "                    RELEASE_MONTH INT,\n",
    "                    NME_CARD STRING,\n",
    "                    DESC_MANA_COST STRING,\n",
    "                    MANA_COST INT,\n",
    "                    COD_COLORS STRING,\n",
    "                    COD_COLOR_IDENTITY STRING,\n",
    "                    NME_CARD_TYPE STRING,\n",
    "                    DESC_SUBTYPES STRING,\n",
    "                    NME_RARITY STRING,\n",
    "                    COD_SET STRING,\n",
    "                    NME_SET STRING,\n",
    "                    DESC_CARD STRING,\n",
    "                    NME_ARTIST STRING,\n",
    "                    COD_NUMBER STRING,\n",
    "                    NME_POWER INT,\n",
    "                    NME_TOUGHNESS INT,\n",
    "                    NME_LAYOUT STRING,\n",
    "                    URL_IMAGE STRING,\n",
    "                    NME_COLOR_CATEGORY STRING,\n",
    "                    QTY_COLORS INT,\n",
    "                    ANO_PART INT,\n",
    "                    MES_PART INT\n",
    "                )\n",
    "                USING DELTA\n",
    "                PARTITIONED BY (ANO_PART, MES_PART)\n",
    "                LOCATION '{delta_path}'\n",
    "            ''')\n",
    "            print(f\"Tabela Unity Catalog criada com particionamento explícito: {full_table_name}\")\n",
    "    except AnalysisException as e:\n",
    "        print(f\"Erro ao criar tabela no Unity Catalog: {e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "615fbe3c-2226-42ec-826f-cd47da353549",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PIPELINE PRINCIPAL\n",
    "# =============================================================================\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "CATALOG_NAME = get_secret(\"catalog_name\")\n",
    "SCHEMA_NAME = \"silver\"\n",
    "TABLE_NAME = \"TB_FATO_SILVER_CARDS\"\n",
    "S3_BUCKET = get_secret(\"s3_bucket\")\n",
    "S3_SILVER_PREFIX = get_secret(\"s3_silver_prefix\", \"magic_the_gathering/silver\")\n",
    "S3_SILVER_PATH = f\"{S3_BUCKET}/{S3_SILVER_PREFIX}\"\n",
    "\n",
    "setup_unity_catalog(CATALOG_NAME, SCHEMA_NAME)\n",
    "df_bronze = extract_from_bronze(CATALOG_NAME, TABLE_NAME)\n",
    "df_final = transform_cards_silver(df_bronze)\n",
    "load_to_silver_unity_incremental(df_final, CATALOG_NAME, SCHEMA_NAME, TABLE_NAME, S3_SILVER_PATH)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5522017842078681,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "TB_FATO_SILVER_CARDS",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

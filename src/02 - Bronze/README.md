# ğŸ¥‰ Camada Bronze - Magic: The Gathering

<div align="center">

![RED XIII - Proud Warrior](https://repositorio.sbrauble.com/arquivos/in/magic/480717/6824dbcfba786-4us35-hj5ls-0e0603650198f950cad99859423dd8cf.jpg)

*"Through the fires of transformation, raw data emerges as structured wisdom."* - RED XIII, Magic: The Gathering - FF Edition

</div>

## ğŸ“‹ VisÃ£o Geral

Esta pasta contÃ©m os notebooks responsÃ¡veis pela **camada Bronze** do pipeline de dados do Magic: The Gathering. A camada Bronze realiza o processo **EL (Extract & Load)**, carregando dados brutos da staging em dados estruturados e organizados no Unity Catalog com Delta Lake.

## ğŸ¯ Objetivo

Carregar dados da camada de staging (S3/Parquet) em dados estruturados na camada Bronze (Unity Catalog/Delta), garantindo:
- **Extract**: Leitura eficiente dos dados de staging
- **Load**: Carregamento incremental com merge inteligente
- **GovernanÃ§a**: Controle atravÃ©s do Unity Catalog
- **Performance**: OtimizaÃ§Ã£o com Delta Lake
- **Rastreabilidade**: HistÃ³rico completo de mudanÃ§as

## ğŸ”„ Processo EL (Extract & Load)

### **Extract - ExtraÃ§Ã£o da Staging**
```python
def extract_from_staging(table_name):
    stage_path = f"{S3_STAGE_PATH}/*_{table_name}.parquet"
    df = spark.read.parquet(stage_path)
    return df
```

### **Load - Carregamento na Bronze**
```python
def load_to_bronze_unity_merge(df, table_name):
    # Merge incremental com Delta Lake
    delta_table.alias("bronze").merge(
        df.alias("novo"),
        "bronze.id = novo.id"  # ou chave especÃ­fica
    ).whenMatchedUpdateAll() \
     .whenNotMatchedInsertAll() \
     .execute()
```

## ğŸ“ Estrutura dos Notebooks

### ğŸƒ `Cards.ipynb`
- **Fonte**: Dados de cartas da staging
- **Chave**: `id` (identificador Ãºnico da carta)
- **CaracterÃ­sticas**: 
  - Merge incremental por ID
  - Filtro temporal de 5 anos
  - Schema complexo com 25+ campos
  - Tratamento de arrays JSON
- **Tipo**: ğŸ´ Creature/Spell/Artifact (dados temporais)

### ğŸ´ `Sets.ipynb`
- **Fonte**: Dados de expansÃµes da staging
- **Chave**: `code` (cÃ³digo da expansÃ£o)
- **CaracterÃ­sticas**:
  - Merge incremental por cÃ³digo
  - Filtro temporal de 5 anos
  - Campo booster explodido em mÃºltiplas colunas
  - Metadados de lanÃ§amento
- **Tipo**: ğŸ“¦ Expansion Set (dados temporais)

### ğŸ·ï¸ `Types.ipynb`
- **Fonte**: Dados de tipos da staging
- **Chave**: `type_name` (nome do tipo)
- **CaracterÃ­sticas**:
  - Dados de referÃªncia estÃ¡ticos
  - Sem filtro temporal
  - Schema simples (1 campo principal)
  - Compatibilidade com schemas antigos
- **Tipo**: ğŸ·ï¸ Reference Card (dados estÃ¡ticos)

### â­ `SuperTypes.ipynb`
- **Fonte**: Dados de super tipos da staging
- **Chave**: `supertype_name` (nome do super tipo)
- **CaracterÃ­sticas**:
  - Dados de referÃªncia estÃ¡ticos
  - Sem filtro temporal
  - Schema simples (1 campo principal)
- **Tipo**: â­ Reference Card (dados estÃ¡ticos)

### ğŸ”– `SubTypes.ipynb`
- **Fonte**: Dados de sub tipos da staging
- **Chave**: `subtype_name` (nome do sub tipo)
- **CaracterÃ­sticas**:
  - Dados de referÃªncia estÃ¡ticos
  - Sem filtro temporal
  - Schema simples (1 campo principal)
- **Tipo**: ğŸ”– Reference Card (dados estÃ¡ticos)

### ğŸ® `Formats.ipynb`
- **Fonte**: Dados de formatos da staging
- **Chave**: `format_name` (nome do formato)
- **CaracterÃ­sticas**:
  - Dados de referÃªncia estÃ¡ticos
  - Sem filtro temporal
  - Schema simples (1 campo principal)
- **Tipo**: ğŸ® Reference Card (dados estÃ¡ticos)

### ğŸ’° `Card_Prices.ipynb`
- **Fonte**: Dados de preÃ§os da staging (Scryfall API)
- **Chave**: `name` (nome da carta)
- **CaracterÃ­sticas**:
  - Dados de preÃ§os em tempo real (USD, EUR, TIX)
  - Merge incremental por nome da carta
  - Particionamento por `ingestion_timestamp`
  - DependÃªncia: Requer dados de cards jÃ¡ processados
  - Fonte de dados: Scryfall API (diferente da MTG API)
- **Tipo**: ğŸ’° Market Data (dados dinÃ¢micos)

## âš™ï¸ ConfiguraÃ§Ãµes NecessÃ¡rias

### ğŸ” Segredos do Databricks
Configure os seguintes segredos no scope `mtg-pipeline`:

```python
# Unity Catalog Configuration
catalog_name           # Nome do catÃ¡logo Unity
s3_bucket             # Bucket S3 para armazenamento
s3_stage_prefix       # Prefixo da camada staging
s3_bronze_prefix      # Prefixo da camada bronze

# Temporal Configuration
years_back            # Anos para trÃ¡s no filtro temporal (padrÃ£o: 5)
```

### Estrutura Unity Catalog
```
{catalog_name}/
â””â”€â”€ bronze/
    â”œâ”€â”€ cards
    â”œâ”€â”€ sets
    â”œâ”€â”€ types
    â”œâ”€â”€ supertypes
    â”œâ”€â”€ subtypes
    â””â”€â”€ formats
```

## ğŸ”„ Fluxo de ExecuÃ§Ã£o

### 1. **Setup Unity Catalog**
- CriaÃ§Ã£o do catÃ¡logo e schema
- ConfiguraÃ§Ã£o de permissÃµes
- VerificaÃ§Ã£o de estrutura

### 2. **Extract da Staging**
- Leitura de arquivos Parquet da staging
- AplicaÃ§Ã£o de filtros temporais
- ValidaÃ§Ã£o de dados de entrada

### 3. **PreparaÃ§Ã£o para Merge**
- RemoÃ§Ã£o de duplicatas
- Compatibilidade de schema
- PreparaÃ§Ã£o de chaves de merge

### 4. **Load Incremental**
- VerificaÃ§Ã£o de tabela existente
- Merge incremental com Delta Lake
- CriaÃ§Ã£o/atualizaÃ§Ã£o da tabela Unity Catalog

### 5. **ValidaÃ§Ã£o e Monitoramento**
- Contagem de registros
- VerificaÃ§Ã£o de integridade
- Logs de processamento

### ğŸ´ **Flavor Text do Processamento**
*"Como um ferreiro forjando armas lendÃ¡rias, a camada Bronze transforma dados brutos em estruturas refinadas, preparando-os para as batalhas analÃ­ticas que virÃ£o."*

## ğŸ›¡ï¸ Controle de Qualidade

### **ValidaÃ§Ãµes Implementadas**
- âœ… **VerificaÃ§Ã£o de dados vazios**: ValidaÃ§Ã£o antes do processamento
- âœ… **RemoÃ§Ã£o de duplicatas**: Baseada em chaves especÃ­ficas
- âœ… **Compatibilidade de schema**: VerificaÃ§Ã£o de colunas
- âœ… **Merge incremental**: AtualizaÃ§Ã£o inteligente de dados

### ğŸ´ **Flavor Text da Qualidade**
*"Como um guardiÃ£o vigilante, a camada Bronze protege a integridade dos dados com escudos de validaÃ§Ã£o e espadas de verificaÃ§Ã£o, garantindo que apenas informaÃ§Ãµes puras avancem para as prÃ³ximas camadas."*

### **Tratamento de Erros e RecuperaÃ§Ã£o**
```python
# VerificaÃ§Ã£o de tabela Delta existente
def check_delta_table_exists(delta_path):
    delta_log_path = f"{delta_path}/_delta_log"
    files = dbutils.fs.ls(delta_log_path)
    return len(files) > 0

# PreparaÃ§Ã£o para merge
def prepare_dataframe_for_merge(df, table_name, target_schema=None):
    # RemoÃ§Ã£o de duplicatas
    # Compatibilidade de schema
    # Filtros temporais

# PreservaÃ§Ã£o de modificaÃ§Ãµes Unity Catalog
def create_or_update_unity_catalog(full_table_name, delta_path):
    table_exists = check_unity_table_exists(full_table_name)
    
    if not table_exists:
        # Criar tabela pela primeira vez
        create_table_sql = f"CREATE TABLE {full_table_name} USING DELTA LOCATION '{delta_path}'"
        spark.sql(create_table_sql)
    else:
        # Tabela jÃ¡ existe, apenas atualizar propriedades de pipeline
        # (preservando modificaÃ§Ãµes customizadas existentes)
        print("Atualizando apenas propriedades de pipeline")
```

#### **EstratÃ©gias de RecuperaÃ§Ã£o**
- **VerificaÃ§Ã£o de existÃªncia**: Antes de criar/atualizar tabelas
- **PreservaÃ§Ã£o de modificaÃ§Ãµes**: NÃ£o sobrescreve alteraÃ§Ãµes customizadas
- **Rollback automÃ¡tico**: Em caso de falha no merge
- **Logs detalhados**: Para debugging e auditoria

### **Logs e Monitoramento**
- **Contagem de registros**: Antes e depois do processamento
- **Duplicatas removidas**: Quantidade e chave utilizada
- **Schema compatÃ­vel**: Colunas utilizadas no merge
- **Tempo de execuÃ§Ã£o**: Performance do processamento

### **Monitoramento de Particionamento**
```python
# Consulta de partiÃ§Ãµes por volume de dados
partition_query = f"""
SELECT 
    partition_year,
    partition_month,
    COUNT(*) as records
FROM {table_name}
GROUP BY partition_year, partition_month
ORDER BY partition_year DESC, partition_month DESC, records DESC
LIMIT 10
"""
```

#### **MÃ©tricas de Particionamento**
- **DistribuiÃ§Ã£o de dados**: Registros por partiÃ§Ã£o
- **Performance**: Tempo de consulta por partiÃ§Ã£o
- **OtimizaÃ§Ã£o**: IdentificaÃ§Ã£o de partiÃ§Ãµes desbalanceadas
- **ManutenÃ§Ã£o**: Limpeza de partiÃ§Ãµes antigas

### **FunÃ§Ãµes de Monitoramento e ValidaÃ§Ã£o**

#### **VerificaÃ§Ã£o de Tabelas**
```python
def check_delta_table_exists(delta_path):
    # Verifica se a tabela Delta existe e tem dados
    delta_log_path = f"{delta_path}/_delta_log"
    files = dbutils.fs.ls(delta_log_path)
    if files:
        df = spark.read.format("delta").load(delta_path)
        count = df.count()
        return True, count > 0
    return False, False

def check_unity_table_exists(full_table_name):
    # Verifica se a tabela Unity Catalog existe
    tables_df = spark.sql(f"SHOW TABLES IN {CATALOG_NAME}.{SCHEMA_NAME}")
    tables_list = [row.tableName for row in tables_df.collect()]
    return TABLE_NAME in tables_list
```

#### **InformaÃ§Ãµes de Processamento**
```python
def show_delta_info(table_name):
    # Mostra histÃ³rico de versÃµes Delta
    delta_table = DeltaTable.forPath(spark, delta_path)
    history = delta_table.history()
    print(f"VersÃµes Delta: {history.count()}")

def show_sample_data(table_name):
    # Mostra dados de exemplo da tabela
    sample_query = f"SELECT * FROM {full_table_name} LIMIT 5"
    spark.sql(sample_query).show(truncate=False)
```

## ğŸ“Š CaracterÃ­sticas dos Dados

### **Dados Temporais (Cards e Sets)**
- **Filtro**: Ãšltimos 5 anos por padrÃ£o
- **Merge**: Incremental por ID/cÃ³digo
- **Particionamento**: Por ano/mÃªs baseado em `releaseDate`
- **HistÃ³rico**: Mantido no Delta Lake
- **Tipo**: ğŸƒ Creature/Spell/Artifact (dinÃ¢micos)

### **Dados de ReferÃªncia (Types, SuperTypes, SubTypes, Formats)**
- **Filtro**: Sem filtro temporal (dados estÃ¡ticos)
- **Merge**: Incremental por nome
- **Particionamento**: Por ano/mÃªs baseado em `ingestion_timestamp`
- **FrequÃªncia**: AtualizaÃ§Ã£o ocasional
- **Compatibilidade**: Suporte a schemas antigos
- **Tipo**: ğŸ·ï¸ Reference Card (estÃ¡ticos)

### **Dados de PreÃ§os (Card Prices)**
- **Filtro**: Baseado em cards existentes (sem filtro temporal direto)
- **Merge**: Incremental por nome da carta
- **Particionamento**: Por ano/mÃªs baseado em `ingestion_timestamp`
- **FrequÃªncia**: AtualizaÃ§Ã£o frequente (preÃ§os dinÃ¢micos)
- **Fonte**: Scryfall API (diferente da MTG API)
- **DependÃªncia**: Requer dados de cards jÃ¡ processados
- **Tipo**: ğŸ’° Market Data (dados dinÃ¢micos)

### ğŸ´ **Flavor Text dos Dados**
*"Como um bibliotecÃ¡rio organizando grimÃ³rios antigos e novos, a camada Bronze separa conhecimento temporal de sabedoria eterna, cada um com sua prÃ³pria estratÃ©gia de preservaÃ§Ã£o."*

## ğŸ”§ Funcionalidades AvanÃ§adas

### ğŸ´ **Flavor Text das Funcionalidades**
*"Como um arcanista dominando feitiÃ§os complexos, a camada Bronze utiliza magias avanÃ§adas de merge, particionamento e compatibilidade para forjar dados de qualidade superior."*

### **Merge Incremental Inteligente**
```python
# Merge com colunas especÃ­ficas
delta_table.alias("bronze").merge(
    df.alias("novo"),
    merge_condition
).whenMatchedUpdate(set=update_actions) \
 .whenNotMatchedInsert(values=insert_actions) \
 .execute()
```

### **Compatibilidade de Schema AvanÃ§ada**
```python
# DetecÃ§Ã£o automÃ¡tica de schema para merge
def get_delta_schema_for_merge(delta_path):
    df = spark.read.format("delta").load(delta_path)
    schema_fields = [field.name for field in df.schema.fields]
    # Excluir colunas de particionamento
    partition_columns = ['partition_year', 'partition_month']
    merge_schema = [col for col in schema_fields if col not in partition_columns]
    return merge_schema

# PreparaÃ§Ã£o de dados para merge
def prepare_dataframe_for_merge(df, table_name, target_schema=None):
    # Determinar chave baseada na tabela
    if table_name == "types":
        key_column = 'type_name'
    else:
        key_column = 'name'
    
    # RemoÃ§Ã£o de duplicatas
    df = df.dropDuplicates([key_column])
    
    # Filtro de colunas compatÃ­veis
    if target_schema:
        compatible_columns = [col for col in df.columns if col in target_schema]
        df = df.select(compatible_columns)
    
    return df
```

### **Compatibilidade de Schema**
- **DetecÃ§Ã£o automÃ¡tica** de diferenÃ§as de schema
- **RenomeaÃ§Ã£o de colunas** para compatibilidade
- **Filtro de colunas** compatÃ­veis
- **PreservaÃ§Ã£o** de dados existentes

### **Metadados e Propriedades das Tabelas**
```python
# Propriedades automÃ¡ticas configuradas
spark.sql(f"""
ALTER TABLE {full_table_name} SET TBLPROPERTIES (
    'bronze_layer' = 'true',
    'data_source' = 'mtg_api',
    'last_processing_date' = '{datetime.now().strftime("%Y-%m-%d %H:%M:%S")}',
    'table_type' = 'bronze',
    'load_mode' = 'incremental_merge_fixed',
    'temporal_window_years' = '{YEARS_BACK}',
    'partitioning' = 'ingestion_timestamp_year_month'
)
""")
```

#### **Propriedades Configuradas**
- **`bronze_layer`**: IdentificaÃ§Ã£o da camada
- **`data_source`**: Origem dos dados (mtg_api)
- **`last_processing_date`**: Data/hora do Ãºltimo processamento
- **`table_type`**: Tipo da tabela (bronze)
- **`load_mode`**: Modo de carregamento (incremental_merge_fixed)
- **`temporal_window_years`**: Janela temporal configurada
- **`partitioning`**: EstratÃ©gia de particionamento utilizada

### **Filtros Temporais**
- **ConfigurÃ¡vel**: Anos para trÃ¡s ajustÃ¡vel
- **Seletivo**: Aplicado apenas em dados temporais
- **Performance**: OtimizaÃ§Ã£o de consultas
- **Flexibilidade**: Diferentes estratÃ©gias por tabela

### **Particionamento das Tabelas**
```python
# Dados Temporais (Cards e Sets) - Particionamento por releaseDate
df_with_partition = df.withColumn("partition_year", 
                  when(col("releaseDate").isNotNull(), 
                       year(col("releaseDate")))
                  .otherwise(year(col("ingestion_timestamp")))) \
       .withColumn("partition_month", 
                  when(col("releaseDate").isNotNull(), 
                       month(col("releaseDate")))
                  .otherwise(month(col("ingestion_timestamp"))))

# Dados de ReferÃªncia (Types, SuperTypes, SubTypes, Formats) - Particionamento por ingestion_timestamp
df_with_partition = df.withColumn("partition_year", 
                  year(col("ingestion_timestamp"))) \
       .withColumn("partition_month", 
                  month(col("ingestion_timestamp")))

# Dados de PreÃ§os (Card Prices) - Particionamento por ingestion_timestamp
df_with_partition = df.withColumn("partition_year", 
                  year(col("ingestion_timestamp"))) \
       .withColumn("partition_month", 
                  month(col("ingestion_timestamp")))

# AplicaÃ§Ã£o do particionamento
df_with_partition.write.format("delta") \
       .partitionBy("partition_year", "partition_month") \
       .save(delta_path)

#### **Campos de Particionamento**
- **`partition_year`**: Ano extraÃ­do da data de referÃªncia
- **`partition_month`**: MÃªs extraÃ­do da data de referÃªncia
- **EstratÃ©gia**: 
  - **Dados temporais**: Prioriza `releaseDate`, fallback para `ingestion_timestamp`
  - **Dados de referÃªncia**: Usa `ingestion_timestamp` (data de ingestÃ£o)
  - **Dados de preÃ§os**: Usa `ingestion_timestamp` (data de ingestÃ£o)

## ğŸš€ Como Executar

### ExecuÃ§Ã£o Individual
```python
# Executar notebook especÃ­fico
Cards.ipynb
Sets.ipynb
Types.ipynb
SuperTypes.ipynb
SubTypes.ipynb
Formats.ipynb
Card_Prices.ipynb
```

### ExecuÃ§Ã£o Sequencial
```python
# Executar todos os notebooks em ordem
Types.ipynb
SuperTypes.ipynb
SubTypes.ipynb
Formats.ipynb
Sets.ipynb
Cards.ipynb
Card_Prices.ipynb  # Deve ser executado apÃ³s Cards.ipynb
```

## ğŸ“‹ Checklist de ExecuÃ§Ã£o

- [ ] Segredos configurados no Databricks
- [ ] PermissÃµes Unity Catalog verificadas
- [ ] Cluster Spark disponÃ­vel
- [ ] Dados de staging disponÃ­veis
- [ ] EspaÃ§o em disco suficiente
- [ ] LimitaÃ§Ãµes de demonstraÃ§Ã£o compreendidas

## ğŸ”— PrÃ³ximos Passos

ApÃ³s o processamento na Bronze, os dados estarÃ£o disponÃ­veis para:
1. **Camada Silver**: TransformaÃ§Ãµes e enriquecimento
2. **Camada Gold**: Modelos de dados finais
3. **AnÃ¡lises**: Consultas diretas no Unity Catalog

## ğŸ—ï¸ Engenharia de Dados

### ğŸ´ **Flavor Text da Engenharia**
*"Como um mestre ferreiro forjando armas lendÃ¡rias, a engenharia da camada Bronze combina arte e ciÃªncia para criar estruturas de dados que resistem ao teste do tempo."*

### ğŸ¯ PrincÃ­pios da Camada Bronze

#### **1. EstruturaÃ§Ã£o**
- âœ… TransformaÃ§Ã£o de dados brutos em estruturados
- âœ… Schema explÃ­cito e validado
- âœ… Tipos de dados apropriados
- âœ… Metadados organizados

#### **2. Incrementalidade**
- âœ… Merge inteligente de dados
- âœ… PreservaÃ§Ã£o de histÃ³rico
- âœ… Performance otimizada
- âœ… RecuperaÃ§Ã£o de falhas

#### **3. GovernanÃ§a**
- âœ… Unity Catalog para controle
- âœ… PermissÃµes granulares
- âœ… Rastreabilidade completa
- âœ… Auditoria de mudanÃ§as

#### **4. Qualidade**
- âœ… ValidaÃ§Ã£o de integridade
- âœ… RemoÃ§Ã£o de duplicatas
- âœ… Compatibilidade de schema
- âœ… Logs estruturados

### ğŸ“ Regras da Camada

#### **Regra #1: Merge Incremental**
```python
# âœ… CORRETO - Merge com chave especÃ­fica
delta_table.merge(df, "bronze.id = novo.id")

# âŒ INCORRETO - Overwrite completo
df.write.mode("overwrite").save(delta_path)
```

#### **Regra #2: Compatibilidade de Schema**
```python
# VerificaÃ§Ã£o de compatibilidade antes do merge
target_schema = get_delta_schema_for_merge(delta_path)
compatible_columns = [col for col in df.columns if col in target_schema]
```

#### **Regra #3: RemoÃ§Ã£o de Duplicatas**
```python
# RemoÃ§Ã£o baseada em chave especÃ­fica
df = df.dropDuplicates([key_column])
```

#### **Regra #4: Logs Estruturados**
```python
print(f"ExtraÃ­dos {df.count()} registros de staging")
print(f"Removidas {duplicates} duplicatas")
print(f"Merge executado com sucesso")
```

## ğŸ´ Galeria Visual - Camada Bronze

### ğŸ—ï¸ Elementos da Camada Bronze
```
ğŸ—ï¸ EstruturaÃ§Ã£o    ğŸ”„ Incrementalidade    ğŸ›¡ï¸ GovernanÃ§a    ğŸ“Š Qualidade
```

### ğŸ‰ Criaturas da Bronze
```
ğŸ‰ Bronze Dragon    ğŸ›¡ï¸ Bronze Golem    âš”ï¸ Bronze Sphinx    ğŸ° Bronze Guardian
```

### ğŸ”§ Ferramentas da Forja
```
ğŸ”¥ Forge Hammer    âš’ï¸ Anvil of Data    ğŸ”¨ Refinement Tools    ğŸ­ Processing Plant
```

### ğŸ¯ Metodologias da Bronze
```
ğŸ“ Schema Forge    ğŸ”„ Merge Mastery    ğŸ›¡ï¸ Quality Shield    ğŸ“Š Metrics Crystal
```

### ğŸŒŸ Propriedades MÃ¡gicas
```
âœ¨ Bronze Layer    ğŸ”— Data Source    â° Processing Time    ğŸ® Load Mode
```

### ğŸ›ï¸ Arquitetura da Bronze
```
ğŸ›ï¸ Unity Catalog    ğŸ—„ï¸ Delta Lake    ğŸ“ Schema Bronze    ğŸ” Governance
```

### ğŸ´ Tipos de Dados Processados
```
ğŸƒ Cards (Temporais)    ğŸ“¦ Sets (Temporais)    ğŸ·ï¸ Types (ReferÃªncia)
â­ SuperTypes (ReferÃªncia)    ğŸ”– SubTypes (ReferÃªncia)    ğŸ® Formats (ReferÃªncia)
ğŸ’° Card Prices (Market Data)
```

### ğŸ”„ OperaÃ§Ãµes de Merge
```
ğŸ”„ Incremental Merge    ğŸ“Š Schema Compatibility    ğŸ›¡ï¸ Quality Validation
âš¡ Performance Optimization    ğŸ“ˆ Data Monitoring    ğŸ” Error Handling
```

## ğŸ“š DocumentaÃ§Ã£o Completa

### ğŸ¯ **DocumentaÃ§Ã£o Detalhada das Tabelas**
Para informaÃ§Ãµes completas sobre cada tabela da camada Bronze, incluindo schema detalhado, regras de implementaÃ§Ã£o, particionamento e linhagem de dados, consulte nossa **documentaÃ§Ã£o completa**:

**[ğŸ“– Ver DocumentaÃ§Ã£o Completa da Camada Bronze](./DocumentaÃ§Ã£o/README.md)**

### ğŸ“‹ **O que vocÃª encontrarÃ¡ na documentaÃ§Ã£o:**
- **Schema detalhado** de todas as 7 tabelas
- **Regras de renomeaÃ§Ã£o**
- **EstratÃ©gias de particionamento** especÃ­ficas
- **Linhagem de dados** e fluxo de processamento
- **Regras de implementaÃ§Ã£o** e filtros temporais
- **Exemplos de uso** e casos especÃ­ficos

### ğŸ´ **Flavor Text da DocumentaÃ§Ã£o**
*"Como um grimÃ³rio sagrado que contÃ©m todos os segredos da magia, a documentaÃ§Ã£o completa da camada Bronze revela os mistÃ©rios de cada tabela, permitindo que os magos da engenharia de dados dominem completamente o poder dos dados estruturados."*

## ğŸ“ Suporte

Para dÃºvidas ou problemas:
- Verificar logs de execuÃ§Ã£o
- Consultar histÃ³rico do Delta Lake
- Revisar configuraÃ§Ãµes de segredos
- Verificar permissÃµes Unity Catalog
- Para detalhes tÃ©cnicos: Acessar [DocumentaÃ§Ã£o Completa](./DocumentaÃ§Ã£o/README.md) 


# ü•â Camada Bronze - Magic: The Gathering

<div align="center">

![RED XIII - Proud Warrior](https://repositorio.sbrauble.com/arquivos/in/magic/480717/6824dbcfba786-4us35-hj5ls-0e0603650198f950cad99859423dd8cf.jpg)

*"Through the fires of transformation, raw data emerges as structured wisdom."* - RED XIII, Magic: The Gathering - FF Edition

</div>

## üìã Vis√£o Geral

Esta pasta cont√©m os notebooks respons√°veis pela **camada Bronze** do pipeline de dados do Magic: The Gathering. A camada Bronze realiza o processo **EL (Extract & Load)**, carregando dados brutos da staging em dados estruturados e organizados no Unity Catalog com Delta Lake.

## üéØ Objetivo

Carregar dados da camada de staging (S3/Parquet) em dados estruturados na camada Bronze (Unity Catalog/Delta), garantindo:
- **Extract**: Leitura eficiente dos dados de staging
- **Load**: Carregamento incremental com merge inteligente
- **Governan√ßa**: Controle atrav√©s do Unity Catalog
- **Performance**: Otimiza√ß√£o com Delta Lake
- **Rastreabilidade**: Hist√≥rico completo de mudan√ßas

## üîÑ Processo EL (Extract & Load)

### **Extract - Extra√ß√£o da Staging**
```python
def extract_from_staging(table_name):
    stage_path = f"{S3_STAGE_PATH}/*_{table_name}.parquet"
    df = spark.read.parquet(stage_path)
    return df
```

### **Load - Carregamento na Bronze**
```python
def load_to_bronze_unity_merge(df, table_name):
    # Merge incremental com Delta Lake
    delta_table.alias("bronze").merge(
        df.alias("novo"),
        "bronze.id = novo.id"  # ou chave espec√≠fica
    ).whenMatchedUpdateAll() \
     .whenNotMatchedInsertAll() \
     .execute()
```

## üìÅ Estrutura dos Notebooks

### üÉè `Cards.ipynb`
- **Fonte**: Dados de cartas da staging
- **Chave**: `id` (identificador √∫nico da carta)
- **Caracter√≠sticas**: 
  - Merge incremental por ID
  - Filtro temporal de 5 anos
  - Schema complexo com 25+ campos
  - Tratamento de arrays JSON
- **Tipo**: üé¥ Creature/Spell/Artifact (dados temporais)

### üé¥ `Sets.ipynb`
- **Fonte**: Dados de expans√µes da staging
- **Chave**: `code` (c√≥digo da expans√£o)
- **Caracter√≠sticas**:
  - Merge incremental por c√≥digo
  - Filtro temporal de 5 anos
  - Campo booster explodido em m√∫ltiplas colunas
  - Metadados de lan√ßamento
- **Tipo**: üì¶ Expansion Set (dados temporais)

### üè∑Ô∏è `Types.ipynb`
- **Fonte**: Dados de tipos da staging
- **Chave**: `type_name` (nome do tipo)
- **Caracter√≠sticas**:
  - Dados de refer√™ncia est√°ticos
  - Sem filtro temporal
  - Schema simples (1 campo principal)
  - Compatibilidade com schemas antigos
- **Tipo**: üè∑Ô∏è Reference Card (dados est√°ticos)

### ‚≠ê `SuperTypes.ipynb`
- **Fonte**: Dados de super tipos da staging
- **Chave**: `supertype_name` (nome do super tipo)
- **Caracter√≠sticas**:
  - Dados de refer√™ncia est√°ticos
  - Sem filtro temporal
  - Schema simples (1 campo principal)
- **Tipo**: ‚≠ê Reference Card (dados est√°ticos)

### üîñ `SubTypes.ipynb`
- **Fonte**: Dados de sub tipos da staging
- **Chave**: `subtype_name` (nome do sub tipo)
- **Caracter√≠sticas**:
  - Dados de refer√™ncia est√°ticos
  - Sem filtro temporal
  - Schema simples (1 campo principal)
- **Tipo**: üîñ Reference Card (dados est√°ticos)

### üéÆ `Formats.ipynb`
- **Fonte**: Dados de formatos da staging
- **Chave**: `format_name` (nome do formato)
- **Caracter√≠sticas**:
  - Dados de refer√™ncia est√°ticos
  - Sem filtro temporal
  - Schema simples (1 campo principal)
- **Tipo**: üéÆ Reference Card (dados est√°ticos)

### üí∞ `Card_Prices.ipynb`
- **Fonte**: Dados de pre√ßos da staging (Scryfall API)
- **Chave**: `name` (nome da carta)
- **Caracter√≠sticas**:
  - Dados de pre√ßos em tempo real (USD, EUR, TIX)
  - Merge incremental por nome da carta
  - Particionamento por `ingestion_timestamp`
  - Depend√™ncia: Requer dados de cards j√° processados
  - Fonte de dados: Scryfall API (diferente da MTG API)
- **Tipo**: üí∞ Market Data (dados din√¢micos)

## ‚öôÔ∏è Configura√ß√µes Necess√°rias

### üîê Segredos do Databricks
Configure os seguintes segredos no scope `mtg-pipeline`:

```python
# Unity Catalog Configuration
catalog_name           # Nome do cat√°logo Unity
s3_bucket             # Bucket S3 para armazenamento
s3_stage_prefix       # Prefixo da camada staging
s3_bronze_prefix      # Prefixo da camada bronze

# Temporal Configuration
years_back            # Anos para tr√°s no filtro temporal (padr√£o: 5)
```

### Estrutura Unity Catalog
```
{catalog_name}/
‚îî‚îÄ‚îÄ bronze/
    ‚îú‚îÄ‚îÄ cards
    ‚îú‚îÄ‚îÄ sets
    ‚îú‚îÄ‚îÄ types
    ‚îú‚îÄ‚îÄ supertypes
    ‚îú‚îÄ‚îÄ subtypes
    ‚îî‚îÄ‚îÄ formats
```

## üîÑ Fluxo de Execu√ß√£o

### 1. **Setup Unity Catalog**
- Cria√ß√£o do cat√°logo e schema
- Configura√ß√£o de permiss√µes
- Verifica√ß√£o de estrutura

### 2. **Extract da Staging**
- Leitura de arquivos Parquet da staging
- Aplica√ß√£o de filtros temporais
- Valida√ß√£o de dados de entrada

### 3. **Prepara√ß√£o para Merge**
- Remo√ß√£o de duplicatas
- Compatibilidade de schema
- Prepara√ß√£o de chaves de merge

### 4. **Load Incremental**
- Verifica√ß√£o de tabela existente
- Merge incremental com Delta Lake
- Cria√ß√£o/atualiza√ß√£o da tabela Unity Catalog

### 5. **Valida√ß√£o e Monitoramento**
- Contagem de registros
- Verifica√ß√£o de integridade
- Logs de processamento

### üé¥ **Flavor Text do Processamento**
*"Como um ferreiro forjando armas lend√°rias, a camada Bronze transforma dados brutos em estruturas refinadas, preparando-os para as batalhas anal√≠ticas que vir√£o."*

## üõ°Ô∏è Controle de Qualidade

### **Valida√ß√µes Implementadas**
- ‚úÖ **Verifica√ß√£o de dados vazios**: Valida√ß√£o antes do processamento
- ‚úÖ **Remo√ß√£o de duplicatas**: Baseada em chaves espec√≠ficas
- ‚úÖ **Compatibilidade de schema**: Verifica√ß√£o de colunas
- ‚úÖ **Merge incremental**: Atualiza√ß√£o inteligente de dados

### üé¥ **Flavor Text da Qualidade**
*"Como um guardi√£o vigilante, a camada Bronze protege a integridade dos dados com escudos de valida√ß√£o e espadas de verifica√ß√£o, garantindo que apenas informa√ß√µes puras avancem para as pr√≥ximas camadas."*

### **Tratamento de Erros e Recupera√ß√£o**
```python
# Verifica√ß√£o de tabela Delta existente
def check_delta_table_exists(delta_path):
    delta_log_path = f"{delta_path}/_delta_log"
    files = dbutils.fs.ls(delta_log_path)
    return len(files) > 0

# Prepara√ß√£o para merge
def prepare_dataframe_for_merge(df, table_name, target_schema=None):
    # Remo√ß√£o de duplicatas
    # Compatibilidade de schema
    # Filtros temporais

# Preserva√ß√£o de modifica√ß√µes Unity Catalog
def create_or_update_unity_catalog(full_table_name, delta_path):
    table_exists = check_unity_table_exists(full_table_name)
    
    if not table_exists:
        # Criar tabela pela primeira vez
        create_table_sql = f"CREATE TABLE {full_table_name} USING DELTA LOCATION '{delta_path}'"
        spark.sql(create_table_sql)
    else:
        # Tabela j√° existe, apenas atualizar propriedades de pipeline
        # (preservando modifica√ß√µes customizadas existentes)
        print("Atualizando apenas propriedades de pipeline")
```

#### **Estrat√©gias de Recupera√ß√£o**
- **Verifica√ß√£o de exist√™ncia**: Antes de criar/atualizar tabelas
- **Preserva√ß√£o de modifica√ß√µes**: N√£o sobrescreve altera√ß√µes customizadas
- **Rollback autom√°tico**: Em caso de falha no merge
- **Logs detalhados**: Para debugging e auditoria

### **Logs e Monitoramento**
- **Contagem de registros**: Antes e depois do processamento
- **Duplicatas removidas**: Quantidade e chave utilizada
- **Schema compat√≠vel**: Colunas utilizadas no merge
- **Tempo de execu√ß√£o**: Performance do processamento

### **Monitoramento de Particionamento**
```python
# Consulta de parti√ß√µes por volume de dados
partition_query = f"""
SELECT 
    partition_year,
    partition_month,
    COUNT(*) as records
FROM {table_name}
GROUP BY partition_year, partition_month
ORDER BY partition_year DESC, partition_month DESC, records DESC
LIMIT 10
"""
```

#### **M√©tricas de Particionamento**
- **Distribui√ß√£o de dados**: Registros por parti√ß√£o
- **Performance**: Tempo de consulta por parti√ß√£o
- **Otimiza√ß√£o**: Identifica√ß√£o de parti√ß√µes desbalanceadas
- **Manuten√ß√£o**: Limpeza de parti√ß√µes antigas

### **Fun√ß√µes de Monitoramento e Valida√ß√£o**

#### **Verifica√ß√£o de Tabelas**
```python
def check_delta_table_exists(delta_path):
    # Verifica se a tabela Delta existe e tem dados
    delta_log_path = f"{delta_path}/_delta_log"
    files = dbutils.fs.ls(delta_log_path)
    if files:
        df = spark.read.format("delta").load(delta_path)
        count = df.count()
        return True, count > 0
    return False, False

def check_unity_table_exists(full_table_name):
    # Verifica se a tabela Unity Catalog existe
    tables_df = spark.sql(f"SHOW TABLES IN {CATALOG_NAME}.{SCHEMA_NAME}")
    tables_list = [row.tableName for row in tables_df.collect()]
    return TABLE_NAME in tables_list
```

#### **Informa√ß√µes de Processamento**
```python
def show_delta_info(table_name):
    # Mostra hist√≥rico de vers√µes Delta
    delta_table = DeltaTable.forPath(spark, delta_path)
    history = delta_table.history()
    print(f"Vers√µes Delta: {history.count()}")

def show_sample_data(table_name):
    # Mostra dados de exemplo da tabela
    sample_query = f"SELECT * FROM {full_table_name} LIMIT 5"
    spark.sql(sample_query).show(truncate=False)
```

## üìä Caracter√≠sticas dos Dados

### **Dados Temporais (Cards e Sets)**
- **Filtro**: √öltimos 5 anos por padr√£o
- **Merge**: Incremental por ID/c√≥digo
- **Particionamento**: Por ano/m√™s baseado em `releaseDate`
- **Hist√≥rico**: Mantido no Delta Lake
- **Tipo**: üÉè Creature/Spell/Artifact (din√¢micos)

### **Dados de Refer√™ncia (Types, SuperTypes, SubTypes, Formats)**
- **Filtro**: Sem filtro temporal (dados est√°ticos)
- **Merge**: Incremental por nome
- **Particionamento**: Por ano/m√™s baseado em `ingestion_timestamp`
- **Frequ√™ncia**: Atualiza√ß√£o ocasional
- **Compatibilidade**: Suporte a schemas antigos
- **Tipo**: üè∑Ô∏è Reference Card (est√°ticos)

### **Dados de Pre√ßos (Card Prices)**
- **Filtro**: Baseado em cards existentes (sem filtro temporal direto)
- **Merge**: Incremental por nome da carta
- **Particionamento**: Por ano/m√™s baseado em `ingestion_timestamp`
- **Frequ√™ncia**: Atualiza√ß√£o frequente (pre√ßos din√¢micos)
- **Fonte**: Scryfall API (diferente da MTG API)
- **Depend√™ncia**: Requer dados de cards j√° processados
- **Tipo**: üí∞ Market Data (dados din√¢micos)

### üé¥ **Flavor Text dos Dados**
*"Como um bibliotec√°rio organizando grim√≥rios antigos e novos, a camada Bronze separa conhecimento temporal de sabedoria eterna, cada um com sua pr√≥pria estrat√©gia de preserva√ß√£o."*

## üîß Funcionalidades Avan√ßadas

### üé¥ **Flavor Text das Funcionalidades**
*"Como um arcanista dominando feiti√ßos complexos, a camada Bronze utiliza magias avan√ßadas de merge, particionamento e compatibilidade para forjar dados de qualidade superior."*

### **Merge Incremental Inteligente**
```python
# Merge com colunas espec√≠ficas
delta_table.alias("bronze").merge(
    df.alias("novo"),
    merge_condition
).whenMatchedUpdate(set=update_actions) \
 .whenNotMatchedInsert(values=insert_actions) \
 .execute()
```

### **Compatibilidade de Schema Avan√ßada**
```python
# Detec√ß√£o autom√°tica de schema para merge
def get_delta_schema_for_merge(delta_path):
    df = spark.read.format("delta").load(delta_path)
    schema_fields = [field.name for field in df.schema.fields]
    # Excluir colunas de particionamento
    partition_columns = ['partition_year', 'partition_month']
    merge_schema = [col for col in schema_fields if col not in partition_columns]
    return merge_schema

# Prepara√ß√£o de dados para merge
def prepare_dataframe_for_merge(df, table_name, target_schema=None):
    # Determinar chave baseada na tabela
    if table_name == "types":
        key_column = 'type_name'
    else:
        key_column = 'name'
    
    # Remo√ß√£o de duplicatas
    df = df.dropDuplicates([key_column])
    
    # Filtro de colunas compat√≠veis
    if target_schema:
        compatible_columns = [col for col in df.columns if col in target_schema]
        df = df.select(compatible_columns)
    
    return df
```

### **Compatibilidade de Schema**
- **Detec√ß√£o autom√°tica** de diferen√ßas de schema
- **Renomea√ß√£o de colunas** para compatibilidade
- **Filtro de colunas** compat√≠veis
- **Preserva√ß√£o** de dados existentes

### **Metadados e Propriedades das Tabelas**
```python
# Propriedades autom√°ticas configuradas
spark.sql(f"""
ALTER TABLE {full_table_name} SET TBLPROPERTIES (
    'bronze_layer' = 'true',
    'data_source' = 'mtg_api',
    'last_processing_date' = '{datetime.now().strftime("%Y-%m-%d %H:%M:%S")}',
    'table_type' = 'bronze',
    'load_mode' = 'incremental_merge_fixed',
    'temporal_window_years' = '{YEARS_BACK}',
    'partitioning' = 'ingestion_timestamp_year_month'
)
""")
```

#### **Propriedades Configuradas**
- **`bronze_layer`**: Identifica√ß√£o da camada
- **`data_source`**: Origem dos dados (mtg_api)
- **`last_processing_date`**: Data/hora do √∫ltimo processamento
- **`table_type`**: Tipo da tabela (bronze)
- **`load_mode`**: Modo de carregamento (incremental_merge_fixed)
- **`temporal_window_years`**: Janela temporal configurada
- **`partitioning`**: Estrat√©gia de particionamento utilizada

### **Filtros Temporais**
- **Configur√°vel**: Anos para tr√°s ajust√°vel
- **Seletivo**: Aplicado apenas em dados temporais
- **Performance**: Otimiza√ß√£o de consultas
- **Flexibilidade**: Diferentes estrat√©gias por tabela

### **Particionamento das Tabelas**
```python
# Dados Temporais (Cards e Sets) - Particionamento por releaseDate
df_with_partition = df.withColumn("partition_year", 
                  when(col("releaseDate").isNotNull(), 
                       year(col("releaseDate")))
                  .otherwise(year(col("ingestion_timestamp")))) \
       .withColumn("partition_month", 
                  when(col("releaseDate").isNotNull(), 
                       month(col("releaseDate")))
                  .otherwise(month(col("ingestion_timestamp"))))

# Dados de Refer√™ncia (Types, SuperTypes, SubTypes, Formats) - Particionamento por ingestion_timestamp
df_with_partition = df.withColumn("partition_year", 
                  year(col("ingestion_timestamp"))) \
       .withColumn("partition_month", 
                  month(col("ingestion_timestamp")))

# Dados de Pre√ßos (Card Prices) - Particionamento por ingestion_timestamp
df_with_partition = df.withColumn("partition_year", 
                  year(col("ingestion_timestamp"))) \
       .withColumn("partition_month", 
                  month(col("ingestion_timestamp")))

# Aplica√ß√£o do particionamento
df_with_partition.write.format("delta") \
       .partitionBy("partition_year", "partition_month") \
       .save(delta_path)

#### **Campos de Particionamento**
- **`partition_year`**: Ano extra√≠do da data de refer√™ncia
- **`partition_month`**: M√™s extra√≠do da data de refer√™ncia
- **Estrat√©gia**: 
  - **Dados temporais**: Prioriza `releaseDate`, fallback para `ingestion_timestamp`
  - **Dados de refer√™ncia**: Usa `ingestion_timestamp` (data de ingest√£o)
  - **Dados de pre√ßos**: Usa `ingestion_timestamp` (data de ingest√£o)

## üöÄ Como Executar

### Execu√ß√£o Individual
```python
# Executar notebook espec√≠fico
Cards.ipynb
Sets.ipynb
Types.ipynb
SuperTypes.ipynb
SubTypes.ipynb
Formats.ipynb
Card_Prices.ipynb
```

### Execu√ß√£o Sequencial
```python
# Executar todos os notebooks em ordem
Types.ipynb
SuperTypes.ipynb
SubTypes.ipynb
Formats.ipynb
Sets.ipynb
Cards.ipynb
Card_Prices.ipynb  # Deve ser executado ap√≥s Cards.ipynb
```

## üìã Checklist de Execu√ß√£o

- [ ] Segredos configurados no Databricks
- [ ] Permiss√µes Unity Catalog verificadas
- [ ] Cluster Spark dispon√≠vel
- [ ] Dados de staging dispon√≠veis
- [ ] Espa√ßo em disco suficiente
- [ ] Limita√ß√µes de demonstra√ß√£o compreendidas

## üîó Pr√≥ximos Passos

Ap√≥s o processamento na Bronze, os dados estar√£o dispon√≠veis para:
1. **Camada Silver**: Transforma√ß√µes e enriquecimento
2. **Camada Gold**: Modelos de dados finais
3. **An√°lises**: Consultas diretas no Unity Catalog

## üèóÔ∏è Engenharia de Dados

### üé¥ **Flavor Text da Engenharia**
*"Como um mestre ferreiro forjando armas lend√°rias, a engenharia da camada Bronze combina arte e ci√™ncia para criar estruturas de dados que resistem ao teste do tempo."*

### üéØ Princ√≠pios da Camada Bronze

#### **1. Estrutura√ß√£o**
- ‚úÖ Transforma√ß√£o de dados brutos em estruturados
- ‚úÖ Schema expl√≠cito e validado
- ‚úÖ Tipos de dados apropriados
- ‚úÖ Metadados organizados

#### **2. Incrementalidade**
- ‚úÖ Merge inteligente de dados
- ‚úÖ Preserva√ß√£o de hist√≥rico
- ‚úÖ Performance otimizada
- ‚úÖ Recupera√ß√£o de falhas

#### **3. Governan√ßa**
- ‚úÖ Unity Catalog para controle
- ‚úÖ Permiss√µes granulares
- ‚úÖ Rastreabilidade completa
- ‚úÖ Auditoria de mudan√ßas

#### **4. Qualidade**
- ‚úÖ Valida√ß√£o de integridade
- ‚úÖ Remo√ß√£o de duplicatas
- ‚úÖ Compatibilidade de schema
- ‚úÖ Logs estruturados

### üìê Regras da Camada

#### **Regra #1: Merge Incremental**
```python
# ‚úÖ CORRETO - Merge com chave espec√≠fica
delta_table.merge(df, "bronze.id = novo.id")

# ‚ùå INCORRETO - Overwrite completo
df.write.mode("overwrite").save(delta_path)
```

#### **Regra #2: Compatibilidade de Schema**
```python
# Verifica√ß√£o de compatibilidade antes do merge
target_schema = get_delta_schema_for_merge(delta_path)
compatible_columns = [col for col in df.columns if col in target_schema]
```

#### **Regra #3: Remo√ß√£o de Duplicatas**
```python
# Remo√ß√£o baseada em chave espec√≠fica
df = df.dropDuplicates([key_column])
```

#### **Regra #4: Logs Estruturados**
```python
print(f"Extra√≠dos {df.count()} registros de staging")
print(f"Removidas {duplicates} duplicatas")
print(f"Merge executado com sucesso")
```

## üé¥ Galeria Visual - Camada Bronze

### üèóÔ∏è Elementos da Camada Bronze
```
üèóÔ∏è Estrutura√ß√£o    üîÑ Incrementalidade    üõ°Ô∏è Governan√ßa    üìä Qualidade
```

### üêâ Criaturas da Bronze
```
üêâ Bronze Dragon    üõ°Ô∏è Bronze Golem    ‚öîÔ∏è Bronze Sphinx    üè∞ Bronze Guardian
```

### üîß Ferramentas da Forja
```
üî• Forge Hammer    ‚öíÔ∏è Anvil of Data    üî® Refinement Tools    üè≠ Processing Plant
```

### üéØ Metodologias da Bronze
```
üìê Schema Forge    üîÑ Merge Mastery    üõ°Ô∏è Quality Shield    üìä Metrics Crystal
```

### üåü Propriedades M√°gicas
```
‚ú® Bronze Layer    üîó Data Source    ‚è∞ Processing Time    üéÆ Load Mode
```

### üèõÔ∏è Arquitetura da Bronze
```
üèõÔ∏è Unity Catalog    üóÑÔ∏è Delta Lake    üìÅ Schema Bronze    üîê Governance
```

### üé¥ Tipos de Dados Processados
```
üÉè Cards (Temporais)    üì¶ Sets (Temporais)    üè∑Ô∏è Types (Refer√™ncia)
‚≠ê SuperTypes (Refer√™ncia)    üîñ SubTypes (Refer√™ncia)    üéÆ Formats (Refer√™ncia)
üí∞ Card Prices (Market Data)
```

### üîÑ Opera√ß√µes de Merge
```
üîÑ Incremental Merge    üìä Schema Compatibility    üõ°Ô∏è Quality Validation
‚ö° Performance Optimization    üìà Data Monitoring    üîç Error Handling
```

## üìö Documenta√ß√£o Completa

### üéØ **Documenta√ß√£o Detalhada das Tabelas**
Para informa√ß√µes completas sobre cada tabela da camada Bronze, incluindo schema detalhado, regras de implementa√ß√£o, particionamento e linhagem de dados, consulte nossa **documenta√ß√£o completa**:

**[üìñ Ver Documenta√ß√£o Completa da Camada Bronze](./Documenta√ß√£o/README.md)**

### üìã **O que voc√™ encontrar√° na documenta√ß√£o:**
- **Schema detalhado** de todas as 7 tabelas
- **Regras de renomea√ß√£o**
- **Estrat√©gias de particionamento** espec√≠ficas
- **Linhagem de dados** e fluxo de processamento
- **Regras de implementa√ß√£o** e filtros temporais
- **Exemplos de uso** e casos espec√≠ficos

### üé¥ **Flavor Text da Documenta√ß√£o**
*"Como um grim√≥rio sagrado que cont√©m todos os segredos da magia, a documenta√ß√£o completa da camada Bronze revela os mist√©rios de cada tabela, permitindo que os magos da engenharia de dados dominem completamente o poder dos dados estruturados."*

## üìû Suporte

Para d√∫vidas ou problemas:
- Verificar logs de execu√ß√£o
- Consultar hist√≥rico do Delta Lake
- Revisar configura√ß√µes de segredos
- Verificar permiss√µes Unity Catalog
- Para detalhes t√©cnicos: Acessar [Documenta√ß√£o Completa](./Documenta√ß√£o/README.md) 


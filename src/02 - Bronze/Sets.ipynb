{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27c62fca-9a03-47e9-90c9-0afafedded1d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import e Segredos"
    }
   },
   "outputs": [],
   "source": [
    "# Camada Bronze - Sets - Magic: The Gathering (Unity + Incremental Merge)\n",
    "# Objetivo: Processo EL (Extract & Load) - Extrair dados da staging e carregar na bronze\n",
    "# Características: Extract da staging (S3/Parquet) -> Load na bronze (Unity Catalog/Delta) - INCREMENTAL\n",
    "# Melhorias: Janela de 5 anos, particionamento por ano mes\n",
    "\n",
    "# =============================================================================\n",
    "# BIBLIOTECAS UTILIZADAS\n",
    "# =============================================================================\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURAÇÃO DE SEGREDOS\n",
    "# =============================================================================\n",
    "def get_secret(secret_name, default_value=None):\n",
    "    try:\n",
    "        return dbutils.secrets.get(scope=\"mtg-pipeline\", key=secret_name)\n",
    "    except:\n",
    "        if default_value is not None:\n",
    "            print(f\"Segredo '{secret_name}' não encontrado, usando valor padrão\")\n",
    "            return default_value\n",
    "        else:\n",
    "            print(f\"Segredo obrigatório '{secret_name}' não encontrado\")\n",
    "            raise Exception(f\"Segredo '{secret_name}' não configurado\")\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURAÇÕES GLOBAIS\n",
    "# =============================================================================\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "CATALOG_NAME = get_secret(\"catalog_name\")\n",
    "SCHEMA_NAME = \"bronze\"\n",
    "TABLE_NAME = \"sets\"\n",
    "\n",
    "S3_BUCKET = get_secret(\"s3_bucket\")\n",
    "S3_STAGE_PREFIX = get_secret(\"s3_stage_prefix\", \"magic_the_gathering/stage\")\n",
    "S3_BRONZE_PREFIX = get_secret(\"s3_bronze_prefix\", \"magic_the_gathering/bronze\")\n",
    "S3_STAGE_PATH = f\"s3://{S3_BUCKET}/{S3_STAGE_PREFIX}\"\n",
    "S3_BRONZE_PATH = f\"{S3_BUCKET}/{S3_BRONZE_PREFIX}\"\n",
    "\n",
    "# Configurações de janela temporal (5 anos)\n",
    "YEARS_BACK = int(get_secret(\"years_back\", \"5\"))\n",
    "current_year = datetime.now().year\n",
    "cutoff_year = current_year - YEARS_BACK\n",
    "CUTOFF_DATE = datetime(cutoff_year, 1, 1)\n",
    "CUTOFF_DATE_STR = CUTOFF_DATE.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "print(\"Iniciando pipeline EL Unity + Incremental (MERGE) - SETS\")\n",
    "print(f\"Filtro temporal: últimos {YEARS_BACK} anos (a partir de {CUTOFF_DATE_STR})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0adc1708-2177-42ea-91f0-529495d909ea",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Configurações"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FUNÇÕES UTILITÁRIAS\n",
    "# =============================================================================\n",
    "def setup_unity_catalog():\n",
    "    # Configura o Unity Catalog e schema\n",
    "    try:\n",
    "        spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG_NAME}\")\n",
    "        spark.sql(f\"USE CATALOG {CATALOG_NAME}\")\n",
    "        spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {SCHEMA_NAME}\")\n",
    "        spark.sql(f\"USE SCHEMA {SCHEMA_NAME}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao configurar Unity Catalog: {e}\")\n",
    "        return False\n",
    "\n",
    "def extract_from_staging(table_name):\n",
    "    # EXTRACT: Lê dados da camada staging\n",
    "    try:\n",
    "        stage_path = f\"{S3_STAGE_PATH}/*_{table_name}.parquet\"\n",
    "        df = spark.read.parquet(stage_path)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Erro no EXTRACT de staging: {e}\")\n",
    "        return None\n",
    "\n",
    "def apply_temporal_filter(df, table_name):\n",
    "    # Aplica filtro temporal de 5 anos\n",
    "    if table_name == \"sets\":\n",
    "        # Para sets, filtrar por data de lançamento (últimos 5 anos)\n",
    "        df = df.filter(col(\"releaseDate\") >= lit(CUTOFF_DATE_STR))\n",
    "        print(f\"Filtro temporal aplicado: sets dos últimos {YEARS_BACK} anos\")\n",
    "    return df\n",
    "\n",
    "def load_to_bronze_unity_merge(df, table_name):\n",
    "    # LOAD: Carrega dados na camada bronze (Unity + Incremental Merge)\n",
    "    if not df:\n",
    "        return None\n",
    "    try:\n",
    "        # Elimina duplicidade de code no DataFrame de origem\n",
    "        df = df.dropDuplicates(['code'])\n",
    "        \n",
    "        # Aplicar filtro temporal\n",
    "        df = apply_temporal_filter(df, table_name)\n",
    "        \n",
    "        # Particionamento para sets: por ano e mês de ingestão\n",
    "        if table_name == \"sets\":\n",
    "            df = df.withColumn(\"partition_year\", year(col(\"ingestion_timestamp\"))) \\\n",
    "                   .withColumn(\"partition_month\", month(col(\"ingestion_timestamp\")))\n",
    "        \n",
    "        delta_path = f\"s3://{S3_BRONZE_PATH}/{table_name}\"\n",
    "        full_table_name = f\"{CATALOG_NAME}.{SCHEMA_NAME}.{table_name}\"\n",
    "        from delta.tables import DeltaTable\n",
    "        \n",
    "        # Verificar se _delta_log existe e tem arquivos\n",
    "        delta_log_path = f\"{delta_path}/_delta_log\"\n",
    "        delta_exists = False\n",
    "        try:\n",
    "            files = dbutils.fs.ls(delta_log_path)\n",
    "            if files:\n",
    "                delta_exists = True\n",
    "        except:\n",
    "            delta_exists = False\n",
    "\n",
    "        if not delta_exists:\n",
    "            # Primeira criação com particionamento\n",
    "            if table_name == \"sets\":\n",
    "                df.write.format(\"delta\") \\\n",
    "                       .mode(\"overwrite\") \\\n",
    "                       .option(\"overwriteSchema\", \"true\") \\\n",
    "                       .partitionBy(\"partition_year\", \"partition_month\") \\\n",
    "                       .save(delta_path)\n",
    "            else:\n",
    "                df.write.format(\"delta\") \\\n",
    "                       .mode(\"overwrite\") \\\n",
    "                       .option(\"overwriteSchema\", \"true\") \\\n",
    "                       .save(delta_path)\n",
    "        else:\n",
    "            delta_table = DeltaTable.forPath(spark, delta_path)\n",
    "            delta_table.alias(\"bronze\").merge(\n",
    "                df.alias(\"novo\"),\n",
    "                \"bronze.code = novo.code\"\n",
    "            ).whenMatchedUpdateAll() \\\n",
    "             .whenNotMatchedInsertAll() \\\n",
    "             .execute()\n",
    "\n",
    "        # Criar tabela Unity Catalog sempre\n",
    "        try:\n",
    "            try:\n",
    "                spark.sql(f\"DROP TABLE IF EXISTS {full_table_name}\")\n",
    "            except:\n",
    "                pass\n",
    "            create_table_sql = f\"\"\"\n",
    "            CREATE TABLE {full_table_name}\n",
    "            USING DELTA\n",
    "            LOCATION '{delta_path}'\n",
    "            COMMENT 'Tabela bronze de sets do Magic: The Gathering'\n",
    "            \"\"\"\n",
    "            spark.sql(create_table_sql)\n",
    "            print(f\"Tabela Unity Catalog criada: {full_table_name}\")\n",
    "            try:\n",
    "                spark.sql(f\"\"\"\n",
    "                ALTER TABLE {full_table_name} SET TBLPROPERTIES (\n",
    "                    'bronze_layer' = 'true',\n",
    "                    'data_source' = 'mtg_api',\n",
    "                    'processing_date' = '{datetime.now().strftime(\"%Y-%m-%d\")}',\n",
    "                    'table_type' = 'bronze',\n",
    "                    'load_mode' = 'merge_incremental',\n",
    "                    'temporal_window_years' = '{YEARS_BACK}',\n",
    "                    'partitioning' = 'release_date_year_month'\n",
    "                )\n",
    "                \"\"\")\n",
    "            except Exception as e:\n",
    "                pass\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao criar tabela Unity Catalog: {e}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Erro no LOAD para bronze {table_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_el_unity_merge(table_name):\n",
    "    # Executa o pipeline EL Unity + Incremental Merge\n",
    "    stage_df = extract_from_staging(table_name)\n",
    "    if not stage_df:\n",
    "        print(f\"Falha no EXTRACT de staging para {table_name}\")\n",
    "        return None\n",
    "    \n",
    "    result_df = load_to_bronze_unity_merge(stage_df, table_name)\n",
    "    return result_df\n",
    "\n",
    "def query_bronze_unity(table_name):\n",
    "    # Consulta na tabela bronze Unity Catalog\n",
    "    try:\n",
    "        full_table_name = f\"{CATALOG_NAME}.{SCHEMA_NAME}.{table_name}\"\n",
    "        count_query = f\"SELECT COUNT(*) as total FROM {full_table_name}\"\n",
    "        count_result = spark.sql(count_query)\n",
    "        count_result.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao consultar tabela bronze: {e}\")\n",
    "\n",
    "def show_delta_info(table_name):\n",
    "    # Mostra informações da tabela Delta\n",
    "    try:\n",
    "        delta_path = f\"s3://{S3_BRONZE_PATH}/{table_name}\"\n",
    "        from delta.tables import DeltaTable\n",
    "        delta_table = DeltaTable.forPath(spark, delta_path)\n",
    "        history = delta_table.history()\n",
    "        print(f\"Versões Delta: {history.count()}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao mostrar informações Delta: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28f1d29d-c3d7-45f2-9d53-5823876ca16e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Query Bronze"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXECUÇÃO PRINCIPAL\n",
    "# =============================================================================\n",
    "\n",
    "try:\n",
    "    spark\n",
    "except NameError:\n",
    "    from pyspark.sql import SparkSession\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "setup_success = setup_unity_catalog()\n",
    "if not setup_success:\n",
    "    raise Exception(\"Falha ao configurar Unity Catalog\")\n",
    "\n",
    "sets_bronze_df = process_el_unity_merge(\"sets\")\n",
    "\n",
    "if sets_bronze_df:\n",
    "    print(\"Pipeline executado com sucesso\")\n",
    "    query_bronze_unity(\"sets\")\n",
    "    show_delta_info(\"sets\")\n",
    "else:\n",
    "    print(\"Falha no pipeline\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Sets",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

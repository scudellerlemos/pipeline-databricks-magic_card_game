{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dfac136-a0c2-4fdc-a2da-679862f615dc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import e Segredos"
    }
   },
   "outputs": [],
   "source": [
    "# Camada Bronze - Card Prices - Magic: The Gathering \n",
    "# Objetivo: Processo EL (Extract & Load) com nomenclatura padronizada\n",
    "# Características: Extract da staging (S3/Parquet) -> Load na bronze (Unity Catalog/Delta) \n",
    "# Melhorias: Nomenclatura padronizada, colunas padronizadas, merge incremental robusto\n",
    "\n",
    "# =============================================================================\n",
    "# BIBLIOTECAS UTILIZADAS\n",
    "# =============================================================================\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURAÇÃO DE SEGREDOS\n",
    "# =============================================================================\n",
    "def get_secret(secret_name, default_value=None):\n",
    "    try:\n",
    "        return dbutils.secrets.get(scope=\"mtg-pipeline\", key=secret_name)\n",
    "    except:\n",
    "        if default_value is not None:\n",
    "            print(f\"Segredo '{secret_name}' não encontrado, usando valor padrão\")\n",
    "            return default_value\n",
    "        else:\n",
    "            print(f\"Segredo obrigatório '{secret_name}' não encontrado\")\n",
    "            raise Exception(f\"Segredo '{secret_name}' não configurado\")\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURAÇÕES GLOBAIS\n",
    "# =============================================================================\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "CATALOG_NAME = get_secret(\"catalog_name\")\n",
    "SCHEMA_NAME = \"bronze\"\n",
    "TABLE_NAME = \"TB_BRONZE_CARDPRICES\"\n",
    "\n",
    "S3_BUCKET = get_secret(\"s3_bucket\")\n",
    "S3_STAGE_PREFIX = get_secret(\"s3_stage_prefix\", \"magic_the_gathering/stage\")\n",
    "S3_BRONZE_PREFIX = get_secret(\"s3_bronze_prefix\", \"magic_the_gathering/bronze\")\n",
    "S3_STAGE_PATH = f\"s3://{S3_BUCKET}/{S3_STAGE_PREFIX}\"\n",
    "S3_BRONZE_PATH = f\"{S3_BUCKET}/{S3_BRONZE_PREFIX}\"\n",
    "\n",
    "# =============================================================================\n",
    "# FUNÇÃO PARA CALCULAR PERÍODO TEMPORAL\n",
    "# =============================================================================\n",
    "def calculate_temporal_period(years_back=5):\n",
    "    # Calcula período de 5 anos completos baseado no ano atual\n",
    "    current_year = datetime.now().year\n",
    "    start_year = current_year - years_back\n",
    "    end_year = current_year  # Incluir o ano atual\n",
    "    \n",
    "    cutoff_date = datetime(start_year, 1, 1)\n",
    "    end_date = datetime(end_year, 12, 31)\n",
    "    \n",
    "    return {\n",
    "        'start_year': start_year,\n",
    "        'end_year': end_year,\n",
    "        'cutoff_date': cutoff_date,\n",
    "        'end_date': end_date,\n",
    "        'cutoff_date_str': cutoff_date.strftime(\"%Y-%m-%d\"),\n",
    "        'end_date_str': end_date.strftime(\"%Y-%m-%d\")\n",
    "    }\n",
    "\n",
    "# Configurar período temporal\n",
    "YEARS_BACK = 5\n",
    "temporal_config = calculate_temporal_period(YEARS_BACK)\n",
    "\n",
    "START_YEAR = temporal_config['start_year']\n",
    "END_YEAR = temporal_config['end_year']\n",
    "CUTOFF_DATE = temporal_config['cutoff_date']\n",
    "END_DATE = temporal_config['end_date']\n",
    "CUTOFF_DATE_STR = temporal_config['cutoff_date_str']\n",
    "END_DATE_STR = temporal_config['end_date_str']\n",
    "\n",
    "print(\"Iniciando pipeline EL Unity com nomenclatura padronizada - TB_BRONZE_CARDPRICES\")\n",
    "print(f\"Filtro temporal: {YEARS_BACK} anos incluindo ano atual ({START_YEAR} a {END_YEAR})\")\n",
    "print(f\"Período: {CUTOFF_DATE_STR} até {END_DATE_STR}\")\n",
    "\n",
    "# =============================================================================\n",
    "# MAPEAMENTO DE COLUNAS GOV\n",
    "# =============================================================================\n",
    "# Mapeamento das colunas originais para nomenclatura GOV (alinhado com Silver)\n",
    "COLUMN_MAPPING = {\n",
    "    # Identificação\n",
    "    'id': 'ID_CARD',\n",
    "    'name': 'NME_CARD',\n",
    "    \n",
    "    # Preços\n",
    "    'usd': 'VLR_USD',\n",
    "    'usd_foil': 'VLR_USD_FOIL',\n",
    "    'eur': 'VLR_EUR',\n",
    "    'eur_foil': 'VLR_EUR_FOIL',\n",
    "    'tix': 'VLR_TIX',\n",
    "    \n",
    "    # Metadados de preço\n",
    "    'prices': 'DESC_PRICES',\n",
    "    'price_updated_at': 'DT_PRICE_UPDATE',\n",
    "    \n",
    "    # Informações de mercado\n",
    "    'market_price': 'VLR_MARKET',\n",
    "    'market_price_foil': 'VLR_MARKET_FOIL',\n",
    "    'low_price': 'VLR_LOW',\n",
    "    'low_price_foil': 'VLR_LOW_FOIL',\n",
    "    'high_price': 'VLR_HIGH',\n",
    "    'high_price_foil': 'VLR_HIGH_FOIL',\n",
    "    \n",
    "    # Status de preço\n",
    "    'price_status': 'NME_PRICE_STATUS',\n",
    "    'price_available': 'FLG_PRICE_AVAILABLE',\n",
    "    \n",
    "    # URLs e Links\n",
    "    'image_url': 'URL_IMAGE',\n",
    "    'scryfall_uri': 'URL_SCRYFALL',\n",
    "    \n",
    "    # Informações de Carta\n",
    "    'rarity': 'NME_RARITY',\n",
    "    'set': 'COD_SET',\n",
    "    \n",
    "    # Metadados\n",
    "    'source': 'NME_SOURCE',\n",
    "    'error': 'DESC_ERROR',\n",
    "    'ingestion_timestamp': 'DT_INGESTION',\n",
    "    \n",
    "    # Colunas de particionamento (se existirem)\n",
    "    'partition_year': 'RELEASE_YEAR',\n",
    "    'partition_month': 'RELEASE_MONTH',\n",
    "    \n",
    "    # Datas\n",
    "    'created_at': 'DT_CREATED',\n",
    "    'updated_at': 'DT_UPDATED'\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "601c7638-7214-4dfc-ba1b-672aa40ecd93",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Configurações"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FUNÇÕES UTILITÁRIAS\n",
    "# =============================================================================\n",
    "def setup_unity_catalog():\n",
    "    # Configura o Unity Catalog e schema\n",
    "    try:\n",
    "        spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG_NAME}\")\n",
    "        spark.sql(f\"USE CATALOG {CATALOG_NAME}\")\n",
    "        spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {SCHEMA_NAME}\")\n",
    "        spark.sql(f\"USE SCHEMA {SCHEMA_NAME}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao configurar Unity Catalog: {e}\")\n",
    "        return False\n",
    "\n",
    "def extract_from_staging(table_name):\n",
    "    # EXTRACT: Lê dados da camada staging\n",
    "    try:\n",
    "        # Para TB_BRONZE_CARDPRICES, ler do arquivo card_prices da staging\n",
    "        if table_name == \"TB_BRONZE_CARDPRICES\":\n",
    "            stage_path = f\"{S3_STAGE_PATH}/*_card_prices.parquet\"\n",
    "        else:\n",
    "            stage_path = f\"{S3_STAGE_PATH}/*_{table_name}.parquet\"\n",
    "        df = spark.read.parquet(stage_path)\n",
    "        print(f\"Extraídos {df.count()} registros de staging para {table_name}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Erro no EXTRACT de staging: {e}\")\n",
    "        return None\n",
    "\n",
    "def apply_governance_naming(df):\n",
    "    # Aplica nomenclatura GOV nas colunas\n",
    "    try:\n",
    "        # Obter colunas disponíveis no DataFrame\n",
    "        available_columns = df.columns\n",
    "        \n",
    "        # Criar mapeamento apenas para colunas que existem\n",
    "        column_mapping = {}\n",
    "        for original_col, gov_col in COLUMN_MAPPING.items():\n",
    "            if original_col in available_columns:\n",
    "                column_mapping[original_col] = gov_col\n",
    "        \n",
    "        # Renomear colunas\n",
    "        for original_col, gov_col in column_mapping.items():\n",
    "            df = df.withColumnRenamed(original_col, gov_col)\n",
    "        \n",
    "        print(f\"Aplicada nomenclatura GOV em {len(column_mapping)} colunas\")\n",
    "        print(f\"Colunas renomeadas: {list(column_mapping.keys())}\")\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao aplicar nomenclatura GOV: {e}\")\n",
    "        return df\n",
    "\n",
    "\n",
    "\n",
    "def apply_temporal_filter(df, table_name):\n",
    "    # Aplica filtro temporal de 5 anos\n",
    "    if table_name == \"TB_BRONZE_CARDPRICES\":\n",
    "        # Para TB_BRONZE_CARDPRICES, aplicar filtro baseado nos cards existentes\n",
    "        # Filtro: preços de cards que estão na janela temporal\n",
    "        from pyspark.sql.functions import col, current_date, date_sub\n",
    "        \n",
    "        try:\n",
    "            # Usar período completo de 5 anos (2020 a 2024)\n",
    "            from pyspark.sql.functions import to_date\n",
    "            \n",
    "            # Ler dados de cards da staging para filtrar preços\n",
    "            cards_stage_path = f\"{S3_STAGE_PATH}/*_cards.parquet\"\n",
    "            print(f\"Carregando dados de cards para filtro temporal: {cards_stage_path}\")\n",
    "            \n",
    "            cards_df = spark.read.parquet(cards_stage_path)\n",
    "            print(f\"Cards carregados: {cards_df.count()} registros\")\n",
    "            \n",
    "            # Ler dados de sets da staging\n",
    "            sets_stage_path = f\"{S3_STAGE_PATH}/*_sets.parquet\"\n",
    "            print(f\"Carregando dados de sets para filtro temporal: {sets_stage_path}\")\n",
    "            \n",
    "            sets_df = spark.read.parquet(sets_stage_path)\n",
    "            print(f\"Sets carregados: {sets_df.count()} registros\")\n",
    "            \n",
    "            # Filtrar sets do período completo (5 anos incluindo ano atual)\n",
    "            sets_filtered = sets_df.filter(\n",
    "                (col(\"releaseDate\") >= CUTOFF_DATE_STR) & \n",
    "                (col(\"releaseDate\") <= END_DATE_STR)\n",
    "            )\n",
    "            print(f\"Sets filtrados ({START_YEAR} a {END_YEAR}): {sets_filtered.count()} registros\")\n",
    "            \n",
    "            # Obter lista de códigos de sets válidos\n",
    "            valid_sets = sets_filtered.select(\"code\").distinct()\n",
    "            valid_set_codes = [row.code for row in valid_sets.collect()]\n",
    "            print(f\"Sets válidos para filtro: {len(valid_set_codes)} códigos\")\n",
    "            \n",
    "            # Filtrar cards baseado nos sets válidos\n",
    "            cards_filtered = cards_df.filter(col(\"set\").isin(valid_set_codes))\n",
    "            print(f\"Cards filtrados por sets válidos: {cards_filtered.count()} registros\")\n",
    "            \n",
    "            # Obter lista de nomes de cards válidos\n",
    "            valid_cards = cards_filtered.select(\"name\").distinct()\n",
    "            valid_card_names = [row.name for row in valid_cards.collect()]\n",
    "            print(f\"Cards válidos para filtro: {len(valid_card_names)} nomes\")\n",
    "            \n",
    "            # Aplicar filtro nos preços baseado nos cards válidos\n",
    "            df_filtered = df.filter(col(\"name\").isin(valid_card_names))\n",
    "            \n",
    "            # Filtro adicional: garantir que só tenha preços de cards que existem em TB_BRONZE_CARDS\n",
    "            try:\n",
    "                bronze_cards_table = f\"{CATALOG_NAME}.{SCHEMA_NAME}.TB_BRONZE_CARDS\"\n",
    "                existing_cards_df = spark.table(bronze_cards_table).select(\"NME_CARD\").distinct()\n",
    "                existing_card_names = [row.NME_CARD for row in existing_cards_df.collect()]\n",
    "                \n",
    "                # Aplicar filtro adicional baseado em TB_BRONZE_CARDS\n",
    "                df_filtered = df_filtered.filter(col(\"name\").isin(existing_card_names))\n",
    "                \n",
    "                print(f\"Filtro adicional aplicado: {len(existing_card_names)} cards de TB_BRONZE_CARDS\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao aplicar filtro adicional baseado em TB_BRONZE_CARDS: {e}\")\n",
    "                print(\"Continuando apenas com filtro temporal...\")\n",
    "            \n",
    "            total_before = df.count()\n",
    "            total_after = df_filtered.count()\n",
    "            \n",
    "            print(f\"Filtro temporal aplicado para TB_BRONZE_CARDPRICES:\")\n",
    "            print(f\"  - Período: {CUTOFF_DATE_STR} até {END_DATE_STR}\")\n",
    "            print(f\"  - Anos incluindo atual: {START_YEAR} a {END_YEAR}\")\n",
    "            print(f\"  - Sets válidos: {len(valid_set_codes)}\")\n",
    "            print(f\"  - Cards válidos: {len(valid_card_names)}\")\n",
    "            print(f\"  - Registros antes: {total_before}\")\n",
    "            print(f\"  - Registros depois: {total_after}\")\n",
    "            print(f\"  - Registros removidos: {total_before - total_after}\")\n",
    "            \n",
    "            return df_filtered\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao aplicar filtro temporal baseado em cards: {e}\")\n",
    "            print(\"Continuando sem filtro temporal...\")\n",
    "            return df\n",
    "    \n",
    "    return df\n",
    "\n",
    "def check_delta_table_exists(delta_path):\n",
    "    # Verifica se a tabela Delta existe\n",
    "    try:\n",
    "        # Verificar se _delta_log existe\n",
    "        delta_log_path = f\"{delta_path}/_delta_log\"\n",
    "        try:\n",
    "            files = dbutils.fs.ls(delta_log_path)\n",
    "            if files:\n",
    "                # Tentar ler a tabela para verificar se tem dados\n",
    "                df = spark.read.format(\"delta\").load(delta_path)\n",
    "                count = df.count()\n",
    "                print(f\"Tabela Delta existe com {count} registros\")\n",
    "                return True, count > 0\n",
    "            else:\n",
    "                return False, False\n",
    "        except:\n",
    "            return False, False\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao verificar tabela Delta: {e}\")\n",
    "        return False, False\n",
    "\n",
    "def get_delta_schema_for_merge(delta_path):\n",
    "    # Obtém o schema da tabela Delta existente, excluindo colunas de particionamento\n",
    "    try:\n",
    "        df = spark.read.format(\"delta\").load(delta_path)\n",
    "        schema_fields = [field.name for field in df.schema.fields]\n",
    "        \n",
    "        # Excluir colunas de particionamento do schema para merge\n",
    "        partition_columns = ['RELEASE_YEAR', 'RELEASE_MONTH']\n",
    "        merge_schema = [col for col in schema_fields if col not in partition_columns]\n",
    "        \n",
    "        print(f\"Schema completo da tabela Delta: {schema_fields}\")\n",
    "        print(f\"Schema para merge (sem particionamento): {merge_schema}\")\n",
    "        return merge_schema\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao obter schema Delta: {e}\")\n",
    "        return []\n",
    "\n",
    "def prepare_dataframe_for_merge(df, table_name, target_schema=None):\n",
    "    # Prepara DataFrame para merge, garantindo compatibilidade de schema\n",
    "    if not df:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Aplicar nomenclatura GOV\n",
    "        df = apply_governance_naming(df)\n",
    "        \n",
    "        # Elimina duplicidade de NME_CARD no DataFrame de origem\n",
    "        total_before = df.count()\n",
    "        df = df.dropDuplicates(['NME_CARD'])\n",
    "        total_after = df.count()\n",
    "        print(f\"Removidas {total_before - total_after} duplicatas baseadas em 'NME_CARD'\")\n",
    "        \n",
    "        # Aplicar filtro temporal\n",
    "        df = apply_temporal_filter(df, table_name)\n",
    "        \n",
    "        # Se temos schema de destino, filtrar colunas compatíveis\n",
    "        if target_schema:\n",
    "            available_columns = df.columns\n",
    "            compatible_columns = [col for col in available_columns if col in target_schema]\n",
    "            \n",
    "            if not compatible_columns:\n",
    "                print(\"Nenhuma coluna compatível encontrada para merge\")\n",
    "                return None\n",
    "            \n",
    "            print(f\"Colunas compatíveis para merge: {compatible_columns}\")\n",
    "            df = df.select(compatible_columns)\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao preparar DataFrame para merge: {e}\")\n",
    "        return None\n",
    "\n",
    "def execute_merge_with_specific_columns(delta_table, merge_df, merge_columns):\n",
    "    # Executa merge especificando exatamente quais colunas atualizar\n",
    "    try:\n",
    "        # Construir a condição de merge (card_prices usa 'NME_CARD' como chave)\n",
    "        merge_condition = \"bronze.NME_CARD = novo.NME_CARD\"\n",
    "        \n",
    "        # Construir as ações de UPDATE especificando colunas\n",
    "        update_actions = {}\n",
    "        for col_name in merge_columns:\n",
    "            if col_name != 'NME_CARD':  # Não atualizar a chave de merge\n",
    "                update_actions[col_name] = f\"novo.{col_name}\"\n",
    "        \n",
    "        # Construir as ações de INSERT especificando colunas\n",
    "        insert_actions = {}\n",
    "        for col_name in merge_columns:\n",
    "            insert_actions[col_name] = f\"novo.{col_name}\"\n",
    "        \n",
    "        print(f\"Executando merge com {len(update_actions)} colunas para UPDATE\")\n",
    "        print(f\"Colunas para UPDATE: {list(update_actions.keys())}\")\n",
    "        \n",
    "        # Executar merge com ações específicas\n",
    "        delta_table.alias(\"bronze\").merge(\n",
    "            merge_df.alias(\"novo\"),\n",
    "            merge_condition\n",
    "        ).whenMatchedUpdate(set=update_actions) \\\n",
    "         .whenNotMatchedInsert(values=insert_actions) \\\n",
    "         .execute()\n",
    "        \n",
    "        print(\"Merge Delta executado com sucesso\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Erro no merge Delta: {e}\")\n",
    "        return False\n",
    "\n",
    "def check_unity_table_exists(full_table_name):\n",
    "    # Verifica se a tabela Unity Catalog existe\n",
    "    try:\n",
    "        # Tentar fazer uma consulta simples na tabela\n",
    "        test_query = f\"SELECT 1 FROM {full_table_name} LIMIT 1\"\n",
    "        spark.sql(test_query)\n",
    "        print(f\"Tabela Unity Catalog '{full_table_name}' existe\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Tabela Unity Catalog '{full_table_name}' não existe ou não está acessível\")\n",
    "        return False\n",
    "\n",
    "def create_or_update_unity_catalog(full_table_name, delta_path):\n",
    "    # Cria ou atualiza tabela Unity Catalog preservando modificações existentes\n",
    "    try:\n",
    "        table_exists = check_unity_table_exists(full_table_name)\n",
    "        \n",
    "        if not table_exists:\n",
    "            # Criar tabela pela primeira vez\n",
    "            print(f\"Criando tabela Unity Catalog: {full_table_name}\")\n",
    "            create_table_sql = f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {full_table_name}\n",
    "            USING DELTA\n",
    "            LOCATION '{delta_path}'\n",
    "            COMMENT 'Tabela bronze de fato de preços de cards do Magic: The Gathering com nomenclatura padronizada'\n",
    "            \"\"\"\n",
    "            spark.sql(create_table_sql)\n",
    "            print(f\"Tabela Unity Catalog criada: {full_table_name}\")\n",
    "        else:\n",
    "            # Tabela já existe, apenas atualizar propriedades de pipeline (sem perder modificações)\n",
    "            print(f\"Tabela Unity Catalog já existe: {full_table_name}\")\n",
    "            print(\"Atualizando apenas propriedades de pipeline (preservando modificações existentes)\")\n",
    "        \n",
    "        # Sempre atualizar propriedades de pipeline (não afeta modificações customizadas)\n",
    "        try:\n",
    "            spark.sql(f\"\"\"\n",
    "            ALTER TABLE {full_table_name} SET TBLPROPERTIES (\n",
    "                'bronze_layer' = 'true',\n",
    "                'data_source' = 'scryfall_api',\n",
    "                'last_processing_date' = '{datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}',\n",
    "                'table_type' = 'bronze',\n",
    "                'load_mode' = 'incremental_merge_gov',\n",
    "                'governance_applied' = 'true',\n",
    "                'naming_convention' = 'GOV',\n",
    "                'temporal_window_years' = '{YEARS_BACK}',\n",
    "                'temporal_filter_source' = 'cards_sets_releaseDate',\n",
    "                'temporal_filter_column' = 'name',\n",
    "                'partitioning' = 'RELEASE_YEAR_MONTH'\n",
    "            )\n",
    "            \"\"\")\n",
    "            print(\"Propriedades de pipeline atualizadas\")\n",
    "        except Exception as e:\n",
    "            print(f\"Aviso: Não foi possível atualizar propriedades de pipeline: {e}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao criar/atualizar tabela Unity Catalog: {e}\")\n",
    "\n",
    "def load_to_bronze_unity_incremental_gov(df, table_name):\n",
    "    # LOAD: Carrega dados na camada bronze (Unity + Incremental + GOV)\n",
    "    if not df:\n",
    "        return None\n",
    "    try:\n",
    "        delta_path = f\"s3://{S3_BRONZE_PATH}/{table_name}\"\n",
    "        full_table_name = f\"{CATALOG_NAME}.{SCHEMA_NAME}.{table_name}\"\n",
    "        from delta.tables import DeltaTable\n",
    "        \n",
    "        # Verificar se tabela Delta existe\n",
    "        delta_exists, has_data = check_delta_table_exists(delta_path)\n",
    "        \n",
    "        if not delta_exists or not has_data:\n",
    "            # Primeira criação com particionamento\n",
    "            print(\"Criando tabela Delta pela primeira vez com nomenclatura GOV...\")\n",
    "            df_processed = prepare_dataframe_for_merge(df, table_name)\n",
    "            \n",
    "            if table_name == \"TB_BRONZE_CARDPRICES\":\n",
    "                # Para TB_BRONZE_CARDPRICES, particionamento por ano de lançamento do set\n",
    "                # JOIN com cards e sets para pegar releaseDate real\n",
    "                from pyspark.sql.functions import current_timestamp, lit, year, month\n",
    "                \n",
    "                # Carregar dados de cards\n",
    "                cards_stage_path = f\"{S3_STAGE_PATH}/*_cards.parquet\"\n",
    "                print(f\"Carregando cards para particionamento: {cards_stage_path}\")\n",
    "                cards_df = spark.read.parquet(cards_stage_path)\n",
    "                \n",
    "                # Carregar dados de sets\n",
    "                sets_stage_path = f\"{S3_STAGE_PATH}/*_sets.parquet\"\n",
    "                print(f\"Carregando sets para particionamento: {sets_stage_path}\")\n",
    "                sets_df = spark.read.parquet(sets_stage_path)\n",
    "                \n",
    "                # Selecionar apenas colunas necessárias\n",
    "                cards_for_join = cards_df.select(\"name\", \"set\")\n",
    "                sets_for_join = sets_df.select(\"code\", \"releaseDate\")\n",
    "                print(f\"Cards carregados para JOIN: {cards_for_join.count()} registros\")\n",
    "                print(f\"Sets carregados para JOIN: {sets_for_join.count()} registros\")\n",
    "                \n",
    "                # JOIN com cards para pegar set (renomeando 'set' para 'card_set')\n",
    "                df_with_cards = df_processed.join(\n",
    "                    cards_for_join.withColumnRenamed(\"set\", \"card_set\"),\n",
    "                    df_processed.NME_CARD == cards_for_join.name,\n",
    "                    \"left\"\n",
    "                )\n",
    "                \n",
    "                # JOIN com sets para pegar releaseDate\n",
    "                df_with_sets = df_with_cards.join(\n",
    "                    sets_for_join,\n",
    "                    df_with_cards.card_set == sets_for_join.code,\n",
    "                    \"left\"\n",
    "                )\n",
    "                \n",
    "                # Criar colunas de particionamento baseadas no releaseDate real\n",
    "                df_with_partition = df_with_sets.withColumn(\"RELEASE_YEAR\", \n",
    "                                  year(col(\"releaseDate\"))) \\\n",
    "                       .withColumn(\"RELEASE_MONTH\", \n",
    "                                  month(col(\"releaseDate\")))\n",
    "                \n",
    "                # Remover colunas de JOIN desnecessárias\n",
    "                df_with_partition = df_with_partition.drop(\"name\", \"card_set\", \"code\", \"releaseDate\")\n",
    "                \n",
    "                print(f\"Particionamento criado com releaseDate real dos sets\")\n",
    "                \n",
    "                df_with_partition.write.format(\"delta\") \\\n",
    "                       .mode(\"overwrite\") \\\n",
    "                       .option(\"overwriteSchema\", \"true\") \\\n",
    "                       .partitionBy(\"RELEASE_YEAR\", \"RELEASE_MONTH\") \\\n",
    "                       .save(delta_path)\n",
    "            else:\n",
    "                df_processed.write.format(\"delta\") \\\n",
    "                       .mode(\"overwrite\") \\\n",
    "                       .option(\"overwriteSchema\", \"true\") \\\n",
    "                       .save(delta_path)\n",
    "            print(\"Tabela Delta criada com sucesso\")\n",
    "        else:\n",
    "            # Para merge, obter schema da tabela existente (excluindo particionamento) e preparar dados\n",
    "            print(\"Tabela Delta existe, preparando para merge...\")\n",
    "            target_schema = get_delta_schema_for_merge(delta_path)\n",
    "            \n",
    "            # Preparar DataFrame para merge\n",
    "            merge_df = prepare_dataframe_for_merge(df, table_name, target_schema)\n",
    "            if not merge_df:\n",
    "                print(\"Nenhum DataFrame compatível para merge\")\n",
    "                return None\n",
    "            \n",
    "            # Executar merge com colunas específicas (sem particionamento)\n",
    "            print(\"Executando merge Delta...\")\n",
    "            delta_table = DeltaTable.forPath(spark, delta_path)\n",
    "            \n",
    "            # Usar função específica para merge com colunas definidas\n",
    "            merge_success = execute_merge_with_specific_columns(delta_table, merge_df, target_schema)\n",
    "            if not merge_success:\n",
    "                print(\"Falha no merge Delta\")\n",
    "                return None\n",
    "\n",
    "        # Criar ou atualizar tabela Unity Catalog (PRESERVANDO MODIFICAÇÕES)\n",
    "        create_or_update_unity_catalog(full_table_name, delta_path)\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Erro no LOAD para bronze {table_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_el_unity_incremental_gov(table_name):\n",
    "    # Executa o pipeline EL Unity + Incremental + GOV\n",
    "    stage_df = extract_from_staging(table_name)\n",
    "    if not stage_df:\n",
    "        print(f\"Falha no EXTRACT de staging para {table_name}\")\n",
    "        return None\n",
    "    \n",
    "    result_df = load_to_bronze_unity_incremental_gov(stage_df, table_name)\n",
    "    return result_df\n",
    "\n",
    "def query_bronze_unity(table_name):\n",
    "    # Consulta na tabela bronze Unity Catalog\n",
    "    try:\n",
    "        full_table_name = f\"{CATALOG_NAME}.{SCHEMA_NAME}.{table_name}\"\n",
    "        count_query = f\"SELECT COUNT(*) as total FROM {full_table_name}\"\n",
    "        count_result = spark.sql(count_query)\n",
    "        count_result.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao consultar tabela bronze: {e}\")\n",
    "\n",
    "def show_delta_info(table_name):\n",
    "    # Mostra informações da tabela Delta\n",
    "    try:\n",
    "        delta_path = f\"s3://{S3_BRONZE_PATH}/{table_name}\"\n",
    "        from delta.tables import DeltaTable\n",
    "        delta_table = DeltaTable.forPath(spark, delta_path)\n",
    "        history = delta_table.history()\n",
    "        print(f\"Versões Delta: {history.count()}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao mostrar informações Delta: {e}\")\n",
    "\n",
    "def show_sample_data(table_name):\n",
    "    # Mostra dados de exemplo da tabela\n",
    "    try:\n",
    "        full_table_name = f\"{CATALOG_NAME}.{SCHEMA_NAME}.{table_name}\"\n",
    "        sample_query = f\"SELECT * FROM {full_table_name} LIMIT 5\"\n",
    "        print(\"Dados de exemplo:\")\n",
    "        spark.sql(sample_query).show(truncate=False)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao mostrar dados de exemplo: {e}\")\n",
    "\n",
    "def check_partitioning_working(table_name):\n",
    "    # Verifica se o particionamento está funcionando\n",
    "    try:\n",
    "        delta_path = f\"s3://{S3_BRONZE_PATH}/{table_name}\"\n",
    "        \n",
    "        # Verificar se existem partições\n",
    "        try:\n",
    "            partitions = dbutils.fs.ls(delta_path)\n",
    "            has_partitions = any(\"RELEASE_YEAR=\" in p.path for p in partitions)\n",
    "            \n",
    "            if has_partitions:\n",
    "                print(\"Particionamento funcionando: Encontradas partições RELEASE_YEAR\")\n",
    "                return True\n",
    "            else:\n",
    "                print(\"Particionamento não encontrado: Nenhuma partição RELEASE_YEAR\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao verificar particionamento: {e}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao verificar particionamento: {e}\")\n",
    "        return False\n",
    "\n",
    "def show_governance_info():\n",
    "    # Mostra informações sobre a governança aplicada\n",
    "    print(\"=\" * 60)\n",
    "    print(\"INFORMAÇÕES DE GOVERNAÇA - TB_BRONZE_CARDPRICES\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Nomenclatura GOV aplicada: {len(COLUMN_MAPPING)} colunas\")\n",
    "    print(f\"Período temporal: {YEARS_BACK} anos incluindo atual ({START_YEAR} a {END_YEAR})\")\n",
    "    print(f\"Filtro baseado em: Cards e Sets com releaseDate\")\n",
    "    print(f\"Particionamento: RELEASE_YEAR e RELEASE_MONTH\")\n",
    "    print(f\"Chave de merge: NME_CARD\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "def check_unity_catalog_needs_update(full_table_name, delta_path):\n",
    "    # Verifica se o Unity Catalog precisa ser atualizado\n",
    "    try:\n",
    "        table_exists = check_unity_table_exists(full_table_name)\n",
    "        \n",
    "        if not table_exists:\n",
    "            print(f\"Tabela Unity Catalog não existe: {full_table_name}\")\n",
    "            return True\n",
    "        \n",
    "        # Verificar se a tabela está apontando para o local correto\n",
    "        try:\n",
    "            # Tentar fazer uma consulta simples\n",
    "            test_query = f\"SELECT 1 FROM {full_table_name} LIMIT 1\"\n",
    "            spark.sql(test_query)\n",
    "            print(f\"Tabela Unity Catalog OK: {full_table_name}\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            print(f\"Tabela Unity Catalog com problema: {e}\")\n",
    "            return True\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao verificar Unity Catalog: {e}\")\n",
    "        return True\n",
    "\n",
    "def clean_cardprices_consistency():\n",
    "    # Remove preços de cards que não existem em TB_BRONZE_CARDS\n",
    "    try:\n",
    "        print(\"=\" * 60)\n",
    "        print(\"LIMPEZA DE CONSISTÊNCIA - TB_BRONZE_CARDPRICES\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        bronze_cards_table = f\"{CATALOG_NAME}.{SCHEMA_NAME}.TB_BRONZE_CARDS\"\n",
    "        bronze_prices_table = f\"{CATALOG_NAME}.{SCHEMA_NAME}.TB_BRONZE_CARDPRICES\"\n",
    "        \n",
    "        # Obter cards válidos de TB_BRONZE_CARDS\n",
    "        valid_cards_df = spark.table(bronze_cards_table).select(\"NME_CARD\").distinct()\n",
    "        valid_card_names = [row.NME_CARD for row in valid_cards_df.collect()]\n",
    "        print(f\"Cards válidos em TB_BRONZE_CARDS: {len(valid_card_names)}\")\n",
    "        \n",
    "        # Obter todos os preços\n",
    "        all_prices_df = spark.table(bronze_prices_table)\n",
    "        total_prices_before = all_prices_df.count()\n",
    "        print(f\"Total de preços antes da limpeza: {total_prices_before}\")\n",
    "        \n",
    "        # Filtrar apenas preços de cards válidos\n",
    "        filtered_prices_df = all_prices_df.filter(col(\"NME_CARD\").isin(valid_card_names))\n",
    "        total_prices_after = filtered_prices_df.count()\n",
    "        print(f\"Total de preços após filtro: {total_prices_after}\")\n",
    "        print(f\"Preços removidos: {total_prices_before - total_prices_after}\")\n",
    "        \n",
    "        # Recriar tabela apenas com preços válidos\n",
    "        delta_path = f\"s3://{S3_BRONZE_PATH}/TB_BRONZE_CARDPRICES\"\n",
    "        \n",
    "        # Salvar dados filtrados\n",
    "        filtered_prices_df.write.format(\"delta\") \\\n",
    "               .mode(\"overwrite\") \\\n",
    "               .option(\"overwriteSchema\", \"true\") \\\n",
    "               .partitionBy(\"RELEASE_YEAR\", \"RELEASE_MONTH\") \\\n",
    "               .save(delta_path)\n",
    "        \n",
    "        print(\"Tabela TB_BRONZE_CARDPRICES recriada apenas com preços válidos\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro na limpeza de consistência: {e}\")\n",
    "        return False\n",
    "\n",
    "def force_recreate_table_with_correct_schema(table_name):\n",
    "    # Força a recriação da tabela com schema correto\n",
    "    try:\n",
    "        delta_path = f\"s3://{S3_BRONZE_PATH}/{table_name}\"\n",
    "        full_table_name = f\"{CATALOG_NAME}.{SCHEMA_NAME}.{table_name}\"\n",
    "        \n",
    "        print(f\"Forçando recriação da tabela {table_name} com schema correto...\")\n",
    "        \n",
    "        # Remover tabela Delta existente\n",
    "        try:\n",
    "            dbutils.fs.rm(delta_path, recurse=True)\n",
    "            print(f\"Tabela Delta removida: {delta_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao remover tabela Delta: {e}\")\n",
    "        \n",
    "        # Remover tabela Unity Catalog se existir\n",
    "        try:\n",
    "            spark.sql(f\"DROP TABLE IF EXISTS {full_table_name}\")\n",
    "            print(f\"Tabela Unity Catalog removida: {full_table_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao remover tabela Unity Catalog: {e}\")\n",
    "        \n",
    "        # Recriar tabela com schema correto\n",
    "        print(\"Recriando tabela com schema correto...\")\n",
    "        \n",
    "        # Extrair dados da staging\n",
    "        stage_df = extract_from_staging(table_name)\n",
    "        if not stage_df:\n",
    "            print(\"Falha ao extrair dados da staging\")\n",
    "            return False\n",
    "        \n",
    "        # Aplicar nomenclatura GOV\n",
    "        stage_df = apply_governance_naming(stage_df)\n",
    "        \n",
    "        # Aplicar filtro temporal\n",
    "        stage_df = apply_temporal_filter(stage_df, table_name)\n",
    "        \n",
    "        # Criar tabela com particionamento correto\n",
    "        if table_name == \"TB_BRONZE_CARDPRICES\":\n",
    "            # Para TB_BRONZE_CARDPRICES, particionamento por ano de lançamento do set\n",
    "            from pyspark.sql.functions import current_timestamp, lit, year, month\n",
    "            \n",
    "            # Carregar dados de cards\n",
    "            cards_stage_path = f\"{S3_STAGE_PATH}/*_cards.parquet\"\n",
    "            print(f\"Carregando cards para particionamento: {cards_stage_path}\")\n",
    "            cards_df = spark.read.parquet(cards_stage_path)\n",
    "            \n",
    "            # Carregar dados de sets\n",
    "            sets_stage_path = f\"{S3_STAGE_PATH}/*_sets.parquet\"\n",
    "            print(f\"Carregando sets para particionamento: {sets_stage_path}\")\n",
    "            sets_df = spark.read.parquet(sets_stage_path)\n",
    "            \n",
    "            # Selecionar apenas colunas necessárias\n",
    "            cards_for_join = cards_df.select(\"name\", \"set\")\n",
    "            sets_for_join = sets_df.select(\"code\", \"releaseDate\")\n",
    "            \n",
    "            # JOIN com cards para pegar set (renomeando 'set' para 'card_set')\n",
    "            df_with_cards = stage_df.join(\n",
    "                cards_for_join.withColumnRenamed(\"set\", \"card_set\"),\n",
    "                stage_df.NME_CARD == cards_for_join.name,\n",
    "                \"left\"\n",
    "            )\n",
    "            \n",
    "            # JOIN com sets para pegar releaseDate\n",
    "            df_with_sets = df_with_cards.join(\n",
    "                sets_for_join,\n",
    "                df_with_cards.card_set == sets_for_join.code,\n",
    "                \"left\"\n",
    "            )\n",
    "            \n",
    "            # Criar colunas de particionamento baseadas no releaseDate real\n",
    "            df_with_partition = df_with_sets.withColumn(\"RELEASE_YEAR\", \n",
    "                              year(col(\"releaseDate\"))) \\\n",
    "                   .withColumn(\"RELEASE_MONTH\", \n",
    "                              month(col(\"releaseDate\")))\n",
    "            \n",
    "            # Remover colunas de JOIN desnecessárias\n",
    "            df_with_partition = df_with_partition.drop(\"name\", \"card_set\", \"code\", \"releaseDate\")\n",
    "            \n",
    "            print(f\"Particionamento criado com releaseDate real dos sets\")\n",
    "            \n",
    "            # Criar tabela Delta\n",
    "            df_with_partition.write.format(\"delta\") \\\n",
    "                   .mode(\"overwrite\") \\\n",
    "                   .option(\"overwriteSchema\", \"true\") \\\n",
    "                   .partitionBy(\"RELEASE_YEAR\", \"RELEASE_MONTH\") \\\n",
    "                   .save(delta_path)\n",
    "        \n",
    "        # Criar tabela Unity Catalog\n",
    "        create_or_update_unity_catalog(full_table_name, delta_path)\n",
    "        \n",
    "        print(f\"Tabela {table_name} recriada com schema correto\")\n",
    "        \n",
    "        # Verificar schema após recriação\n",
    "        print(\"\\nVerificando schema após recriação...\")\n",
    "        try:\n",
    "            df = spark.read.format(\"delta\").load(delta_path)\n",
    "            schema_fields = [field.name for field in df.schema.fields]\n",
    "            print(f\"Schema da tabela recriada: {schema_fields}\")\n",
    "            \n",
    "            # Verificar se todas as colunas estão com nomenclatura correta\n",
    "            expected_columns = [\n",
    "                'VLR_EUR', 'URL_IMAGE', 'DT_INGESTION', 'NME_CARD', 'NME_RARITY',\n",
    "                'URL_SCRYFALL', 'COD_SET', 'NME_SOURCE', 'VLR_TIX', 'VLR_USD',\n",
    "                'DESC_ERROR', 'RELEASE_YEAR', 'RELEASE_MONTH'\n",
    "            ]\n",
    "            \n",
    "            missing_columns = [col for col in expected_columns if col not in schema_fields]\n",
    "            incorrect_columns = [col for col in schema_fields if col in [\n",
    "                'image_url', 'rarity', 'scryfall_uri', 'set', 'source', 'error',\n",
    "                'partition_year', 'partition_month'\n",
    "            ]]\n",
    "            \n",
    "            if missing_columns:\n",
    "                print(f\"Colunas faltando: {missing_columns}\")\n",
    "            if incorrect_columns:\n",
    "                print(f\"Colunas com nomenclatura incorreta: {incorrect_columns}\")\n",
    "            if not missing_columns and not incorrect_columns:\n",
    "                print(\"✅ Schema correto - Todas as colunas estão com nomenclatura GOV\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao verificar schema: {e}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao recriar tabela: {e}\")\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2963f46-f64b-4eaf-bf6e-1c1ae5a02cce",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Ingestão"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXECUÇÃO PRINCIPAL\n",
    "# =============================================================================\n",
    "\n",
    "try:\n",
    "    spark\n",
    "except NameError:\n",
    "    from pyspark.sql import SparkSession\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "setup_success = setup_unity_catalog()\n",
    "if not setup_success:\n",
    "    raise Exception(\"Falha ao configurar Unity Catalog\")\n",
    "\n",
    "# Executar pipeline\n",
    "card_prices_bronze_df = process_el_unity_incremental_gov(\"TB_BRONZE_CARDPRICES\")\n",
    "\n",
    "# Verificar se precisa recriar tabela (apenas se schema estiver incorreto)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VERIFICAÇÃO DE SCHEMA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "delta_path = f\"s3://{S3_BRONZE_PATH}/TB_BRONZE_CARDPRICES\"\n",
    "try:\n",
    "    df = spark.read.format(\"delta\").load(delta_path)\n",
    "    schema_fields = [field.name for field in df.schema.fields]\n",
    "    \n",
    "    # Verificar se há colunas com nomenclatura incorreta\n",
    "    incorrect_columns = [col for col in schema_fields if col in [\n",
    "        'image_url', 'rarity', 'scryfall_uri', 'set', 'source', 'error',\n",
    "        'partition_year', 'partition_month'\n",
    "    ]]\n",
    "    \n",
    "    if incorrect_columns:\n",
    "        print(f\"Schema incorreto detectado: {incorrect_columns}\")\n",
    "        print(\"Recriando tabela com schema correto...\")\n",
    "        force_recreate_table_with_correct_schema(\"TB_BRONZE_CARDPRICES\")\n",
    "    else:\n",
    "        print(\"✅ Schema correto - Tabela não precisa ser recriada\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Erro ao verificar schema: {e}\")\n",
    "    print(\"Recriando tabela por segurança...\")\n",
    "    force_recreate_table_with_correct_schema(\"TB_BRONZE_CARDPRICES\")\n",
    "\n",
    "# Verificar consistência (apenas se necessário)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VERIFICAÇÃO DE CONSISTÊNCIA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    bronze_cards_table = f\"{CATALOG_NAME}.{SCHEMA_NAME}.TB_BRONZE_CARDS\"\n",
    "    bronze_prices_table = f\"{CATALOG_NAME}.{SCHEMA_NAME}.TB_BRONZE_CARDPRICES\"\n",
    "    \n",
    "    # Verificar se há inconsistência\n",
    "    df_cards = spark.table(bronze_cards_table).select(\"NME_CARD\").distinct()\n",
    "    df_prices = spark.table(bronze_prices_table).select(\"NME_CARD\").distinct()\n",
    "    \n",
    "    prices_not_in_cards = df_prices.join(df_cards, \"NME_CARD\", \"left_anti\")\n",
    "    inconsistent_count = prices_not_in_cards.count()\n",
    "    \n",
    "    if inconsistent_count > 0:\n",
    "        print(f\"Inconsistência detectada: {inconsistent_count} cards extras em TB_BRONZE_CARDPRICES\")\n",
    "        print(\"Executando limpeza de consistência...\")\n",
    "        clean_cardprices_consistency()\n",
    "    else:\n",
    "        print(\"✅ Consistência OK - Não é necessário limpeza\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Erro ao verificar consistência: {e}\")\n",
    "    print(\"Executando limpeza por segurança...\")\n",
    "    clean_cardprices_consistency()\n",
    "\n",
    "# Verificar Unity Catalog (apenas se necessário)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VERIFICAÇÃO DO UNITY CATALOG\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "full_table_name = f\"{CATALOG_NAME}.{SCHEMA_NAME}.TB_BRONZE_CARDPRICES\"\n",
    "delta_path = f\"s3://{S3_BRONZE_PATH}/TB_BRONZE_CARDPRICES\"\n",
    "\n",
    "if check_unity_catalog_needs_update(full_table_name, delta_path):\n",
    "    print(\"Atualizando Unity Catalog...\")\n",
    "    create_or_update_unity_catalog(full_table_name, delta_path)\n",
    "else:\n",
    "    print(\"✅ Unity Catalog OK - Não precisa ser atualizado\")\n",
    "\n",
    "if card_prices_bronze_df:\n",
    "    print(\"Pipeline executado com sucesso\")\n",
    "    \n",
    "    # Verificações e informações\n",
    "    query_bronze_unity(\"TB_BRONZE_CARDPRICES\")\n",
    "    show_delta_info(\"TB_BRONZE_CARDPRICES\")\n",
    "    #show_sample_data(\"TB_BRONZE_CARDPRICES\")\n",
    "    check_partitioning_working(\"TB_BRONZE_CARDPRICES\")\n",
    "    show_governance_info()\n",
    "else:\n",
    "    print(\"Falha no pipeline\") \n",
    "\n",
    "# =============================================================================\n",
    "# VALIDAÇÃO DE CONSISTÊNCIA ENTRE CARDS E CARDPRICES\n",
    "# =============================================================================\n",
    "try:\n",
    "    bronze_cards = f\"{CATALOG_NAME}.{SCHEMA_NAME}.TB_BRONZE_CARDS\"\n",
    "    bronze_prices = f\"{CATALOG_NAME}.{SCHEMA_NAME}.TB_BRONZE_CARDPRICES\"\n",
    "\n",
    "    df_cards = spark.table(bronze_cards).select(\"NME_CARD\").distinct()\n",
    "    df_prices = spark.table(bronze_prices).select(\"NME_CARD\").distinct()\n",
    "\n",
    "    # Cartas presentes em cards mas não em prices\n",
    "    cards_not_in_prices = df_cards.join(df_prices, \"NME_CARD\", \"left_anti\")\n",
    "    print(f\"Cartas presentes em TB_BRONZE_CARDS mas não em TB_BRONZE_CARDPRICES: {cards_not_in_prices.count()}\")\n",
    "    if cards_not_in_prices.count() > 0:\n",
    "        cards_not_in_prices.show(truncate=False)\n",
    "\n",
    "    # Cartas presentes em prices mas não em cards\n",
    "    prices_not_in_cards = df_prices.join(df_cards, \"NME_CARD\", \"left_anti\")\n",
    "    print(f\"Cartas presentes em TB_BRONZE_CARDPRICES mas não em TB_BRONZE_CARDS: {prices_not_in_cards.count()}\")\n",
    "    if prices_not_in_cards.count() > 0:\n",
    "        prices_not_in_cards.show(truncate=False)\n",
    "\n",
    "    # Contagem total\n",
    "    print(f\"Total de cartas em TB_BRONZE_CARDS: {df_cards.count()}\")\n",
    "    print(f\"Total de cartas em TB_BRONZE_CARDPRICES: {df_prices.count()}\")\n",
    "\n",
    "    if cards_not_in_prices.count() == 0 and prices_not_in_cards.count() == 0:\n",
    "        print(\"Validação de consistência OK: As duas tabelas possuem as mesmas cartas.\")\n",
    "    else:\n",
    "        print(\"Inconsistência detectada entre as tabelas de cards e cardprices!\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro na validação de consistência entre cards e cardprices: {e}\") "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5741366023378704,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "TB_BRONZE_CARDPRICES",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
